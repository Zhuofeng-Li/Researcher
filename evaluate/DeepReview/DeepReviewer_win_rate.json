{"id": "tc1jaWpi7M", "year": 2024, "paper_context": "\\title{Completing Visual Objects via Bridging Generation and Segmentation}\n\n\\begin{abstract}\n\nThis paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named \\textbf{\\textsl{MaskComp}}, delineates the completion process through iterative stages of generation and segmentation. \nIn each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images.\nWe demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, \\eg, ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.\n\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\n\\def\\tabdesignchoice#1{\n\\begin{table*}[#1]\n\t\\centering\n \t\\subfloat[\n\t\\textbf{Segmentation model}.\n\t\\label{tab:segm model}\n\t]{\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|ccc}\n                        Model & CLIPSeg & SEEM & SAM \\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 19.9 & 18.1 & 16.9\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t\\subfloat[\n\t\\textbf{IMD step number}.\n\t\\label{tab:step number}\n\t]{\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccccc}\n        $T$ & 1 & 3 & 5 & 7 \\\\ [.1em]\n        \\shline\n        FID & 24.7 & 19.4 & 16.9 & 16.1  \\\\\n    \\end{tabular}\n\\end{center}}\\end{minipage}\n}\n\\subfloat[\n\t\\textbf{\\# of sampled images}.\n\t\\label{tab:image number}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccc}\n        N & 4 & 5 & 6 \\\\ [.1em]\n        \\shline\n    FID & 17.4 & 16.9 & 16.8 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n    \\subfloat[\n\t\\textbf{Condition gating}.\n\t\\label{tab:gating}\n\t]{\n\t\t\\begin{minipage}{0.2\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        Gating & \\Checkmark & \\XSolidBrush \\\\ [.1em]\n                        \\shline\n\t\t\t\t\t\tFID & 16.9 & 18.2\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t\\caption{\\textbf{Design choices for IMD on AHP dataset.} We ablate (a) the impact of different segmentation networks, (b) IMD step number, (c) the number of sampled images in the segmentation stage, and (d) the gating operation in the CompNet.}\n\t\\label{tab:design choices}\n\\end{table*}}\n\\def\\tableablation#1{\n\\begin{table*}[#1]\n\t\\centering\n\t\\subfloat[\n\t\\textbf{Conditioned mask}.\n\t\\label{tab:conditioned mask}\n\t]{\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccccc}\n        Mask & Partial & Intermed. & Complete \\\\ [.1em]\n        \\shline\n        FID & 16.9 & 15.3 & 12.7   \\\\\n    \\end{tabular}\n\\end{center}}\\end{minipage}\n}\n\t\\subfloat[\n\t\\textbf{Occlusion rate}.\n\t\\label{tab:occ}\n\t]{\n\t\t\\begin{minipage}{0.27\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        Occ. & 20\\% & 40 \\% & 60 \\% & 80\\%\\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 13.4 & 15.7 & 17.2 & 29.9\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n \\subfloat[\n\t\\textbf{Inference time}.\n\t\\label{tab:time}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.2\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccc}\n        Comp. & Gen. & Segm. & Total \\\\ [.1em]\n        \\shline\n    Second &  14.3 & 1.2 & 15.5 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n  \\subfloat[\n\t\\textbf{Amodal baseline}.\n\t\\label{tab:amodal}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccc}\n        Model & Baseline & MaskComp \\\\ [.1em]\n        \\shline\n    FID & 29.4 & 16.9 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t\\caption{\\textbf{Ablation of MaskComp on AHP dataset}. We ablate (a) the different conditioning masks during inference, (b) the occlusion rate during inference, (c) the inference time of each component in an IMD step, and (d) the performance compared with the amodal baseline.}\n\t\\label{tab:ablation}\n\\end{table*}}\n\\def\\tablemoreablation#1{\n\\begin{table*}[#1]\n\t\\centering\n\t\\subfloat[\n\t\\textbf{Iteration for diffusion}.\n\t\\label{tab:iter diff}\n\t]{\n\t\t\\begin{minipage}{0.3\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        Iter & 20 & 40 & 50 \\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 16.9 & 15.7 & 15.1\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t \\subfloat[\n\t\\textbf{Occlusion type}.\n\t\\label{tab:occ type}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.3\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccc}\n        Occ. & Rectangle & Oval & Object \\\\ [.1em]\n        \\shline\n    FID & 15.3 & 15.1 & 16.9 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n \\subfloat[\n\t\\textbf{Availablility of complete object}.\n\t\\label{tab:complete data}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.3\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|cc}\n        Comp. & \\Checkmark & \\XSolidBrush \\\\[.1em]\n        \\shline\n    FID & 16.9 & 19.4 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\\\\\n \\subfloat[\n\t\\textbf{Voting strategies}.\n\t\\label{tab:voting}\n\t]{\n\t\t\\begin{minipage}{0.45\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        Strategy & Logits (V) & Logits (M)  & Mask (V)  & Mask (M)\\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 16.9 & 17.2 & 17.6 & 17.0\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n  \\subfloat[\n\t\\textbf{Mask loss}.\n\t\\label{tab:mask loss}\n\t]{\n\t\t\\begin{minipage}{0.3\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        $\\mathcal{L}_{mask}$ & \\Checkmark & \\XSolidBrush \\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 16.9 & 17.7 \\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t\\caption{\\textbf{More ablation of MaskComp.} We report the performance with the AHP dataset. (a) We ablate the iteration number of the diffusion model. (b) We report the performance with different types of occlusion. (c) We report the performance of MaskComp trained with or without the complete objects. (d) We ablate voting strategies. V: voting. M: Mean. (e) We ablate the effectiveness of adding intermediate supervision to predict the complete mask.}\n\t\\label{tab:more ablation}\n\\end{table*}}\n\n\\begin{figure*}[t]\n    \\centering    \n    \\includegraphics[width=\\linewidth]{figs/teaser.pdf}\n    \\caption{\\textbf{Illustration of iterative mask denoising (IMD).} Starting from an initial partial object and its corresponding mask, IMD utilizes alternating generation and segmentation stages to progressively refine the partial mask until it converges to the complete mask. With the complete mask as the condition, the final complete object can be seamlessly generated.\n    }\n    \\label{fig:teaser}\n\\end{figure*}\n\n\n\nIn recent years, creative image editing has attracted substantial attention and seen significant advancements. Recent breakthroughs in image generation techniques have delivered impressive results across various image editing tasks, including image inpainting \\citep{Xie_2023_CVPR}, composition \\citep{yang2023paint} and colorization \\citep{Chang_2023_CVPR}. However, another intriguing challenge lies in the domain of object completion (\\cref{fig:teaser0}). This task involves the restoration of partially occluded objects within an image, representing the image as a layered stack of objects and background, which can potentially enable a number of more complicated editing tasks such as object layer switching. Unlike other conditional generation tasks, \\eg, image inpainting, which only generates and integrates complete objects into images, object completion requires seamless alignment between the generated content and the given partial object, which imposes more challenges to recover realistic and comprehensive object shapes.\n \nTo guide the generative model in producing images according to a specific shape, additional conditions can be incorporated \\citep{Koley_2023_CVPR,Yang_2023_CVPR}. Image segmentation has been shown to be a critical technique for enhancing the realism and stability of generative models by providing pixel-level guidance during the synthesis process. Recent research, as exemplified in the latest study by Zhang et al. \\citep{zhang2023adding}, showcases that, by supplying object segmentations as additional high-quality masks for shaping the objects, it becomes possible to generate complex images of remarkable fidelity.\nIn this paper, we present MaskComp, a novel approach that bridges image generation and segmentation for effective object completion. MaskComp is rooted in a fundamental observation: the quality of the resulting image in the mask-conditioned generation is directly influenced by the quality of the conditioned mask \\citep{zhang2023adding}. That says the more detailed the conditioned mask, the more realistic the generated image. \nBased on this observation, unlike prior object completion methods that solely rely on partially visible objects for generating complete objects, MaskComp introduces an additional mask condition combined with an iterative mask denoising (IMD) process, progressively refining the incomplete mask to provide comprehensive shape guidance to object completion.\n\nOur approach formulates the partial mask as a noisy form of the complete mask and the IMD process is designed to iteratively denoise this noisy partial mask, eventually leading to the attainment of the complete mask. As illustrated in \\cref{fig:teaser}, each IMD step comprises two crucial stages: generation and segmentation. The generation stage's objective is to produce complete object images conditioning on the visible portion of the target object and an object mask. Meanwhile, the segmentation stage is geared towards segmenting the object mask within the generated images and aggregating these segmented masks to obtain a superior mask that serves as the condition for the subsequent IMD step. By seamlessly integrating the generation and segmentation stages, we demonstrate that each IMD step effectively operates as a mask-denoising mechanism, taking a partially observed mask as input and yielding a progressively more complete mask as output. Consequently, through this iterative mask denoising process, the originally incomplete mask evolves into a satisfactory complete object mask, enabling the generation of complete objects guided by this refined mask.\n\nThe effectiveness of MaskComp is demonstrated by its capacity to address scenarios involving heavily occluded objects and its ability to generate realistic object representations through the utilization of mask guidance. In contrast to recent progress in the field of image generation research, our contributions can be succinctly outlined as follows:\n\n\\begin{itemize}\n    \\item We explore and unveil the benefits of incorporating object masks into the object completion task. A novel approach, MaskComp, is proposed to seamlessly bridge the generation and segmentation.\n    \n    \\item We formulate the partial mask as a form of noisy complete mask and introduce an iterative mask denoising (IMD) process, consisting of alternating generation and segmentation stages, to refine the object mask and thus improve the object completion. \n    \n    \\item We conduct extensive experiments for analysis and comparison, the results of which indicate the strength and robustness of MaskComp against previous methods, \\eg, Stable Diffusion. \n\\end{itemize}\n\n\n\\section{Related Works}\n\n\\noindent\\textbf{Conditional image generation.}\nConditional image generation \\cite{lee2022autoregressive,gafni2022make,li2023gligen} involves the process of creating images based on specific conditions. These conditions can take various forms, such as layout \\citep{li2020bachgan,sun2019image,zhao2019image}, sketch \\citep{Koley_2023_CVPR}, or semantic masks \\citep{gu2019mask}. For instance, Cascaded Diffusion Models \\citep{ho2022cascaded} utilize ImageNet class labels as conditions, employing a two-stage pipeline of multiple diffusion models to generate high-resolution images. Meanwhile, in the work by \\citep{sehwag2022generating}, diffusion models are guided to produce novel images from low-density regions within the data manifold. Another noteworthy approach is CLIP \\citep{radford2021clip}, which has gained widespread adoption in guiding image generation in GANs using text prompts \\citep{galatolo2021generating, gal2022stylegan, zhou2021lafite}. In the realm of diffusion models, Semantic Diffusion Guidance \\citep{liu2023more} explores a unified framework for diffusion-based image generation with language, image, or multi-modal conditions. Dhariwal et al. \\citep{dhariwal2021diffusion} employ an ablated diffusion model that utilizes the gradients of a classifier to guide the diffusion process, balancing diversity and fidelity. Furthermore, Ho et al. \\citep{ho2022classifier} introduce classifier-free guidance in conditional diffusion models, incorporating score estimates from both a conditional diffusion model and a jointly trained unconditional diffusion model. \n\n\\noindent\\textbf{Object segmentation.}\nIn the realm of segmentation, traditional approaches have traditionally leaned on domain-specific network architectures to tackle various segmentation tasks, including semantic, instance, and panoptic segmentation \\citep{fcn, deeplabv1, mask-rcnn, spatial-instance, associate-instance, solo, panoptic-deeplab, max-deeplab, li2023towards,li2023panoramic,li2023robust,li2023rethinking,li2023paintseg,li2022video,li2022hybrid}. However, recent strides in transformer-based methodologies, have highlighted the effectiveness of treating these tasks as mask classification challenges \\citep{maskformer, knet, mask2former, detr}. MaskFormer \\citep{maskformer} and its enhanced variant \\citep{mask2former} have introduced transformer-based architectures, coupling each mask prediction with a learnable query. Unlike prior techniques that learn semantic labels at the pixel level, they directly link semantic labels with mask predictions through query-based prediction. Notably, the Segment Anything Model (SAM) \\citep{kirillov2023segment} represents a cutting-edge segmentation model that accommodates diverse visual and textual cues for zero-shot object segmentation. Similarly, SEEM \\citep{zou2023segment} is another universal segmentation model that extends its capabilities to include object referencing through audio and scribble inputs. By leveraging those foundation segmentation models, \\eg, SAM and SEEM, a number of downstream tasks can be boosted \\citep{ma2023segment,cen2023segment,yu2023inpaint}.\n\n\n\n\\section{MaskComp}\n\n\\label{sec:IMD}\n\n\\subsection{Problem Definition and Key Insight}\nWe address the object completion task, wherein the objective is to predict the image of a complete object $I_c\\in\\mathbb{R}^{3\\times H\\times W}$, based on its visible (non-occluded) part $I_p\\in\\mathbb{R}^{3\\times H\\times W}$. \n\nWe first discuss the high-level idea of the proposed \\textbf{I}terative \\textbf{M}ask \\textbf{D}enoising (IMD) and then illustrate the module details in \\cref{sec:object completion} and \\cref{sec:object segmentation}. The core of IMD is based on an essential observation: In the mask-conditioned generation, the quality of the generated object is intricately tied to the quality of the conditioned mask. As shown in \\cref{fig:mask_condition}, we visualize the completion result of the same partial object but with different conditioning masks. We notice a more complete object mask condition will result in a more complete and realistic object image. \nBased on this observation, high-quality occluded object completion can be achieved by providing a complete object mask as the condition.\n\n\\begin{figure}[t]\n\\includegraphics[width=\\linewidth]{figs/mask_condition.pdf}\n\\caption{Object completion with different mask conditions.}\n\\label{fig:mask_condition}\n\\end{figure}\n\n\\begin{figure*}[t]\n    \\centering    \n    \\includegraphics[width=\\linewidth]{figs/pipeline.pdf}\n    \\caption{\\textbf{Illustration of CompNet (generation stage of MaskComp).} The CompNet aims to recover the complete object $I_c$ from the partial object $I_p$ and a mask $M$. An object encoder is utilized to extract partial token $c_p$ which is gated and fed to the ControlNet to form the complete token $c_c$. The complete token $c_c$ serves as the condition to the diffusion U-Net to guide the conditional denoising process. In addition, a pre-diffusion mask is predicted from the partial token to encourage the object encoder to capture shape information.}\n    \\label{fig:pipeline}\n\\end{figure*}\n\\subsection{Iterative Mask Denoising}\nHowever, in real-world scenarios, the complete object mask is not available. To address this problem, we propose the IMD process which leverages intertwined generation and segmentation processes to approach the partial mask to the complete mask gradually. \nGiven a partially visible object $I_p$ and its corresponding partial mask $M_p$, the conventional object completion task aims to find a generative model $\\mathcal{G}$ such that $I_c\\leftarrow\\mathcal{G}(I_p)$, where $I_c$ is the complete object. Here, we additionally add the partial mask $M_p$ to the condition  $I_c\\leftarrow\\mathcal{G}(I_p, M_p)$, where $M_p$ can be assumed as an addition of the complete mask and a noise $M_p=M_c+\\Delta$. By introducing a segmentation model $\\mathcal{S}$, we can find a mask denoiser $\\mathcal{S}\\circ\\mathcal{G}$ from the object completion model:\n\\begin{equation}\nM_c\\leftarrow\\mathcal{S}\\circ\\mathcal{G}(I_p, M_c+\\Delta)\n\\label{equ:mask denoiser}\n\\end{equation}\nwhere $M_c=\\mathcal{S}(I_c)$.\nStarting from the visible mask $M_0=M_p$, as shown in \\cref{fig:teaser}, we repeatedly apply the mask denoiser $\\mathcal{S}\\circ\\mathcal{G}$ to gradually approach the visible mask $M_p$ to complete mask $M_c$. In each step, the input mask is denoised with a stack of generation and segmentation stages. Specifically, as the $\\mathcal{S}\\circ\\mathcal{G}(\\cdot)$ includes a generative process, we can obtain a set of estimations of denoised mask $\\{M_t^{(k)}\\}$. Here, we utilize a function $\\mathcal{V}(\\cdot)$ to find a more complete and reasonable mask from the $N$ sampled masks and leverage it as the input mask for the next iteration to further denoise. The updating rule can be written as:\n\\begin{equation}\nM_{t}^{(k)}=\\mathcal{S}\\circ\\mathcal{G}(I_p,\\hat{M}_{t-1}),\\,\\,\\,\n\\hat{M_t}=\\mathcal{V}(M_t^{(1)},\\cdots,M_t^{(N)})\n\\end{equation}\nwhere $N$ is the number of sampled images in each iteration. With a satisfactory complete mask $\\hat{M}_T$ after $T$ iterations, the object completion can be achieved accordingly by $\\mathcal{G}(I_p,\\hat{M}_T)$. The mathematical explanation of the process will be discussed in \\cref{sec:discussion}. \n\n\n\\subsection{Generation Stage}\n\\label{sec:object completion}\nWe introduce \\textbf{\\textsl{CompNet}} as the generative model $\\mathcal{G}$ which aims to recover complete objects based on partial conditions. We build CompNet based on popular ControlNet \\cite{zhang2023adding} while making fundamental modifications to enable object completion. As shown in \\cref{tab:form diff}, the target of ControlNet is to generate images strictly based on the given conditions, \\ie, $I_p\\leftarrow\\mathcal{G}(I_p,M_p)$, making it unable to complete object. Differently, CompNet is designed to recover the object. With a segmentation network, it can act as a mask denoiser to refine the conditioned mask, \\ie, $M_c\\leftarrow\\mathcal{S}\\circ\\mathcal{G}(I_p,M_p)$.\n\n\\begin{table}[t]\n    \\centering\n    \\scalebox{0.77}{\n    \\begin{tabular}{c|ccc}\n    \\hline\n    Method  & Objective & Objective with Segm. &  Object Comp.\\\\\n    \\hline\n    ControlNet & $I_p\\leftarrow\\mathcal{G}(I_p,M_p)$ & $M_p\\leftarrow\\mathcal{S}\\circ\\mathcal{G}(I_p,M_p)$ & \\textcolor{red}{\\XSolidBrush}\\\\\n    CompNet & $I_c\\leftarrow\\mathcal{G}(I_p,M_p)$ & $M_c\\leftarrow\\mathcal{S}\\circ\\mathcal{G}(I_p,M_p)$ & \\textcolor{mygreen}{\\Checkmark}\\\\\n    \\hline\n    \\end{tabular}}\n    \\caption{Objective difference with ControlNet.}\n    \\label{tab:form diff}\n\\end{table}\n\n\\noindent\\textbf{Mask condition.}\nAs illustrated on the left side of \\cref{fig:pipeline}, we begin with a complete object $I_c$ and its corresponding mask $M_c$. Our approach commences by occluding the complete object, retaining only the partially visible portion as $I_p$. Recall that the mask-denoising procedure initiates with the partial mask $M_p$ and culminates with the complete mask $M_c$. To facilitate this iterative denoising, the model must effectively handle any mask that falls within the interpolation between the initial partial mask and the target complete mask. Consequently, we introduce a mask $M$ with an occlusion rate positioned between the partial and complete masks as a conditioning factor for the generative model. The partial mask $M_p$ is achieved by considering the occluded areas in both occlusion processes. The interpolated mask $M$ is generated by using one of the occlusions. \n\n\\noindent\\textbf{Diffusion model.}\nDiffusion models have achieved notable progress in synthesizing unprecedented image quality and have been successfully applied to many text-based image generation works \\citep{rombach2022high,zhang2023adding}. For our object completion task, the complete object can be generated by leveraging the diffusion process.\n\nSpecifically, the diffusion model generates image latent $x$ by gradually reversing a Markov forward process. As shown in Figure~\\ref{fig:pipeline}, starting from $x_0=\\mathcal{E}(I_c)$, the forward process yields a sequence of increasing noisy tokens $\\{x_\\tau|\\tau\\in[1,T_{\\mathcal{G}}]\\}$, where $x_\\tau=\\sqrt{\\Bar{\\alpha_\\tau}}y_0+\\sqrt{1-\\Bar{\\alpha_\\tau}}\\epsilon$, $\\epsilon$ is the Gaussian noise, and $\\alpha_\\tau$ decreases with the timestep $\\tau$. For the denoising process, the diffusion model progressively denoises a noisy token from the last step given the conditions $c=(I_p,M,E)$ by minimizing the following loss function: $\\mathcal{L}=\\mathbb{E}_{\\tau,x_0,\\epsilon}\\|\\epsilon_\\theta(x_\\tau,c,\\tau)-\\epsilon\\|_2^2$. $I_p$, $M$, and $E$ are the partial object, conditioned mask, and text prompt respectively.\n\\begin{figure*}[t]\n    \\centering    \n    \\includegraphics[width=\\linewidth]{figs/mask.pdf}\n    \\caption{\n    We calculate the mask probability map by averaging and normalizing the masks of sampled images. We show a cross-section of the lower leg to better visualize (shown as \\textcolor{darkyellow}{yellow}). \n    }\n    \\label{fig:mask}\n\\end{figure*}\n\n\\noindent\\textbf{CompNet architecture.}\nPrevious work \\citep{zhang2023adding} has demonstrated an effective way to add additional control to generative diffusion models. We follow this architecture and make necessary modifications to adapt the architecture to object completion. As shown in \\cref{fig:pipeline}, given the visible object $I_p$ and the conditioning mask $M$, we first concatenate them and extract the partial token $c_p$ with an object encoder. \nDifferent from ControlNet \\citep{zhang2023adding} assuming the condition is accurate, the object completion task relies on incomplete conditions. Specifically, in the early diffusion steps, the condition information is vital to complete the object. Nevertheless, in the later steps, inaccurate information in the condition can degrade the generated object. To tackle this problem, we introduce a time-variant gating operation to adjust the importance of conditions in the diffusion steps. We learn a linear transform $f: \\mathbb{R}^{C}\\rightarrow\\mathbb{R}^{1}$ upon the time embedding $e_t\\in\\mathbb{R}^{C}$ and then apply it to the partial token as $f(e_t)\\cdot c_p$ before feeding it to the ControlNet. In this way, the importance of visible features can be adjusted as the diffusion steps forward. The time embedding used for the gating operation is shared with the time embedding for encoding the diffusion step in the stable diffusion.\n\nTo encourage the object encoder to capture shape information, we introduce an auxiliary path to predict the complete object mask from the partial token $c_p$. Specifically, a feature pyramid network \\cite{lin2017feature} is leveraged as the mask decoder which takes $c_p$ and the multi-scale features from the object encoder as input and outputs a pre-diffusion mask $M_{pre}$. We encourage mask completion with supervision as\n\\begin{equation}\n    \\mathcal{L}_{mask}=\\mathcal{L}_{dice}(M_c,M_{pre})+\\lambda_{ce}\\mathcal{L}_{ce}(M_c,M_{pre})\n\\end{equation}\nwhere $\\mathcal{L}_{dice}$ and $\\mathcal{L}{ce}$ are Dice loss \\cite{li2019dice} and BCE loss respectively. $\\lambda_{ce}$ is a constant.\n\n\\subsection{Segmentation Stage}\n\\label{sec:object segmentation}\nIn the segmentation stage, illustrated in \\cref{fig:mask} (a), our approach initiates by sampling $N$ images denoted as $\\{I_t^{(k)}\\}^N_{k=1}$ from the generative model, where $t$ is the IMD step. Subsequently, we employ an off-the-shelf object segmentation model denoted as $\\mathcal{S}(\\cdot)$ to obtain the shapes (object masks) $\\{M_t^{(k)}\\}$ from these sampled images.\n\nTo derive an improved mask for the subsequent IMD step, we seek a function $\\mathcal{V}(\\cdot)$ that can produce a high-quality mask prediction from the set of $N$ generated masks. Interestingly, though the distribution of sampled images is complex, we notice the distribution of masks has good properties.\nIn \\cref{fig:mask} (b), we provide a visualization of the probability map associated with a set of object masks with the same conditions, which is computed by taking the normalized average of the masks.\nTo enhance the visualization of this probability distribution, we focus on a specific cross-section of the fully occluded portion in image $I_p$ (the lower leg, represented as a \\textcolor{darkyellow}{yellow} section) and visualize the probability as a function of the horizontal coordinate which demonstrates an obvious unimodal and symmetric property. Leveraging this observation, we can find an improved mask by taking the high-probability region. \nThe updating can be achieved by conducting a voting process across the $N$ estimated masks, as defined by the following equation:\n\\begin{equation}\n    \\hat{M}_t[i,j] = \\begin{cases}\n                1, & \\text{if}\\quad \\frac{\\sum_{k=1}^NM_t^{(k)}[i,j]}{N} \\geq \\tau \\\\\n                0, & \\text{otherwise}\n        \\end{cases}\n\\end{equation}\nwhere $[i,j]$ denotes the coordinate, and $\\tau$ is the threshold employed for the mask voting process.\n\n\n\n\n\n\\begin{figure}[t]\n\\centering   \n\\includegraphics[width=\\linewidth]{figs/dist.pdf}\n\\caption{\\textbf{Mutual-benificial sampling.}}\n\\label{fig:mutual}\n\\end{figure}\n\n\\begin{table*}[t]\n\\centering\n\\scalebox{0.9}{\n\\begin{tabular}{l|p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}|p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}} \n\\hline\n\\multirow{2}*{Method} & \\multicolumn{4}{c|}{AHP \\citep{zhou2021human}} &\n\\multicolumn{4}{c}{DYCE \\citep{ehsani2018segan}}\\\\\n\\cline{2-9}\n~ & FID-G $\\downarrow$ & FID-S $\\downarrow$ & Rank $\\downarrow$ & Best $\\uparrow$ & FID-G $\\downarrow$ & FID-S $\\downarrow$ & Rank $\\downarrow$ & Best $\\uparrow$\\\\\n\\hline\nControlNet & 40.2 & 45.4 & 3.4 & 0.10 & 42.4 & 49.4 & 3.4 & 0.08 \\\\\nKandinsky 2.1 & 43.9 & 39.2 & 3.2 & 0.11 & 44.3 & 47.7 &  3.4 & 0.06 \\\\\nStable Diffusion 1.5 & 35.7 & 41.4 & 3.2 & 0.12 & 31.2 & 43.4 & 3.4 & 0.11 \\\\\nStable Diffusion 2.1 & 30.8 & 39.9 & 3.1 & 0.14 & 30.0 & 41.1 & 3.0 & 0.12 \\\\\n\\bf MaskComp (Ours) & \\bf16.9 & \\bf21.3 &\\bf2.1 & \\bf0.53 & \\bf 20.0 & \\bf25.4 & \\bf1.9 & \\bf0.63  \\\\\n\\hline\n\\end{tabular}}\n\\caption{\\textbf{Quantitative evaluation on object completion task}. The computing of FID-G and FID-S only considers the object areas within ground truth and foreground regions segmented by SAM, respectively, to eliminate the influence of the generated background. The Rank denotes the average ranking in the user study. The Best denotes the percentage of samples that are ranked as the best. $\\downarrow$ and $\\uparrow$ denote the smaller the better and the larger the better respectively.}\n\\label{tab:main results}\n\\end{table*}\n\n\\subsection{Discussion}\n\\label{sec:discussion}\nIn this section, we will omit the conditioned partial image $I_p$ for simplicity.\n\n\\noindent\\textbf{Joint modeling of mask and object.}\nIn practical scenarios where the complete object mask $M_c$ is unavailable, modeling object completion through a marginal probability $p(I_c|M_c)$ becomes infeasible. Instead, it necessitates the more challenging joint modeling of objects and masks, denoted as $p(I,M)$, where the images and masks can range from partial to complete. Let us understand the joint distribution by exploring its marginals. Since the relation between mask and image is one-to-many (each object image only has one mask while the same mask can be segmented from multiple images), the $p(M|I)$ is actually a Dirac delta distribution $\\delta$ and only the $p(I|M)$ is a real distribution. This way, the joint distribution of mask and image is discrete and complex, making the modeling difficult. To address this issue, we introduce a slack condition to the joint distribution $p(I,M)$ that \\textit{the mask and image can follow a many-to-many relation}, which makes its marginal $p(M|I)$ a real distribution and permits $p(I|M)$ to predict image $I$ that has a different shape as the conditioned $M$ and vice versa. \n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/compare.pdf}\n    \\caption{\\textbf{Qualitative comparison against ControlNet, Kandinsky and Stable Diffusion}. The background is filtered out for better visualization.}\n    \\label{fig:compare}\n\\end{figure*}\n\n\\noindent\\textbf{Mutual-beneficial sampling.}\nAfter discussing the joint distribution that we are targeting, we introduce the mathematical explanation of MaskComp. MaskComp introduces the alternating modeling of two marginal distributions $p(I|M)$ (generation stage) and $p(M|I)$ (segmentation stage), which is actually a Markov Chain Monte Carlo-like (MCMC-like) process and more specifically Gibbs sampling-like. It samples the joint distribution $p(I,M)$ by iterative sampling from the marginal distributions. Two core insights are incorporated in MaskComp: (1) providing a mask as a condition can effectively enhance object generation and (2) fusing the mask of generated object images can result in a more accurate and complete object mask. Based on these insights, we train CompNet to maximize $p(I|M)$ and leverage mask voting to maximize the $p(M|I)$. As shown in \\cref{fig:mutual}, MaskComp develops a mutual-beneficial sampling process from the joint distribution $p(I,M)$, where the object mask is provided to boost the image generation and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. Through alternating sampling from the marginal distributions, we can effectively address the object completion task.\n\n\n\n\\section{Experiment}\n\n\\subsection{Experimental Settings}\n\\noindent\\textbf{Dataset.}\nWe evaluate MaskComp on two popular datasets: AHP \\citep{zhou2021human} and DYCE \\citep{ehsani2018segan}. \nAHP is an amodal human perception dataset with 56,302 images with annotations of integrated humans.\nDYCE is a synthetic dataset with photo-realistic images and the natural configuration of objects in indoor scenes.\nFor both datasets, the non-occluded object and its corresponding mask for each object are available. We train MaskComp on the AHP and a filtered subset of OpenImage v6 \\citep{kuznetsova2020open}. OpenImage is a large-scale dataset offering heterogeneous annotations. We select a subset of OpenImage that contains 429,358 objects as a training set of MaskComp.\n\\tableablation{t}\n\n\\noindent\\textbf{Evaluation metrics.}\nIn accordance with previous methods \\citep{zhou2021human}, we evaluate image generation quality Fréchet Inception Distance (FID). The background is removed with object masks before evaluation. \nAs the FID score cannot reflect the object completeness, we further conduct a user study, leveraging human assessment to compare the quality and completeness of images. During the assessment, given a partial object, the participants are required to rank the generated object from different methods based on their completeness and quality.\n\n\\noindent\\textbf{Implementation details.}\nFor the generation stage, we train the CompNet with frozen Stable Diffusion \\citep{rombach2022high} on the AHP dataset for 50 epochs. The learning rate is set for 1e-5. We adopt $\\mathrm{batchsize}=8$ and an Adam \\citep{loshchilov2017adamw} optimizer. \nThe image is resized to $512\\times 512$ for both training and inference. \nThe object is cropped and resized to have the longest side 360 before sticking on the image. \nFor a more generalized setting, we train the CompNet on a subset of the OpenImage \\citep{kuznetsova2020open} dataset for 36 epochs. We generate text prompts using BLIP \\citep{li2022blip} for all experiments (prompts are necessary to train ControlNet). For the segmentation stage, we leverage SAM \\citep{kirillov2023segment} as $\\mathcal{S}(\\cdot)$. We vote mask with a threshold of $\\tau=0.5$. During inference, if no other specification, we conduct the IMD process for 5 steps with $N=5$ images for each step. We give the class label as the text prompt to facilitate the CompNet to effectively generate objects. All baseline methods are given the same text prompts during the experiments. The code will be made publicly available.\n\n\\subsection{Main Results}\n\n\\noindent\\textbf{Quantitative results.}\nWe compare the MaskComp with state-of-the-art methods, ControlNet \\citep{zhang2023adding}, Kandinsky 2.1 \\citep{kandinsky2}, Stable Diffusion 1.5 \\citep{rombach2022high} and Stable Diffusion 2.1 \\citep{rombach2022high} on AHP \\citep{zhou2021human} and DYCE \\citep{ehsani2018segan} dataset. The results in \\cref{tab:main results} indicate that MaskComp consistently outperforms other methods, as evidenced by its notably lower FID scores, signifying the superior quality of its generated content. We conducted a user study to evaluate object completeness in which participants ranked images generated by different approaches. MaskComp achieved an impressive average ranking of 2.1 and 1.9 on the AHP and DYCE datasets respectively. Furthermore, MaskComp also generates the highest number of images ranked as the most complete and realistic compared to previous methods. We consider the introduced mask condition and the proposed IMD process benefits the performance of MaskComp, where the additional conditioned mask provides robust shape guidance to the generation process and the proposed iterative mask denoising process refines the initial conditioned mask to a more complete shape, further enhancing the generated image quality.   \n\n\\noindent\\textbf{Qualitative results.}\nWe present visual comparisons among ControlNet, Kandinsky 2.1, Stable Diffusion 1.5, and Stable Diffusion 2.1, illustrated in \\cref{fig:compare}. Our visualizations showcase MaskComp's ability to produce realistic and complete object images given partial images as the condition, whereas previous approaches exhibit noticeable artifacts and struggle to achieve realistic object completion. In addition, without mask guidance, it is common for previous methods to generate images that fail to align with the partial object.\n\n\\begin{figure*}[t]\n    \\centering    \n    \\includegraphics[width=\\linewidth]{figs/vis_imd.pdf}\n    \\caption{\\textbf{Visualization of the IMD process.} For each step, we randomly demonstrate one generated image and the averaged mask for all generated images. We omit the input mask which has the same shape as the input occluded object.\n    }\n    \\label{fig:vis_imd}\n\\end{figure*}\n\n\\tabdesignchoice{t}\n\n\\subsection{Analysis}\n\\label{sec:analysis}\nIn this section, we provide an experimental analysis of MaskComp. All the results are evaluated with GT masks to filter out the background, \\ie, FID-G.\n\n\\noindent\\textbf{Performance with different mask conditions.}\nWe evaluated the quality of generated images when conditioned on the same partial images along with three distinct types of masks: (1) partial mask (mask of the partial image), (2) intermediate mask (less occlusion than partial), and (3) complete mask. As shown in \\cref{tab:conditioned mask}, the model achieves its highest performance when it is conditioned with complete object masks, whereas relying solely on partial masks yields less optimal results. These results provide strong evidence that the quality of the conditioned mask significantly influences the quality of the generated images.\n\n\\noindent\\textbf{Performance with different occlusion rates.}\nWe perform ablation studies to assess the resilience of MaskComp under varying occlusion levels. As presented in \\cref{tab:occ}, we evaluate MaskComp at different occlusion levels (proportion of the obscured area relative to the complete object) ranging from 20\\% to 80\\%, and the results indicate that its performance does not degrade significantly up to 60\\% occlusion.\n\n\n\\noindent\\textbf{Inference time.} \\cref{tab:time} reports the inference time of each component in IMD (with a single NVIDIA V100 GPU). Although MaskComp's throughput is reduced due to the inclusion of multiple diffusion processes in each IMD step, it is capable of attaining a higher degree of accuracy in visual object completion. Based on our empirical experiments, reducing the number of diffusion steps during the first few IMD steps can increase model speed without sacrificing much performance. With this idea incorporated into MaskComp, the average running time could be reduced to 2/3 of the original time with FID slightly increasing by 0.50. While beyond the scope of this study, we expect more advanced techniques could be explored to optimize the tradeoff between model speed and performance.\n\n\\noindent\\textbf{Comparison to amodal segmentation baseline.}\nAmodal segmentation has a similar objective to the proposed IMD process. To demonstrate the effectiveness of MaskComp, we construct an amodal baseline that generates amodal masks from the SOTA amodal segmentation method \\cite{tran2022aisformer} and then utilize ControlNet to generate images based on the amodal masks. s shown in \\cref{tab:amodal}, we notice that our method outperforms the amodal baseline by a considerable margin, which could be attributed to the strong mask completion capability of the proposed IMD process.\n\n\\begin{table}[t]\n    \\centering\n    \\scalebox{0.95}{\n    \\begin{tabular}{c|ccccc}\n        Noise degree & Iter. 1 & Iter. 3 & Iter. 5 & Iter. 7 & Iter. 9 \\\\\n        \\toprule\n        15\\% area & 28.4 & 22.7 & 18.9 & 17.2 & 16.5 \\\\\n        10\\% area & 26.4 & 21.4 & 18.1 & 17.0 & 16.4 \\\\\n        5\\% area & 24.9 & 19.6 & 17.0 & 16.2 & 16.0 \\\\\n        No noise & 24.7 & 19.4 & 16.9 & 16.1 & 15.9 \n    \\end{tabular}}\n    \\caption{Performance against segmentation errors on AHP dataset.}\n    \\label{tab:robust}\n\\end{table}\n\n\\noindent\\textbf{Impact of different segmentation networks.} \nWe adopt SAM to obtain object masks at the segmentation stage. To study the impacts of different segmenters, we replace SAM with two smaller segmentation networks, CLIPSeg \\cite{luddecke2022image} and SEEM \\cite{zou2023segment}. \\cref{tab:segm model} shows that the FID score with CLIPSeg (19.9) is slightly higher than with SAM (16.9), but remains competitive against other state-of-the-art methods, e.g., Stable Diffusion 2.1 (30.8 reported in \\cref{tab:main results}). MaskComp is an iterative mask denoising (IMD) process that progressively refines a partial object mask to boost image generation. The results support our hypothesis that the impact of the segmenter is modest.\n\n\\noindent\\textbf{Design choices in IMD.}\nWe conduct experiments to ablate the design choices in IMD and their impacts on the completion performance. We first study the effect of IMD step number. With a larger step number, IMD can better advance the partial mask to the complete mask. As shown in \\cref{tab:step number}, we notice that the image quality keeps increasing and slows down at a step number of 5. In this way, we choose 5 as our IMD step number. After that, we ablate the number of sampled images in the segmentation stage in \\cref{tab:image number}. We notice more sampled images generally leading to a better performance. We leverage an image number of 5 with the efficiency consideration. As we leverage the diffusion-based method for image generation, we ablate the iterations for the diffusion process. As shown in \\cref{tab:gating}, we notice the gating operation improves the generation quality by 1.3 FID, indicating the necessity of conditional gating.\n\\noindent\\textbf{Robustness to segmentation errors.} \nWe conduct experiments to manually add random errors to masks. As shown in \\cref{tab:robust}, we ablate on the number of iterations and the degree of segmentation error. We observe that segmentation errors will increase the convergence iteration number while not affecting the final performance significantly. As IMD is a reciprocal process intended to provide effective control for later-generated masks to be refined based on adaptive feedback, mask errors are mitigated and not propagated.\n\\noindent\\textbf{Visualization of iterative mask denoising.}\nTo provide a clearer depiction of the IMD process, as depicted in \\cref{fig:vis_imd}, we present visualizations of the generated image and the averaged mask for each step. In the initial step, we observe the emergence of artifacts alongside the object. As we progress through the steps, both the image and mask quality exhibit continuous improvement.\n\n\n\n\n\\section{Conclusion}\n\nIn this paper, we introduce MaskComp, a novel approach for object completion. MaskComp addresses the object completion task by seamlessly integrating conditional generation and segmentation, capitalizing on the crucial observation that the quality of generated objects is intricately tied to the quality of the conditioned masks. We augment the object completion process with an additional mask condition and propose an iterative mask denoising (IMD) process. This iterative approach gradually refines the partial object mask, ultimately leading to the generation of satisfactory objects by leveraging the complete mask as a guiding condition. Our extensive experiments demonstrate the robustness and effectiveness of MaskComp, particularly in challenging scenarios involving heavily occluded objects.\n\n\n\n\n\n\n\n\n", "DeepReviewer": "\n## Summary:\n\nThis paper introduces MaskComp, a novel approach to object completion that iteratively refines object masks through alternating stages of image generation and segmentation. The core idea is to leverage the synergy between these two processes, where the generation stage, conditioned on a partial object and its mask, produces a completed object, and the segmentation stage, applied to the generated image, provides a more accurate mask. This iterative process, termed Iterative Mask Denoising (IMD), progressively improves the mask quality, leading to more precise object completion. The method builds upon ControlNet, a conditional image generation model, and utilizes a pre-trained Stable Diffusion model for the generative component. For the segmentation stage, MaskComp employs the Segment Anything Model (SAM). The authors demonstrate the effectiveness of MaskComp on two datasets, AHP and DYCE, showing superior performance compared to existing methods like ControlNet and Stable Diffusion. The method's key contribution lies in its iterative refinement strategy, which effectively combines generation and segmentation to achieve high-quality object completion. The paper presents a clear methodology, supported by experimental results, and highlights the potential of iterative approaches for addressing complex image editing tasks. However, the paper also acknowledges certain limitations, such as the computational cost of the iterative process and the reliance on paired data for training. Overall, the paper presents a valuable contribution to the field of object completion, offering a novel and effective approach that combines generation and segmentation in a synergistic manner. The results are compelling, but the paper also identifies areas for future research, such as improving the efficiency of the method and reducing its dependence on paired training data. The authors also suggest that the method could be used for other editing tasks such as object layer switching, which could be a promising direction for future work.\n\n\n## Soundness:\n\n3.0\n\n\n## Presentation:\n\n3.0\n\n\n## Contribution:\n\n2.5\n\n\n## Strengths:\n\nI find the core strength of this paper to be the innovative approach of combining image generation and segmentation in an iterative manner for object completion. The proposed Iterative Mask Denoising (IMD) process is a novel and effective way to progressively refine object masks, leading to superior completion results. The idea of using the generated image to improve the mask, and then using the improved mask to generate a better image, is a clever way to leverage the strengths of both generative and segmentation models. The paper clearly articulates this methodology, making it easy to understand the core contribution. The experimental results, which demonstrate the superiority of MaskComp over existing methods like ControlNet and Stable Diffusion, provide strong empirical support for the proposed approach. The authors have conducted sufficient experiments on two datasets, AHP and DYCE, to validate their claims. The paper is also well-written and easy to follow, which enhances its accessibility and impact. The authors have clearly explained the motivation behind their work, the details of their method, and the results of their experiments. The use of ControlNet as a base model, while also a weakness, is also a strength in that it provides a strong foundation for the proposed method. The authors have made fundamental modifications to ControlNet to enable object completion, which demonstrates their understanding of the underlying techniques. The use of Stable Diffusion as the generative model also contributes to the strength of the method, as it is a powerful and widely used model. The authors have also clearly stated the limitations of their method, which is a sign of good scientific practice. Overall, I believe the paper presents a significant contribution to the field of object completion, offering a novel and effective approach that combines generation and segmentation in a synergistic manner. The results are compelling, and the paper is well-written and easy to follow. The authors have clearly articulated their contribution and have provided strong empirical support for their claims.\n\n\n## Weaknesses:\n\nAfter a thorough examination of the paper, I have identified several key weaknesses that warrant careful consideration. First, the proposed method's efficiency is a significant concern. The iterative nature of the IMD process, which involves alternating between generation and segmentation stages, introduces a substantial computational overhead. As the paper states, \"The IMD process is repeated for a fixed number of steps (T), with each step involving the generation and segmentation stages.\" (Method -> Iterative Mask Denoising (IMD) Process). The experimental section further reveals that, during inference, the IMD process is conducted for 5 steps, with 5 images generated for each step. The inference time for each component per IMD step is 14.3 seconds for generation and 1.2 seconds for segmentation, resulting in a total of 15.5 seconds per step. This means that the total inference time for the entire process is 77.5 seconds per image. This computational cost is substantial and could limit the practical application of the method in real-time scenarios or when processing large datasets. This is a valid concern, and I have high confidence in this assessment, based on the explicit description of the iterative process and the provided inference times. Second, the method's reliance on paired data for training is a significant limitation. While the paper does not explicitly state that paired data is required, the problem formulation and the training process strongly suggest that paired or near-paired data is used. The paper states, \"We address the object completion task, wherein the objective is to predict the image of a complete object $I_c$ based on its visible (non-occluded) part $I_p$.\" (Method -> Problem Definition and Key Insight). This implies a mapping from partial to complete objects, which typically requires paired data. Furthermore, the training is conducted on the AHP dataset and a filtered subset of OpenImage, both of which likely contain paired or near-paired information. This reliance on paired data restricts the applicability of the method in real-world scenarios where such data is often unavailable. This is a partially valid concern, and I have medium confidence in this assessment, based on the problem formulation and the training datasets used. Third, the method's dependence on a pre-trained object completion method, specifically ControlNet, raises concerns about the fairness of comparisons with methods that do not use such priors. The paper states, \"We introduce \\textbf{\\textsl{CompNet}} as the generative model $\\mathcal{G}$ which aims to recover complete objects based on partial conditions. We build CompNet based on popular ControlNet \\cite{zhang2023adding} while making fundamental modifications to enable object completion.\" (Method -> Generation Stage). The performance of MaskComp is therefore influenced by the quality of the base model, Stable Diffusion, which is used in a frozen state. The comparison with methods that are not based on ControlNet, such as Kandinsky 2.1 and Stable Diffusion, may not be entirely fair, as MaskComp benefits from the pre-training of ControlNet. This is a valid concern, and I have high confidence in this assessment, based on the explicit statement about building CompNet on ControlNet. Fourth, the method's requirement for ground-truth text prompts is a significant limitation for real-world applications. The paper mentions using BLIP to generate text prompts for training ControlNet, stating, \"For a more generalized setting, we train the CompNet on a subset of the OpenImage \\citep{kuznetsova2020open} dataset for 36 epochs. We generate text prompts using BLIP \\citep{li2022blip} for all experiments (prompts are necessary to train ControlNet).\" (Experiments -> Implementation details). The fact that all baseline methods are given the same text prompts during experiments further supports the claim that the method relies on these prompts. While the method description itself doesn't explicitly mention text prompts as input to the generation stage, the reliance on ControlNet and Stable Diffusion, which are text-conditional, makes this a valid concern. Obtaining such prompts is not always feasible in real-world scenarios, limiting the method's applicability. This is a valid concern, and I have high confidence in this assessment, based on the mention of using BLIP for prompt generation and the statement about prompts being necessary for training ControlNet. Finally, the experimental evaluation is limited by the choice of baselines. The paper compares MaskComp with ControlNet, Kandinsky 2.1, and Stable Diffusion, which are not specifically designed for object completion. The lack of comparisons with recent object completion methods is a significant weakness, as it does not fully demonstrate the method's superiority over state-of-the-art techniques in this specific domain. This is a valid concern, and I have high confidence in this assessment, based on the list of baselines in the experimental section and the absence of comparisons with dedicated object completion methods. Additionally, the evaluation is primarily focused on human-related datasets and relatively simple objects, which does not fully demonstrate the method's robustness and scalability. Testing on more intricate objects with varying shapes, textures, and occlusion patterns is necessary to validate its broader applicability. This is a partially valid concern, and I have medium confidence in this assessment, based on the description of the datasets used and the visual examples provided in the paper.\n\n\n## Suggestions:\n\nBased on the identified weaknesses, I propose several concrete and actionable improvements. First, to address the efficiency issue, future work should explore techniques to reduce the number of iterations required for convergence. This could involve incorporating more sophisticated mask refinement strategies or using more efficient generative and segmentation models. For example, instead of a fixed number of iterations, the process could terminate when the change in mask quality between iterations falls below a certain threshold. Additionally, investigating the possibility of parallelizing the generation and segmentation steps could also lead to significant speedups. The current approach processes each stage sequentially, which may not be optimal. Exploring methods to perform these steps concurrently or with minimal dependencies could lead to significant speedups. Second, to mitigate the reliance on paired data, future research should explore methods to reduce or eliminate this requirement. This could involve exploring self-supervised learning methods or leveraging existing large-scale datasets with unpaired data. Techniques like cycle-consistency or adversarial training could be explored to learn the mapping between partial and complete objects without requiring explicit paired examples. Furthermore, investigating the use of synthetic data or data augmentation techniques could also help to mitigate the need for large amounts of real-world paired data. The authors should also consider exploring the use of pre-trained models or transfer learning to reduce the need for large datasets. Third, to address the dependence on ControlNet, future work should explore methods to reduce the reliance on this specific pre-trained model. This could involve exploring alternative base models or developing a method that is less dependent on a specific pre-trained architecture. A more rigorous evaluation would involve comparing MaskComp against methods that start from a similar baseline, or alternatively, providing a detailed ablation study that isolates the contribution of the IMD pipeline from the base model. Furthermore, the paper should explore the sensitivity of MaskComp to the choice of the base model. For example, how would MaskComp perform with different base models, and what are the trade-offs in terms of computational cost and performance? This would provide a more comprehensive understanding of the method's robustness and generalizability. Fourth, to address the requirement for ground-truth text prompts, the authors should acknowledge that obtaining such prompts is not always feasible and explore alternative approaches to address this issue. For instance, the authors could investigate the use of automatically generated text prompts or explore methods that do not rely on text prompts at all. The paper should also discuss the impact of inaccurate or misleading text prompts on the performance of MaskComp. How does the method behave when the provided text prompt does not accurately describe the object in the image? This analysis would provide a more realistic assessment of the method's applicability in practical scenarios. Additionally, the authors could consider incorporating techniques for prompt refinement or correction to mitigate the impact of noisy prompts. Finally, to improve the evaluation, the authors should include comparisons with more recent and specialized object completion methods. This would provide a more comprehensive evaluation of MaskComp's performance and demonstrate its advantages over state-of-the-art object completion techniques. The authors should also consider using a wider range of evaluation metrics that are specifically designed for object completion, such as metrics that measure the accuracy of the completed object's shape and appearance. This would provide a more detailed and nuanced assessment of the method's performance. The evaluation should also be expanded to include more complex objects and scenes. This could involve using datasets with a wider variety of object categories, including those with intricate shapes, textures, and occlusion patterns. The authors should also consider evaluating the method's performance on scenes with multiple objects and varying levels of occlusion. This would provide a more comprehensive assessment of the method's robustness and scalability. Additionally, the authors should analyze the failure cases of the method and discuss the limitations of the current approach. This would help in identifying areas for future improvement and provide a more balanced view of the method's capabilities.\n\n\n## Questions:\n\nBased on my analysis, I have several questions that I believe are crucial for a deeper understanding of the method. First, what is the efficiency of the proposed method in terms of processing time per image, and how does this compare to existing methods? While the paper provides the inference time per IMD step, a more direct comparison with other methods would be beneficial. Second, how does the proposed method perform on other datasets, such as the dataset used in ControlNet, or other datasets specifically designed for object completion? This would provide a more comprehensive evaluation of the method's generalizability. Third, how does the proposed method handle cases where the partial object is heavily occluded or has significant variations in appearance compared to the complete object? Are there any limitations in terms of the types of objects or scenes that the method can handle effectively? Fourth, what are the specific conditions under which the iterative refinement process fails to converge to a good solution? Are there specific types of objects or occlusions that are particularly challenging for the method? A more thorough analysis of these failure cases would provide valuable insights into the strengths and weaknesses of the proposed approach. Fifth, the paper mentions that the proposed method can be used for other editing tasks such as object layer switching. Can the authors provide more details on how this can be achieved and show some examples of such applications? This would demonstrate the versatility of the method and its potential for broader impact. Finally, how does the computational cost of the proposed method compare to existing methods? Are there any potential optimizations that could be made to improve its efficiency? This is a critical question, given the iterative nature of the method. These questions target key uncertainties and methodological choices, seeking clarification of critical assumptions and limitations.\n\n\n## Rating:\n\n5.5\n\n\n## Confidence:\n\n3.5\n\n\n## Decision:\n\nReject", "other": "\n## Summary:\n\nThis paper introduces MaskComp, a novel approach to object completion that iteratively refines both the object's mask and its visual representation. The core idea revolves around the observation that the quality of mask-conditioned generation is directly tied to the quality of the mask itself. To leverage this, MaskComp employs an Iterative Mask Denoising (IMD) process, which alternates between generating an image based on the current mask and then refining that mask using an off-the-shelf segmentation model. This iterative process allows the method to progressively improve both the mask and the generated object, even in cases of significant occlusion. The method is built upon ControlNet, a conditional image generation framework, and incorporates a time-variant gating operation to modulate the influence of the mask during the diffusion process. The authors demonstrate the effectiveness of MaskComp through extensive experiments on two datasets, AHP and DYCE, showing that it outperforms existing methods in terms of both quantitative metrics and qualitative visual results. The paper also includes a thorough ablation study, exploring the impact of various design choices, such as the number of iterations, the choice of segmentation model, and the use of different conditioning masks. The results indicate that the proposed iterative approach is robust to segmentation errors and can handle various occlusion scenarios. The authors also show that the method can be used for amodal segmentation by comparing it to a baseline that combines a state-of-the-art amodal segmentation method with ControlNet. Overall, the paper presents a well-motivated and effective approach to object completion, with a clear focus on the interplay between mask quality and generative performance. The iterative nature of the method, coupled with the use of off-the-shelf segmentation models, makes it a practical and versatile solution for a range of image editing tasks. The paper is well-written and easy to follow, making it a valuable contribution to the field of image generation and editing.\n\n\n## Soundness:\n\n3.25\n\n\n## Presentation:\n\n3.25\n\n\n## Contribution:\n\n3.0\n\n\n## Strengths:\n\nI find several aspects of this paper to be particularly strong. First, the core idea of iteratively refining both the mask and the generated object is both intuitive and effective. The authors clearly articulate the motivation behind this approach, highlighting the crucial relationship between mask quality and generative performance. This is a key insight that drives the entire method. Second, the experimental results are compelling. The authors demonstrate through extensive experiments on two datasets that MaskComp outperforms existing methods, including ControlNet and various versions of Stable Diffusion. The quantitative results, measured by FID scores, are significantly better than the baselines, and the qualitative results are also impressive, showing that MaskComp can generate realistic and complete objects even in cases of significant occlusion. The ablation study is also well-designed and thorough, exploring the impact of various design choices. This provides valuable insights into the inner workings of the method and demonstrates its robustness. The authors also show that the method is robust to segmentation errors, which is a crucial aspect for practical applications. The use of off-the-shelf segmentation models makes the method versatile and adaptable to different scenarios. Furthermore, the paper is well-written and easy to follow. The authors clearly explain the method and its components, making it accessible to a broad audience. The figures and tables are also well-presented and help to convey the key ideas and results. Finally, the authors have provided the code for their method, which is a valuable contribution to the community and will facilitate further research in this area. The fact that the method is built upon existing models like ControlNet and Stable Diffusion, and uses off-the-shelf segmentation models, makes it a practical and readily applicable solution for various image editing tasks. The authors have successfully demonstrated the effectiveness of their approach and have made a valuable contribution to the field.\n\n\n## Weaknesses:\n\nDespite the strengths of this paper, I have identified several weaknesses that warrant further discussion. First, while the paper demonstrates the effectiveness of MaskComp for object completion, it does not fully explore the potential of the learned model for other tasks. The method is trained specifically for the object completion task, and the experiments primarily focus on this application. While the authors show a comparison to an amodal segmentation baseline, they do not explore the possibility of using the trained model for amodal segmentation directly, or for other tasks such as image inpainting or outpainting. This limits the perceived versatility of the method. The paper states that the model is trained for 50 epochs on the AHP dataset and 36 epochs on a subset of OpenImage, which is a relatively small number of iterations, and I agree with the reviewer that more training iterations might improve the model's performance. Second, the paper lacks a thorough analysis of the failure cases of the method. While the authors present successful examples of object completion, they do not discuss the limitations of the approach or the scenarios where it might fail. This is a crucial aspect for understanding the practical applicability of the method. For example, the paper does not discuss how the method performs when the initial partial mask is inaccurate or when the object being completed is very complex. The paper also does not discuss the potential for the method to generate unrealistic or inconsistent results. This lack of discussion of limitations makes it difficult to fully assess the robustness of the method. Third, the paper does not provide a detailed analysis of the computational cost of the iterative mask denoising (IMD) process. While the authors provide the inference time for each component, they do not discuss the memory requirements or the impact of the number of iterations on the computational cost. This is an important consideration for practical applications, and the paper should provide more details on this aspect. The paper also does not compare the computational cost of MaskComp with other object completion methods. Fourth, the paper does not provide a clear explanation of the time-variant gating operation. While the authors mention that it is used to adjust the importance of conditions during the diffusion steps, they do not provide a detailed analysis of how this operation affects the generation process. The paper states that the gating operation improves the generation quality by 1.3 FID, but it does not provide a more detailed analysis of the impact of this operation. The paper also does not provide a clear explanation of the linear transform used in the gating operation. Fifth, the paper does not provide a clear explanation of the mask voting process. While the authors describe the mathematical formulation of the voting process, they do not provide a more detailed explanation of how this process works and how it affects the quality of the generated masks. The paper also does not discuss the potential limitations of the voting process, such as the possibility of introducing artifacts or biases. Sixth, the paper does not provide a clear explanation of the role of the object encoder and the auxiliary path for predicting the complete object mask. While the authors mention that the encoder captures shape information, they do not provide a more detailed analysis of how this is achieved. The paper also does not discuss the potential limitations of this approach, such as the possibility of the encoder failing to capture the complete shape of the object. Finally, the paper does not provide a clear explanation of the text prompts used in the experiments. While the authors mention that they use class labels as text prompts, they do not provide a more detailed explanation of how these prompts are generated and how they affect the generation process. The paper also does not discuss the potential limitations of using class labels as text prompts, such as the possibility of generating generic or unrealistic results. In summary, while the paper presents a novel and effective approach to object completion, it lacks a thorough analysis of the limitations, computational cost, and the impact of various design choices. These weaknesses limit the practical applicability of the method and make it difficult to fully assess its robustness.\n\n\n## Suggestions:\n\nBased on the identified weaknesses, I recommend several concrete improvements for this paper. First, the authors should explore the potential of the learned model for other tasks beyond object completion. Specifically, they should investigate the possibility of using the trained model for amodal segmentation, image inpainting, or outpainting. This would demonstrate the versatility of the method and make it more appealing to a broader audience. The authors could also explore fine-tuning the model on different datasets to assess its generalization capabilities. Second, the authors should conduct a more thorough analysis of the failure cases of the method. This should include a discussion of the scenarios where the method might fail, such as when the initial partial mask is inaccurate or when the object being completed is very complex. The authors should also discuss the potential for the method to generate unrealistic or inconsistent results. This analysis would provide a more balanced view of the method's capabilities and limitations. Third, the authors should provide a more detailed analysis of the computational cost of the iterative mask denoising (IMD) process. This should include a discussion of the memory requirements and the impact of the number of iterations on the computational cost. The authors should also compare the computational cost of MaskComp with other object completion methods. This analysis would help to assess the practical applicability of the method. Fourth, the authors should provide a more detailed analysis of the time-variant gating operation. This should include a discussion of how this operation affects the generation process and how it is related to the diffusion steps. The authors should also provide a more detailed explanation of the linear transform used in the gating operation. Fifth, the authors should provide a more detailed explanation of the mask voting process. This should include a discussion of how this process works and how it affects the quality of the generated masks. The authors should also discuss the potential limitations of the voting process, such as the possibility of introducing artifacts or biases. Sixth, the authors should provide a more detailed explanation of the role of the object encoder and the auxiliary path for predicting the complete object mask. This should include a discussion of how the encoder captures shape information and how the auxiliary path contributes to the overall performance of the method. The authors should also discuss the potential limitations of this approach, such as the possibility of the encoder failing to capture the complete shape of the object. Finally, the authors should provide a more detailed explanation of the text prompts used in the experiments. This should include a discussion of how these prompts are generated and how they affect the generation process. The authors should also discuss the potential limitations of using class labels as text prompts, such as the possibility of generating generic or unrealistic results. In addition to these specific suggestions, I also recommend that the authors conduct more training iterations, as this might improve the model's performance. Overall, these suggestions are aimed at addressing the identified weaknesses and making the paper more complete and robust.\n\n\n## Questions:\n\nI have several questions that arise from my analysis of this paper. First, I am curious about the specific details of the time-variant gating operation. While the paper mentions that it is used to adjust the importance of conditions during the diffusion steps, I would like to understand more about the exact mechanism of this operation. Specifically, how is the linear transform learned, and how does it relate to the diffusion steps? What is the impact of different gating functions on the final results? Second, I am interested in the details of the mask voting process. While the paper provides the mathematical formulation of this process, I would like to understand more about how it works in practice. How does the voting process handle cases where there are multiple distinct object parts in the generated images? What is the impact of different thresholds on the final mask quality? Third, I would like to understand more about the role of the object encoder and the auxiliary path for predicting the complete object mask. How does the encoder capture shape information from the partial object and mask? What is the impact of the auxiliary path on the overall performance of the method? How does the auxiliary path interact with the main diffusion process? Fourth, I am curious about the choice of text prompts used in the experiments. While the paper mentions that class labels are used as text prompts, I would like to understand more about how these prompts are generated and how they affect the generation process. What is the impact of using different types of text prompts, such as more descriptive or ambiguous prompts? Fifth, I would like to understand more about the limitations of the method. What are the scenarios where the method is likely to fail? How does the method perform when the initial partial mask is inaccurate or when the object being completed is very complex? What is the potential for the method to generate unrealistic or inconsistent results? Finally, I would like to understand more about the potential for extending the method to other tasks beyond object completion. How could the learned model be used for amodal segmentation, image inpainting, or outpainting? What are the challenges and opportunities for extending the method to these other tasks? These questions are aimed at clarifying some of the key uncertainties in the paper and at exploring the potential for future research directions.\n\n\n## Rating:\n\n6.5\n\n\n## Confidence:\n\n3.75\n\n\n## Decision:\n\nAccept", "other_type": "DeepReviewer_fast", "v.s.": "B", "result": "**Technical Accuracy - Evidence Analysis:**\n- From Assistant A:\n  - Claim 1: \"The core idea of iteratively refining both the mask and the generated object is both intuitive and effective.\"\n    - Evidence:  Assistant A refers to the paper's description of IMD and its motivation based on the relationship between mask quality and generation quality.\n    - Analysis: This claim accurately reflects the core contribution of the paper, which is the IMD process. The paper emphasizes the iterative refinement of masks and images.\n  - Claim 2: \"The experimental results are compelling... MaskComp outperforms existing methods, including ControlNet and various versions of Stable Diffusion.\"\n    - Evidence: Assistant A points to the quantitative results (FID scores) and qualitative results presented in the paper, and the ablation study.\n    - Analysis: This is consistent with the paper's \"Main Results\" section and Table 2, which shows lower FID scores for MaskComp compared to baselines. Figure 4 also visually supports this claim.\n  - Claim 3: \"The ablation study is also well-designed and thorough, exploring the impact of various design choices.\"\n    - Evidence: Assistant A refers to the ablation studies mentioned in the paper.\n    - Analysis: The paper includes several ablation studies in Section 5.3 and Tables 1, 3, and 4, examining design choices like mask conditions, occlusion rates, IMD steps, segmentation networks, etc.\n  - Claim 4: \"The use of off-the-shelf segmentation models makes the method versatile and adaptable to different scenarios.\"\n    - Evidence: Assistant A notes the use of SAM in the segmentation stage.\n    - Analysis: The paper explicitly mentions using SAM and ablates with other segmentation models like CLIPSeg and SEEM in Table 1a, supporting the claim of versatility.\n\n- From Assistant B:\n  - Claim 1: \"The proposed Iterative Mask Denoising (IMD) process is a novel and effective way to progressively refine object masks, leading to superior completion results.\"\n    - Evidence: Assistant B refers to the paper's description of IMD and its claimed benefits.\n    - Analysis: This accurately reflects the paper's central claim and methodology.\n  - Claim 2: \"The experimental results, which demonstrate the superiority of MaskComp over existing methods like ControlNet and Stable Diffusion, provide strong empirical support for the proposed approach.\"\n    - Evidence: Assistant B points to the experimental results showing MaskComp's outperformance.\n    - Analysis: Consistent with the paper's \"Main Results\" section and Table 2, similar to Assistant A's claim.\n  - Claim 3 (Weakness): \"First, the proposed method's efficiency is a significant concern... total inference time for the entire process is 77.5 seconds per image.\"\n    - Evidence: Assistant B quotes inference times from Table 1c (14.3s Gen + 1.2s Segm = 15.5s per step) and multiplies by 5 steps and 5 samples per step (implicitly, although N=5 samples is for segmentation stage, not generation within IMD step, and the 5 steps is correct).  The calculation is slightly off, 15.5s * 5 steps = 77.5s.\n    - Analysis:  Table 1c shows inference time per IMD step is 15.5s. The paper mentions 5 IMD steps.  Assistant B's calculation is based on 5 IMD steps, which is stated in the paper (Implementation Details). The calculation is correct based on the provided numbers, although the reasoning about 5 samples per step is slightly misinterpreting N=5.  The concern about efficiency is valid as the paper itself acknowledges reduced throughput due to multiple diffusion processes.\n  - Claim 4 (Weakness): \"Second, the method's reliance on paired data for training is a significant limitation... implies a mapping from partial to complete objects, which typically requires paired data.\"\n    - Evidence: Assistant B infers paired data requirement from the problem definition and training datasets (AHP, OpenImage).\n    - Analysis: The paper describes training CompNet to recover complete objects from partial objects and masks. While not explicitly stated as *paired* data requirement, the training setup implies the need for corresponding partial and complete object data.  Datasets like AHP and OpenImages, when used for object completion tasks, often involve creating partial views from complete objects, suggesting a paired or near-paired data setup.\n  - Claim 5 (Weakness): \"Third, the method's dependence on a pre-trained object completion method, specifically ControlNet, raises concerns about the fairness of comparisons...\"\n    - Evidence: Assistant B quotes the paper stating \"We build CompNet based on popular ControlNet\".\n    - Analysis: The paper explicitly states building upon ControlNet.  Assistant B's concern about fair comparison is valid, as MaskComp leverages ControlNet's pre-existing capabilities.\n  - Claim 6 (Weakness): \"Fourth, the method's requirement for ground-truth text prompts is a significant limitation for real-world applications... prompts are necessary to train ControlNet.\"\n    - Evidence: Assistant B quotes the paper mentioning text prompts generated by BLIP for training ControlNet and that baselines are given the same prompts.\n    - Analysis: The paper mentions using text prompts for training and evaluation. ControlNet and Stable Diffusion are text-conditional models, so text prompts are indeed relevant.  The concern about needing ground-truth prompts in real applications is valid, although the paper uses BLIP to generate them, suggesting a degree of automation.\n  - Claim 7 (Weakness): \"Finally, the experimental evaluation is limited by the choice of baselines... not specifically designed for object completion.\"\n    - Evidence: Assistant B points out that baselines (ControlNet, Kandinsky, Stable Diffusion) are not *specifically* object completion methods.\n    - Analysis: While ControlNet can be adapted for conditional generation tasks, and Stable Diffusion is a general image generation model, Assistant B is correct that these are not *dedicated* object completion methods in the same way MaskComp is designed to be.  This is a reasonable critique of the baseline selection.\n\n- Comparative Assessment:\n  - Both assistants accurately identify the core method and its claimed strengths.\n  - Assistant B delves deeper into potential weaknesses, accurately pointing out efficiency concerns, possible paired data reliance, dependence on ControlNet, and text prompt requirements. These are all valid points based on the paper's description.\n  - Assistant B's calculation of inference time is slightly off in reasoning (misinterpreting N), but the final number is correct based on the paper's data.\n  - Both assistants' claims are generally well-supported by evidence from the paper. Assistant B provides more critical technical assessments.\n\n**Technical Accuracy - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate good technical accuracy in understanding the paper's core contributions and experimental setup. However, Assistant B goes further by identifying and articulating potential technical limitations and concerns related to efficiency, data requirements, and baseline comparisons, all of which are grounded in the paper's details. Assistant B's analysis of weaknesses demonstrates a more critical and in-depth technical understanding.\n**Better Assistant:** B\n\n---\n**Constructive Value - Evidence Analysis:**\n- From Assistant A:\n  - Suggestions:\n    - \"explore the potential of the learned model for other tasks beyond object completion. Specifically, they should investigate the possibility of using the trained model for amodal segmentation, image inpainting, or outpainting.\"\n      - Analysis: This is a reasonable and actionable suggestion to broaden the impact and demonstrate versatility.\n    - \"conduct a more thorough analysis of the failure cases of the method.\"\n      - Analysis:  Standard constructive feedback to improve robustness understanding.\n    - \"provide a more detailed analysis of the computational cost of the iterative mask denoising (IMD) process... compare the computational cost of MaskComp with other object completion methods.\"\n      - Analysis:  Actionable suggestion to address a practical concern.\n    - \"provide a more detailed analysis of the time-variant gating operation... mask voting process... role of the object encoder and the auxiliary path... text prompts used in the experiments.\"\n      - Analysis: These are all requests for more in-depth analysis and explanation of specific components, which are constructive for improving clarity and understanding.\n    - \"conduct more training iterations, as this might improve the model's performance.\"\n      - Analysis:  A standard suggestion for potentially improving performance.\n\n- From Assistant B:\n  - Suggestions:\n    - \"explore techniques to reduce the number of iterations required for convergence... parallelizing the generation and segmentation steps.\"\n      - Analysis:  Specific and actionable suggestions to improve efficiency.\n    - \"explore methods to reduce or eliminate this requirement [paired data]... self-supervised learning methods or leveraging existing large-scale datasets with unpaired data... cycle-consistency or adversarial training... synthetic data or data augmentation... pre-trained models or transfer learning.\"\n      - Analysis:  Provides a range of concrete techniques to address the paired data concern.\n    - \"explore methods to reduce the reliance on this specific pre-trained model [ControlNet]... alternative base models... ablation study that isolates the contribution of the IMD pipeline from the base model... sensitivity of MaskComp to the choice of the base model.\"\n      - Analysis:  Actionable suggestions to address dependence on ControlNet and improve evaluation rigor.\n    - \"acknowledge that obtaining such prompts is not always feasible and explore alternative approaches... automatically generated text prompts or explore methods that do not rely on text prompts at all... impact of inaccurate or misleading text prompts... techniques for prompt refinement or correction.\"\n      - Analysis:  Provides concrete ways to address the text prompt dependency.\n    - \"include comparisons with more recent and specialized object completion methods... wider range of evaluation metrics... expanded to include more complex objects and scenes... analyze the failure cases of the method.\"\n      - Analysis:  Actionable suggestions to improve evaluation comprehensiveness and rigor.\n\n- Comparative Assessment:\n  - Both assistants provide constructive suggestions.\n  - Assistant B's suggestions are generally more specific and actionable. For example, instead of just saying \"analyze computational cost,\" Assistant B suggests \"parallelizing generation and segmentation steps.\" Instead of just \"reduce paired data reliance,\" Assistant B lists specific techniques like \"self-supervised learning,\" \"cycle-consistency,\" etc.\n  - Assistant B's suggestions are more directly tied to the weaknesses identified in their review, making them highly relevant and targeted.\n  - Assistant A's suggestions are also constructive but tend to be broader and less detailed in terms of concrete actions.\n\n**Constructive Value - Judgment:**\n**Evidence-Based Reason:** Both assistants offer valuable suggestions for improvement. However, Assistant B's feedback is more actionable and specific, providing concrete techniques and directions for the authors to address the identified weaknesses. Assistant B's suggestions are also more directly linked to the limitations they pointed out, making the feedback more targeted and impactful.\n**Better Assistant:** B\n\n---\n**Analytical Depth - Evidence Analysis:**\n- From Assistant A:\n  - Analysis focuses on:\n    - Core idea (iterative refinement)\n    - Experimental results (quantitative and qualitative)\n    - Ablation study\n    - Strengths (intuitive idea, compelling results, thorough ablation, versatility, well-written)\n    - Weaknesses (limited task exploration, failure case analysis, computational cost analysis, gating operation explanation, mask voting explanation, object encoder/auxiliary path explanation, text prompt explanation)\n  - Depth:  Provides a good overview of the paper's strengths and weaknesses, covering key aspects like methodology, experiments, and presentation.  Identifies several areas for deeper analysis in the paper.\n\n- From Assistant B:\n  - Analysis focuses on:\n    - Core idea (IMD process)\n    - Strengths (novelty of IMD, empirical support, clarity, ControlNet base)\n    - Weaknesses (efficiency, paired data reliance, ControlNet dependence, text prompt requirement, baseline limitations, evaluation scope)\n    - Questions (efficiency comparison, dataset generalizability, handling heavy occlusion, convergence failure, layer switching details, computational cost comparison)\n  - Depth:  Provides a more in-depth analysis of potential limitations, focusing on efficiency, data dependencies, model dependencies, and evaluation rigor.  Questions are targeted at probing deeper into these limitations and seeking further clarification.  The analysis of weaknesses is more structured and detailed than Assistant A's.\n\n- Comparative Assessment:\n  - Both assistants demonstrate analytical depth by examining key aspects of the paper.\n  - Assistant B's analysis is arguably deeper in its critical evaluation of potential limitations. Assistant B's weaknesses are more focused and structured, delving into specific concerns about the method's practical applicability and evaluation rigor.\n  - Assistant B's questions also reflect a deeper analytical engagement with the paper, probing into specific areas of concern and seeking more detailed explanations.\n  - While Assistant A provides a good overview, Assistant B's analysis is more focused on critical evaluation and identifying potential shortcomings.\n\n**Analytical Depth - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate analytical depth. However, Assistant B's review exhibits a greater analytical depth by focusing more critically on potential limitations and practical concerns. Assistant B's structured analysis of weaknesses and targeted questions demonstrate a more thorough and probing examination of the paper's methodology and evaluation.\n**Better Assistant:** B\n\n---\n**Communication Clarity - Evidence Analysis:**\n- From Assistant A:\n  - Clarity:  Well-written, clear summary and points. Uses bullet points for strengths and weaknesses, and numbered suggestions.  Language is easy to understand.\n  - Specificity:  Suggestions are reasonably specific, though sometimes broad (e.g., \"more detailed analysis\"). Strengths and weaknesses are clearly articulated.\n  - Reasoning: Reasoning is generally well-explained, connecting claims to evidence from the paper.\n  - Examples: Uses examples by referring to specific aspects of the paper (e.g., ablation study, ControlNet).\n\n- From Assistant B:\n  - Clarity:  Well-written, clear summary and points. Uses bullet points for strengths, weaknesses, suggestions, and questions. Language is easy to understand.\n  - Specificity:  Suggestions are highly specific and actionable, listing concrete techniques and directions. Weaknesses are clearly and specifically articulated with justifications. Questions are targeted and specific.\n  - Reasoning: Reasoning is clearly explained, with justifications for weaknesses and suggestions.  Connects claims to evidence from the paper and logical deductions.\n  - Examples: Uses examples by referring to specific parts of the method (IMD process, ControlNet) and experimental details (inference time).\n\n- Comparative Assessment:\n  - Both assistants communicate clearly and effectively.\n  - Assistant B's communication is slightly clearer in terms of structure and organization, using bullet points effectively for each section (strengths, weaknesses, suggestions, questions).\n  - Assistant B's language is also very precise and direct in articulating weaknesses and suggestions.\n  - Both use examples effectively, but Assistant B's examples are often more specific and directly tied to the identified weaknesses and suggested improvements.\n\n**Communication Clarity - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate excellent communication clarity. Both reviews are well-written, clearly structured, and easy to understand. However, Assistant B's review is slightly better in communication clarity due to its more structured organization, highly specific and actionable suggestions, and precise language in articulating weaknesses and recommendations.\n**Better Assistant:** B\n\n---\n**Comprehensive Analysis:**\nAssistant A provides a good, balanced review, summarizing the paper effectively, highlighting strengths and weaknesses, and offering constructive suggestions. Assistant A appreciates the novelty and effectiveness of the method and acknowledges the thoroughness of the experiments. However, the weaknesses and suggestions are somewhat general.\n\nAssistant B provides a more critical and in-depth review. While also acknowledging the strengths, Assistant B focuses more on potential limitations and practical concerns related to efficiency, data dependencies, model dependencies, and evaluation rigor. Assistant B's weaknesses are more specific and well-justified, and the suggestions are more actionable and concrete, directly addressing the identified limitations. Assistant B's questions are also more targeted and probing, seeking deeper clarification on key aspects.\n\nAssistant B's review demonstrates a more rigorous and critical analysis of the paper's technical aspects and practical implications. While both reviews are valuable, Assistant B's review offers a more insightful and actionable critique.\n\n**Overall Judgment:**\n**Evidence-Based Reason:** Assistant B's review is stronger overall because it demonstrates a deeper technical understanding, provides more specific and actionable feedback, and offers a more critical and insightful analysis of the paper's limitations and potential improvements. While both reviews are competent, Assistant B's review provides a more valuable and rigorous assessment that would be more helpful to the authors in improving their work. Assistant B consistently provides more detailed evidence and reasoning for their judgments across all aspects.\n**Better Assistant:** B"}
{"id": "sqRgz88TM3", "year": 2024, "paper_context": "\\title{VFLAIR: A Research Library and Benchmark for Vertical Federated Learning}\n\n\\begin{abstract}\n  Vertical Federated Learning (VFL) has emerged as a collaborative training paradigm that allows participants with different features of the same group of users to accomplish cooperative training without exposing their raw data or model parameters. VFL has gained significant attention for its research potential and real-world applications in recent years, but still faces substantial challenges, such as in defending various kinds of data inference and backdoor attacks. Moreover, most of existing VFL projects are industry-facing and not easily used for keeping track of the current research progress. We also benchmark $11$ attacks and $8$ defenses performance under different communication and model partition settings and draw concrete insights and recommendations on the choice of defense strategies for different practical VFL deployment scenarios. \n  \n\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\nThe concept of Federated Learning (FL) was first introduced by Google in 2016~\\citep{McMahan2016fl} describing a cross-device scenario where millions of mobile users collaboratively train a shared model using their local private data\nwithout centralizing these data. This scenario is regarded as Horizontal FL (HFL)~\\citep{yang2019federatedbook} as data are partitioned by sample. In another type of FL, regarded as Vertical FL (VFL)~\\citep{yang2019federatedbook}, data are partitioned by feature. VFL is often applied in industrial collaborative learning scenarios where each organization controls disjoint features of a common group of users. In VFL, local data and local model are kept private at each participant. Instead, local model outputs and their gradients are transmitted between parties.\n\nVFL has drawn increasing attention from both academic and industry in recent years with hundreds of research papers published every year and a number of open-sourced projects released, including FATE~\\citep{FATE,liu2021fate}, Fedlearner~\\citep{Fedlearner}, PaddleFL~\\citep{PaddleFL}, Pysyft~\\citep{ryffel2018generic,romanini2021pyvertical}, FedTree~\\citep{fedtree2022}, and FedML~\\citep{he2020fedml}. Real-world industrial cases are also emerged in the field of advertising~\\citep{cai2020Bytedance,tencent2021} and finance~\\citep{cheng2020federated,Cheng2022DigitalEra} etc. However, mainstream VFL projects such as FATE are industrial grade and not designed for keeping up with research advances.  %, to name a few.\n\nMeanwhile, research interests for VFL have been growing rapidly over the past years, focusing on improving various aspects of VFL protocols, such as communication efficiency~\\citep{fu2021vf2boost,liu2022fedbcd,castiglia2022compressed,fu2022towards}, robustness to attacks~\\citep{liu2021rvfr,cheng2021secureboost,li2022label,zou2022defending,zou2023mutual,sun2022label,yang2022differentially}, model utility~\\citep{li2022semi,Yitao2022multiview,feng2020multi,feng2022semisupervised}, and fair incentive designs~\\citep{liu2021achieving,qi2022fairvfl}. % are two main lines of reserach work considering VFL. \nFor communication efficiency, methods like decrease communication rounds using multiple local updates between each round~\\citep{liu2022fedbcd,fu2022towards} or compress information~\\citep{castiglia2022compressed} have been proposed. As for data security and privacy, various attacks injected by one or multiple parties aiming to either steal other parties' private label~\\citep{li2022label,fu2021label,zou2022defending}, private features~\\citep{jin2021cafe,luo2021feature,li2022ressfl,jiang2022comprehensive,ye2022feature}, sensitive attributes~\\citep{Song2020Overlearning} and sample relations~\\citep{qiu2022your}, or negatively impact the model behavior~\\citep{liu2021rvfr,zou2022defending} have been put forward. Multiple defending methods have also been proposed to tackle these threats, including adding noise~\\citep{dwork2006DP,zou2022defending,li2022label}, sparsifying gradients~\\citep{aji2017sparse,fu2021label,zou2022defending}, discreting gradients~\\citep{fu2021label}, label differential privacy~\\citep{ghazi2021deep,yang2022differentially}, adding distance correlation regularizor~\\citep{sun2022label,vepakomma2019reducing}, disguising labels~\\citep{zou2022defending}, adding mutual information regularizer~\\citep{zou2023mutual}, adversarial training~\\citep{sun2021defending,li2022ressfl} or performing robust feature recovery~\\citep{liu2021rvfr}. However, each of these defenses are evaluated under specific tasks and settings, lacking of key insights and metrics on evaluating these defense strategies to defend all possible attacks in practical deployment.\n\nTo facilitate future research for VFL, we introduce a lightweight and comprehensive VFL framework, namely \\verb|VFLAIR|, which includes not only basic VFL training and inference for a variety of models and settings but also efficiency enhancement techniques and multiple defense methods that mitigate potential threats. Moreover, we perform extensive experiments on combinations of the above settings using multiple datasets to provide different perspectives on VFL efficiency and safety. We believe \\verb|VFLAIR| and these benchmark results will provide researchers with useful tools and guidance for their future work. Our contributions are summarized in the following:\n\n\\textbf{(1).} We design \\verb+VFLAIR+, a lightweight and extensible VFL framework that aims to facilitate research development of VFL (see \\cref{fig:VFLAIR}). We design standardized pipelines for VFL training and validation, supporting $13$ datasets, $29$ different local model architectures including linear regression, tree and neural networks, $6$ different global models, $2$ model partition settings, $5$ communication protocols, $1$ encryption method, $11$ attacks and $8$ defense methods, each implemented as a distinct module and can be easily extended.\n\n\\textbf{(2).} We propose new evaluation metrics and modules, and perform extensive experiments to benchmark various perspectives of VFL, from which we draw key insights on VFL system design choice, in order to promote future development and practical deployment of VFL. % Performance of VFL models trained with standard VFL training, attack performance of $11$ attacks, defense capability of $8$ defense methods are evaluated under both aggVFL and splitVFL with communication protocol using both FedSGD and FedBCD.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.89\\linewidth]{figures/VFLAIR_7_5.pdf}\n    \\caption{{An overview of the Components of VFLAIR.}} % After specifying the settings of VFL system, attacks and defense methods can be evaluated on top of the system. Overall evaluation metrics can be further exploit to get recommendation for selecting a proper defense.\n\\label{fig:VFLAIR}\n\\end{figure}\n\n\n\n\\section{Related Work}\n\n\nA number of open-source FL projects have been developed supporting VFL. FATE~\\citep{FATE,liu2021fate} is an industry-grade FL project which supports a variety of model architectures and secure computation protocols; Fedlearner~\\citep{Fedlearner} is specialized in advertising scenarios; PaddleFL~\\citep{PaddleFL} supports 2-party and 3-party VFL with MPC protection; Pysyft~\\citep{ryffel2018generic,romanini2021pyvertical} introduces PyVertical, which focus on SplitNN-type of VFL settings; FedTree~\\citep{fedtree2022} focuses on tree-based VFL only; FedML~\\citep{he2020fedml} supports basic training of VFL with logistic regression models. Real-world industrial applications have been witnessed in domains such as advertising~\\citep{cai2020Bytedance,tencent2021} and finance~\\citep{cheng2020federated,Cheng2022DigitalEra}. These works demonstrate the widespread interest and the practical significance of VFL. \nHowever, these works are often relatively heavy-weight as they are designed for industrial deployment. \nOn the other hand, most existing benchmarks on FL focus on HFL scenario~\\citep{chai2020fedeval,lai2022fedscale,zhang2023fedaudio}. For VFL, ~\\citep{kang2022framework} evaluates several defense strategies for data reconstruction attacks; SLPerf~\\citep{zhou2023slperf} focuses on benchmarking and comparing various kinds of splitNN scenarios like splitVFL. No existing work provides a comprehensive evaluation covering a variety of key aspects of VFL settings, including model performance, communication efficiency and robustness to attacks. Due to space limitation, %{\\color{red}{we only include previous works on VFL Framework and Benchmarks here}} but \n} \\tianyuan{No, at least currently, no.}\n\n\n\n\\section{VFL Framework}\n\nIn a typical VFL setting with $K$ parties, each party owns their local private feature $\\{X_{k}\\}_{k=1}^K$  and local model $\\{G_{k}\\}_{k=1}^K$ with parameters $\\{\\theta_{k}\\}_{k=1}^K$ respectively. Only one party controls the private label information $Y$ and is referred to as \\textit{active} party while other parties are referred to as \\textit{passive} parties.\nThe active party also controls a global trainable model parameterized by $\\varphi$ (splitVFL) or global non-trainable function $F$ (aggVFL) to aggregate each party's local model output. Note in tree-based VFL the global function is an aggregation function that identifies the optimal feature split based on feature splitting information received from all parties. %The classification task is also regarded as the main task since it is the purpose for the collaboration between parties and reflects the utility of the VFL model. \nWithout loss of generality, we assume that the $K^{th}$ party is the active party while other $K-1$ parties are passive parties.\nIn the collaborative training process of NN-based VFL, each party computes its local feature embedding $H_k=G_k(X_k,\\theta_k), k=1,\\dots,K$. The active party collects $\\{H_k\\}_{k=1}^K$ and gets the final prediction $\\hat{Y}=F(H_1,\\dots,H_K, \\varphi)$. %Based on whether $F$ is a trainable model or a non-trainable function, we follow~\\citep{liu2022vertical} to categorize VFL systems into splitVFL and aggVFL separately. \nThe loss $\\mathcal{L}=\\ell(Y,\\hat{Y})$ is calculated at the active party. The gradient w.r.s. to $H_k$ as $g_k = \\frac{\\partial \\mathcal{L}}{\\partial H_k}, k=1,\\dots,K$ are then calculated and transmitted back to each party by the active party. Using these gradients, each party performs local model updates by SGD using $\\nabla_{\\theta_k}\\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial \\theta_k}=\\frac{\\partial \\mathcal{L}}{\\partial H_k}\\frac{\\partial H_k}{\\partial \\theta_k}, k=1,\\dots,K$. Also the active party performs model update with SGD on global model $F$ if it is trainable using $\\nabla_{\\varphi}\\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial \\varphi}$. In the inference procedure, the same is done but without the backward gradient descent to get the prediction of labels. If the exchange of $H_k$ and $g_k$ is performed each round, such VFL protocol is referred as FedSGD protocol. On the other hand, if communication is done every $Q>1$ steps of local updates, such protocol is referred to as FedBCD~\\citep{liu2022fedbcd}. %{\\color{red}{Also, communication protocols that quantifies $H_k$ (using only $2^{b}$ bits for each element), or sparsifies $H_k$ by preserving only Top-k useful elements (using only $0<r<1$ proportion of the total elements), before information transmission to reduce communication cost, are referred to as Quantize~\\citep{castiglia2022compressed} and Top-k~\\citep{castiglia2022compressed} respectively. Further, cached FedBCD communication protocol using different batches of data for local update steps are termed CELU-VFL~\\citep{fu2022towards}.}}\n\n\n\n\n\\section{Overview of VFLAIR}\n\n\n\\textbf{Implemented Components.} An overview of the components of \\verb|VFLAIR| is shown in \\cref{fig:VFLAIR}.\n\\verb|VFLAIR| incorporates not only basic VFL training and testing process for both NN-based and tree-based VFL of various settings, but also multiple existing efficiency enhancement techniques, data leakage and model utility impairing attacks as well as defending methods that aim to mitigate potential threats. \\verb|VFLAIR| provides support for both aggVFL and splitVFL with easily adjustable model architectures. %For tree-based VFL, \\verb|VFLAIR| accommodates XGBoost and Random Forest for aggregation. \nCurrently, \\verb|VFLAIR| supports $5$ communication protocols %including FedSGD (vanilla protocol), FedBCD~\\citep{liu2022fedbcd}, CELU-VFL~\\citep{fu2022towards}, Quantize~\\citep{castiglia2022compressed} and Top-k~\\citep{castiglia2022compressed} \nto improve communication efficiency. Also, $11$ existing attacks and $8$ \ndefenses are supported. Moreover, \\verb|VFLAIR| supports the comprehensive assessment of defense performance using carefully designed metrics (see \\cref{sec:metrics_definition}), based on which defense strategy recommendations can be provided. Paillier Encryption~\\citep{cheng2021secureboost} is also supported to further protect transmitted results.\nIn total, $13$ datasets from  a diverse range of industrial domains, including but not limited to medical, financial, and recommendation\nare supported.\n\n\\textbf{How to use and extend.} \\verb|VFLAIR| is a light-weight and comprehensive VFL framework that can be launched on a single GPU or CPU (see \\cref{tab:hardware} for its system requirement compared to FATE). \\verb|VFLAIR| facilitates the easy integration of different datasets for model training and inference through simple dataset loading and partitioning functions. New attacks and defenses can be quickly incorporated into the framework thanks to the modular structure.  % VFL model utility and its susceptibility to attacks under various defense strategies by using $5$ carefully designed evaluation metrics. Furthermore,\n\n\n\n\\section{VFL Benchmark}\n\n\n\\subsection{VFL Settings, Models and Datasets} \nUsing \\verb|VFLAIR|, We benchmark the VFL main task performance using $13$ datasets including MNIST~\\citep{MNISTdataset}, CIFAR10~\\citep{krizhevsky2009learning}, CIFAR100~\\citep{krizhevsky2009learning}, NUSWIDE~\\citep{NUSWIDEdataset}, Breast Cancer~\\citep{street1993nuclear}, Diabetes~\\citep{Diabetes1999dataset}, Adult Income~\\citep{AdultIncome1996dataset}, Criteo~\\citep{guo2017deepfm_Criteo}, Avazu~\\citep{qu2018product_Avazu}, Cora~\\citep{mccallum2000automating_Cora}, News20~\\citep{lang1995News20},Credit~\\citep{Dua:2019} and Nursery~\\citep{Dua:2019}.\n\nWe explore $2$ distinct architectures, namely aggVFL and splitVFL, and comprehensively benchmark their performance. For global model $F$, a global softmax function is applied under aggVFL setting while a 1-layer fully-connected model serves as the global model for splitVFL setting (except for Cora dataset, for which a 1-layer graph convolution layer is applied). \nAdditionally, we investigate the impact of different communication protocols by comparing FedBCD~\\citep{liu2022fedbcd} ($Q=5$) and CELU-VFL~\\citep{fu2022towards} ($Q=5,W=5$), as well as compression mechanisms Quantize ($b=16$)~\\citep{castiglia2022compressed} and Top-k ($r=0.9$)~\\citep{castiglia2022compressed} to the conventional FedSGD, as discussed in \\cref{sec:vfl_setting} and further provide insights into the communication cost reduction achieved by communication efficient protocols,\nas well as the impact of FedBCD when various attacks and defenses are deployed. % demonstrating its effectiveness in enhancing efficiency.\nWe also evaluate the impact of the number of participating parties as well as the type of local model (logistic regression, tree, NN) on the main task performance of VFL. For tee-based VFL, we further benchmark both Random Forrest and XGBoost algorithms. Moreover, for \ntree-based VFL, we employ Paillier Encryption~\\citep{cheng2021secureboost} to protect transmitted information and measure its impact on computation efficiency. %such as labels and gradients. {\\color{red}{We measure the execution time along side the model performance across various tree-based VFL aggregation algorithms with and without the application of Paillier Encryption, shedding light on the execution overhead incurred by encryption.}} \n\n\n\\subsection{Attacks and Defenses} \\label{subsec:attacks}\n \n\\begin{table}[!tb]\n\\caption{Summary of attacks for NN-based VFL}\n\\label{tab:attacks}\n\\resizebox{0.998\\linewidth}{!}{\n    \\begin{tabular}{c|c|c|c|c}\n    \\toprule\n    Attack Type & Attack & \\shortstack{Requirements\\\\/ Limitations} & \\shortstack{Attacker\\\\Party} & Attack Performance (AP) \\\\\n    \\midrule\n    \\multirow{7}{*}{\\shortstack{Label\\\\Inference\\\\(LI)}} & \\shortstack{Norm-based Scoring (NS)~\\citep{li2022label}} & \\multirow{2}{*}{\\shortstack{binary classification,\\\\sample-level}} & \\multirow{7}{*}{passive} & \\multirow{2}{*}{\\shortstack{AUC of\\\\inferred labels}}\\\\\n    \\cline{2-2}\n    \\\\[-1em]\n     & \\shortstack{Direction-based Scoring (DS)~\\citep{li2022label}} &  &  & \\\\\n    \\cline{2-3}\n    \\cline{5-5}\n    \\\\[-1em]\n     & \\shortstack{Direct Label Inference (DLI)~\\citep{li2022label,zou2022defending}} & sample-level &  & \\multirow{5}{*}{\\shortstack{ratio of correctly\\\\inferred labels\\\\ \\,}}\\\\\n    \\cline{2-3}\n    \\\\[-1em]\n    & Batch-level Label Inference (BLI)~\\citep{zou2022defending} & - &  & \\\\\n    \\cline{2-3}\n    \\\\[-1em]\n    & Passive Model Completion (PMC)~\\citep{fu2021label} & \\multirow{2}{*}{\\shortstack{auxiliary labeled\\\\data for each class}} &  & \\\\\n    \\cline{2-2}\n    \\\\[-1em]\n    & Active Model Completion (AMC)~\\citep{fu2021label} &  &  & \\\\\n    \\cline{1-5}\n    \\\\[-1em]\n    \n    \n    \\multirow{3}{*}{\\shortstack{Feature\\\\Reconstruction\\\\(FR)}} & \\shortstack{Generative Regression Network (GRN)~\\citep{luo2021feature}} & \\shortstack{black-box} & \\multirow{3}{*}{\\shortstack{active}} & \\multirow{3}{*}{\\shortstack{$1-\\text{MSE}(U_0,U_{rec})$ }}\\\\\n    \\cline{2-3}\n    \\\\[-1em]\n     & \\shortstack{Training-based Back Mapping by\\\\model inversion (TBM)~\\citep{li2022ressfl}} & \\shortstack{white-box,\\\\auxiliary i.i.d. data} &  & \\\\\n    \\cline{1-5}\n    \\\\[-1em]\n    \\shortstack{Targeted\\\\Backdoor (TB)\\\\ \\,} & \\shortstack{Label Replacement\\\\Backdoor (LRB)~\\citep{zou2022defending}} & \\shortstack{$\\geq1$ sample of\\\\target class} & \\shortstack{passive\\\\ \\,\\\\ \\,} & \\shortstack{ratio of triggered samples\\\\inferred as target class}\\\\\n    \\cline{1-5}\n    \\\\[-1em]\n    \\multirow{2}{*}{\\shortstack{Non-targeted\\\\Backdoor (NTB)}} & Noisy-sample Backdoor (NSB)~\\citep{zou2023mutual} & - & \\multirow{2}{*}{passive} & \\multirow{2}{*}{\\shortstack{MP difference between total\\\\and noisy/missing samples}}\\\\\n    \\cline{2-3}\n    \\\\[-1em]\n     & Missing Feature (MF)~\\citep{liu2021rvfr} & - &  & \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\nWe benchmark the performance of $11$ attacks with $8$ defenses on $3$ datasets including MNIST~\\citep{MNISTdataset}, CIFAR10~\\citep{krizhevsky2009learning} and NUSWIDE~\\citep{NUSWIDEdataset}.\nFor these evaluations, we mainly consider a VFL setting with $1$ active party and $1$ passive party following original works~\\citep{li2022label,luo2021feature,li2022ressfl}, denoted as party $a,p$ respectively, with each party owning their local feature $X_a, X_p$ and local model $G_a, G_p$.\nThe local model output of the active and passive party are denoted as $H_a, H_p$ respectively.\nWe summarized the evaluated attacks in \\cref{tab:attacks}. Note in \\cref{tab:attacks}, NS and DS attacks can only be applied to binary classification scenarios; \"sample-level\" indicates that the attack requires gradient information for each sample, whereas \"batch-level\" means only batch-level gradients information are available; \"black-box\" indicates that the model is kept private at the party under attack, but can be queried by the attacker and honestly return the output to the attacker, whereas \"white-box\" means the attacker has access to the model; $\\text{MSE}(U_0,U_{rec})=\\mathbb{E}[(u_0^{(f)}-u_{rec}^{(f)})^2]$ where $u_0^{(f)},u_{rec}^{(f)}$ are the $f^{th}$ feature of original input $U_0$ and recovered input $U_{rec}$ respectively. LI, FR and NTB attacks are inference time attacks that are launched separately from VFL training procedure while only TB attacks are training time attacks. \n Defense methods are summarized in \\cref{tab:defense_param} with respective hyper-parameters. \n \n\n\\begin{table}[!tb]\n\\caption{Summary of defense methods and tested hyper-parameter values for NN-based VFL.} % \\tianyuan{parameter double check.}\n\\label{tab:defense_param}\n\\resizebox{0.998\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c|c|c|c}\n    \\toprule\n        Defense & Methodology & Hyper-parameter & Hyper-parameter Values \\\\\n    \\midrule\n        G-DP~\\citep{dwork2006DP,fu2021label,zou2022defending} & add noise to gradients or local prediction & DP Strength & $0.0001,0.001,0.01,0.1$\\\\\n        L-DP~\\citep{dwork2006DP,fu2021label,zou2022defending} & add noise to gradients or local prediction & DP Strength & $0.0001,0.001,0.01,0.1$\\\\\n        GS~\\citep{aji2017sparse,fu2021label,zou2022defending} & drop gradient elements close to $0$ & Sparsification Rate & $95.0\\%,97.0\\%,99.0\\%,99.5\\%$\\\\ %\n        GPer~\\citep{yang2022differentially} & perturb gradient with that of other class & Perturbation Strength & $0.0001,0.001,0.01,0.1$\\\\\n        dCor~\\citep{sun2022label,vepakomma2019reducing} & distance correlation regularization & Regularizer Strength & $0.0001,0.01,0.1,0.3$\\\\\n        CAE~\\citep{zou2022defending} & disguise label & Confusion Strength $\\lambda$ & $0.0,0.1,0.5,1.0$\\\\\n        DCAE~\\citep{zou2022defending} & discrete gradient in addition to CAE & Confusion Strength $\\lambda$ & $0.0,0.1,0.5,1.0$\\\\\n    MID~\\citep{zou2023mutual} & mutual information (MI) regularization & Regularizer Strength $\\lambda$ & $0.0,1e^{-8},1e^{-6},1e^{-4},0.01,0.1,1.0,1e^{2},1e^{4}$ \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\n\\subsection{Evaluation Metrics}\\label{sec:metrics_definition}\n\\textbf{Main Task Performance (MP).}\nMP is defined as the final model prediction accuracy on the test dataset, which reveals the utility of the VFL system.\n\n\\textbf{Communication and Computation Efficiency.} Number of communication rounds (\\#Rounds) and the amount of data transferred for each round (Amount) are used for measuring communication efficiency. Execution Time (Exec.Time) is used to measure computation efficiency. \n\n\\textbf{Attack Performance (AP).} %As we also include various kinds of attacks and defense in our platform and benchmark experiments, we define AP so as to evaluate the success rate of a given attack which also reflects the vulnerability of a VFL system to that given attack. \nThe definition of AP varies with respect to the type of the attack and is summarized in \\cref{tab:attacks}. \n\n\\textbf{Defense Capability Score (DCS).} %To evaluate the defense methods, \nIntuitively, an ideal defense should not compromise the utility of the original main task and should thwart the attack completely. Therefore, considering that both AP and MP are key metrics to evaluate defenses. We further propose \\textbf{Defense Capability Score (DCS)}, to directly compare all the defenses under one unified metric. Let $df=(\\text{AP},\\text{MP})$ represents the performance of a defense on an AP-MP graph, then we define its defense capability score (DCS) based on the distance between $df$ to an ideal defense $df^{*}=(\\text{AP}^{*}, \\text{MP}^{*})$.  MP$^{*}$ is the MP of VFL without defense and AP$^{*}$ is set to $0.0$ representing the performance of a completely incapable attacker. Then, we formulate the definition of DCS as:\n\\begin{equation} \\label{eq:dcs}\n \\setlength\\abovedisplayskip{0.05cm}\n \\setlength\\belowdisplayskip{0.05cm}\n    \\text{DCS} = \\frac{1}{1+D(df,df^{*})} = \\frac{1}{1+\\sqrt{(1-\\beta)(\\text{AP}-\\text{AP}^{*})^2+\\beta(\\text{MP}-\\text{MP}^{*})^2}},\n\\end{equation}\n\n\\begin{wrapfigure}{l}{0.36\\textwidth}\n    \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/score/DCS_LI_v4.pdf}\n    \\caption{A visual illustration example of DCS. The numbers on the contour lines are DCSs calculated with $\\beta=0.5$.} % \\yang{make the points larger and font bigger}\n    \\label{fig:dcs}\n\\end{wrapfigure}\n\nwhere $D(\\cdot)$ is a user-defined distance function. Here we use Euclidean distance with an adjustable trade-off weighting parameter $\\beta$. A visualization of DCS on an AP-MP graph with $\\beta=0.5$ can be seen in \\cref{fig:dcs}. %\\yang{add a sketch figure to illustrate the distance}\\tianyuan{DONE, but this figure takes up too much space.}). \nA point closer to the bottom-right corner of an AP-MP graph has a higher DCS score indicating a better defense capability, consistent with intuition. $\\beta=0.5$ is used in our experiments. %Also, $\\hat{m}_{j}^{*}$ denotes the \\textit{ideal MP} defined as the MP of a vanilla VFL without defense, representing no accuracy loss on the main task and $\\hat{a}_{j}^{*}$ denotes the \\textit{ideal AP} defined as the worst AP for each attack (each attack type) as defined above in \\cref{subsubsec:ap_mp} and \\cref{tab:attack_performance_definition}.\n\n\\textbf{Type-level Defense Capability Score (T-DCS).} T-DCS is the DCS score averaged by attack type. %proposed to evaluate the capability of a defense method on a certain type of attack. %we considered in \\cref{subsec:attacks} rather than on a single attack. \nTreating all $I_j$ attacks of the same attack type $j$ as equally important, we average DCS for each attack $i$ to get T-DCS for attack type $j$:% $j\\in \\mathcal{A}$:\n\\begin{equation} \\label{eq:t-dcs}\n \\setlength\\abovedisplayskip{0.05cm}\n \\setlength\\belowdisplayskip{0.05cm}\n    \\text{T-DCS}_j = \\frac{1}{I_j} \\sum_{i=1}^{I_j} \\text{DCS}_i .\n\\end{equation}\n\n\\textbf{Comprehensive Defense Capability Score (C-DCS).} C-DCS is a comprehensive assessment of the capability of a defense strategy with respect to all kinds of attacks and is a weighted average of T-DCS as shown in \\cref{eq:c-dcs}:\n\\begin{equation} \\label{eq:c-dcs}\n \\setlength\\abovedisplayskip{0.05cm}\n \\setlength\\belowdisplayskip{0.05cm}\n    \\text{C-DCS} = \\sum_{j\\in \\mathcal{A}} w_j \\text{T-DCS}_j, \\,\\, \\text{with} \\sum_{j \\in \\mathcal{A}} w_j = 1.0 \\, .\n\\end{equation}\nWeights $\\{w_j\\}_{j \\in \\mathcal{A}}$ can be tailored to user preference. In our experiments, we simply use an unbiased weight $w_j=\\frac{1}{|\\mathcal{A}|}$ for each attack type $j \\in \\mathcal{A}=\\{\\text{LI}, \\text{FR}, \\text{TB}, \\text{NTB}\\}$.\n\n\n\n\n\n\n\n\\section{{Evaluation Results}\n\n\n\n\n\n\\subsection{VFL Main Task Performance} \\label{subsec:MP}\nWe first comprehensively evaluate the impact of various settings on the performance of VFL.\n\n\\textbf{Model Partition.}\nThe splitVFL setting yields a comparable or slightly higher MP  compared to aggVFL on most datasets, due to the additional trainable layer serving as global model, evidenced by results from \\cref{tab:NN_MP,tab:4party_MP,tab:real_world_dataset_MP}.\n\n\\textbf{Communication Protocols.} %We compare $5$ different kinds of communication protocols in VFL. \nAs shown in \\cref{tab:NN_MP} and \\cref{tab:communication_MP}, compared to FedSGD, FedBCD and CELU-VFL exhibit comparable MP across all datasets with fewer communication rounds, supporting their efficacy in reducing communication overhead. Quantize and Top-k compress the transmitted data and successfully reduce the communication cost per round, but may result in an increase in communication rounds. %CELU-VFL also effectively reduces the \\#Rounds compared to FedSGD, and is slightly less than that of FedBCD\n\n\\textbf{Encryption.} %{\\color{red}{We compare the MP and execution time with and without HE protection using Paillier Encryption in \nFor tree-based VFL, we consider two models with and without Paillier Encryption using 512-bit key size in \\cref{tab:tree_MP}. Note that XGBoost with Paillier Encryption is equivalent to SecureBoost~\\citep{cheng2021secureboost}. Although MP values are consistent regardless of encryption, the execution time experiences a notable increase of 3 to 20 times when encryption is applied due to the additional encryption and decryption process. \n\n\\textbf{Number of Participants.}\nImpact of number of participants are shown in \\cref{tab:4party_MP}. \nA slightly lower MP is achieved using fewer communication rounds as the number of participants increases, demonstrating the increasing challenges brought by multi-party collaboration.\n\n\\textbf{Model Architectures.}\n% {\\color{red}{tree-based VFL performs better on Credit dataset, while NN-based VFL performs better on Nursery dataset.\n\n\\textbf{Real world datasets.}\nAdditional results on Criteo~\\citep{guo2017deepfm_Criteo}, Avazu~\\citep{qu2018product_Avazu}, Cora~\\citep{mccallum2000automating_Cora} and News20~\\citep{lang1995News20} datasets using domain specific models (e.g. Wide\\&Deep Model~\\citep{cheng2016wide} for Criteo and Avazu, GNN for Cora) are provided in \\cref{tab:real_world_dataset_MP}\n, as they are considered for typical VFL applications, such as in recommendation problems. %click through rate prediction in advertising, node classification in citation network and classification of news data\n\\begin{table}[!tb]\n\\caption{MP under $4$ different settings of NN-based VFL. $Q=5$ when FedBCD is applied. In \"\\#Rounds\" column, the first and second numbers are the communication rounds needed to reach the specified MP for FedSGD and FedBCD respectively.}\n\\label{tab:NN_MP}\n\\resizebox{0.998\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c||c|c|c||c|c|c}\n    \\toprule\n    Dataset & aggVFL, FedSGD & aggVFL, FedBCD & \\#Rounds & splitVFL, FedSGD & splitVFL, FedBCD & \\#Rounds \\\\\n    \\midrule\n    MNIST & 0.972$\\pm$0.001 & 0.971 $\\pm$0.001 & 150 / 113 & 0.973$\\pm$0.001 & \\textbf{0.974$\\pm$0.001} & 180 / 143 \\\\\n    NUSWIDE & 0.887$\\pm$0.001 & 0.882$\\pm$0.001 & 60 / 26 & \\textbf{0.888$\\pm$0.001} & 0.884$\\pm$0.001 & 60 / 29 \\\\ \n    Breast Cancer & 0.914$\\pm$0.033 & 0.919$\\pm$0.029 & 5 / 3 & \\textbf{0.925$\\pm$0.028} & 0.907$\\pm$0.045 & 5 / 4 \\\\\n    Diabetes & 0.755$\\pm$0.043 & 0.736$\\pm$0.021 & 15 / 13 & \\textbf{0.766$\\pm$0.024} & 0.746$\\pm$0.039 & 15 / 11 \\\\\n    Adult Income & 0.839$\\pm$0.006 & 0.841$\\pm$0.005 & 17 / 15 & 0.842$\\pm$0.004 & \\textbf{0.842$\\pm$0.005} & 30 / 13 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\n\\begin{table}[!tb]\n\\caption{MP under 2-party VFL verses MP under 4-party VFL  under $4$ different settings of NN-based VFL using FedSGD communication protocol. \"\\#Rounds\" has the same meaning as in \\cref{tab:NN_MP}.}\n\\label{tab:4party_MP}\n\\resizebox{0.998\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c|c||c|c||c|c}\n    \\toprule\n    Dataset  & ~ & aggVFL, 2-party & aggVFL, 4-party & splitVFL, 2-party & splitVFL, 4-party\\\\\n    \\midrule\n    \\multirow{2}{*}{CIFAR10} & MP & 0.790$\\pm$0.003 & 0.747$\\pm$0.003 & \\textbf{0.798$\\pm$0.010} & 0.762$\\pm$0.003 \\\\\n    ~ & \\#Rounds & 244$\\pm$16 & 205$\\pm$12 & 238$\\pm$14 & 173$\\pm$3 \\\\\n    \\midrule\n    \\multirow{2}{*}{CIFAR100} & MP & \\textbf{0.454$\\pm$0.006} & 0.417$\\pm$0.008 & 0.423$\\pm$0.005 & 0.382$\\pm$0.004 \\\\\n    ~ & \\#Rounds & 130$\\pm$11 & 124$\\pm$2 & 125$\\pm$2 & 100$\\pm$1 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\\begin{table}[!tb]\n\\caption{MP and execution time under 2 different types of tree-based VFL.} %\\yang{too many digits, round to fewer, especially for time}\n\\label{tab:tree_MP}\n\\resizebox{0.99\\linewidth}{!}{\n\\centering\n\\begin{tabular}{@{}c|c|c|c|c|c@{}}\n\\toprule\nDataset &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Random Forest \\\\ w/o Encryption\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}XGBoost\\\\ w/o Encryption\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Random Forest \\\\ w/ Encryption\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}XGBoost\\\\ w/ Encryption\\\\ (a.k.a. SecureBoost)\\end{tabular} \\\\ \\midrule\n\\multirow{2}{*}{Credit} &\n  MP &\n  0.816$\\pm$0.005 &\n  0.816$\\pm$0.004 &\n  0.816$\\pm$0.005 &\n  0.816$\\pm$0.004 \\\\\n &\n  \\multicolumn{1}{c|}{Exec.Time [s]} &\n  \\multicolumn{1}{c|}{138$\\pm$4} &\n  \\multicolumn{1}{c|}{366$\\pm$16} &\n  \\multicolumn{1}{c|}{410$\\pm$10} &\n  \\multicolumn{1}{c}{881$\\pm$6} \\\\ \n\\hline\n\\\\[-1em]\n\\multirow{2}{*}{Nursery} &\n  MP &\n  0.884$\\pm$0.010 &\n  0.890$\\pm$0.011 &\n  0.884$\\pm$0.010 &\n  0.890$\\pm$0.011 \\\\\n &\n  \\multicolumn{1}{c|}{Exec.Time [s]} &\n  \\multicolumn{1}{c|}{29$\\pm$2} &\n  \\multicolumn{1}{c|}{69$\\pm$4} &\n  \\multicolumn{1}{c|}{243$\\pm$5} &\n  \\multicolumn{1}{c}{1194$\\pm$21} \\\\ \n\\bottomrule\n\\end{tabular}\n}\n\\end{table}\n\n\\begin{table}[!tb]\n\\caption{\nMP, communication rounds (\\#Rounds), amount of information exchanged per round (Amount) under different communication protocols of NN-based VFL under aggVFL setting. %$Q=5$ when FedBCD and CELU-VFL are used, otherwise $Q=1$. For Quantize, $b=16$ while for Top-k, top $90\\%$ of elements are kept in forward local model prediction.\n'Total' column is the total amount that equals to \\#Rounds$\\times$Amount.}\n\\label{tab:communication_MP}\n\\resizebox{0.99\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c|cccc|cccc}\n    \\toprule\n        ~ & \\multicolumn{4}{c|}{MNIST} & \\multicolumn{4}{c}{NUSWIDE} \\\\\n        \\cline{2-9}\n        \\\\[-1em]\n        ~ & MP & \\#Rounds & Amount (MB) & Total (MB) & MP & \\#Rounds & Amount (MB) & Total (MB)\\\\ \n    \\midrule\n        FedSGD & \\textbf{0.972$\\pm$0.001} & 150 & 0.156 & 23.438 & \\textbf{0.887$\\pm$0.001} & 60 & 0.039 & 2.344 \\\\\n        FedBCD & 0.971$\\pm$0.001 & 113 & 0.156 & 17.656 & 0.882$\\pm$0.001 & 26 & 0.039 & 1.016 \\\\\n        Quantize & 0.959$\\pm$0.006 & 161 & 0.117 & 18.867 & 0.881$\\pm$0.002 & 94 & 0.029 & 2.754 \\\\\n        Top-k & 0.968$\\pm$0.001 & 150 & 0.148 & 22.266 & 0.887$\\pm$0.001 & 60 & 0.037 & 2.227 \\\\\n        CELU-VFL & 0.971$\\pm$0.002 & 105 & 0.156 & 16.406 & 0.880$\\pm$0.001 & 25 & 0.039 & 0.977 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\n\\begin{table}[!tb]\n\\caption{Comparison of aggVFL and splitVFL on MP, \\#Rounds, Amount, total communication cost, Exec.Time for reaching specified MP with $4$ real-world datasets of NN-based VFL with FedSGD communication protocol.}\n\\label{tab:real_world_dataset_MP}\n\\resizebox{0.99\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c|ccccc|ccccc}\n    \\toprule\n        \\multirow{2}{*}{Dataset} & \\multicolumn{5}{c|}{aggVFL} & \\multicolumn{5}{c}{splitVFL} \\\\\n        \\cline{2-11}\n        \\\\[-1em]\n        ~ & MP & \\#Rounds & \\shortstack{Amount\\\\(MB)} & \\shortstack{Total\\\\(MB)} & Exec.Time [s] & MP & \\#Rounds & \\shortstack{Amount\\\\(MB)} & \\shortstack{Total\\\\(MB)} & Exec.Time [s]\\\\\n    \\midrule\n        Criteo & 0.715$\\pm$0.053& 2 & 0.125 & 0.250 & 0.190$\\pm$0.132 & 0.744$\\pm$0.001 & 3 & 0.125 & 0.375 & 0.234$\\pm$0.126\\\\\n        Avazu & 0.832$\\pm$0.001 & 5 & 0.125 & 0.625 & 0.517$\\pm$0.185 & 0.832$\\pm$0.001 & 9 & 0.125 & 1.125 & 1.203$\\pm$1.516\\\\ \n        Cora & 0.721$\\pm$0.004 & 11 & 0.145 & 1.591 & 0.205$\\pm$0.085 & 0.724$\\pm$0.012 & 13 & 0.145 & 1.880 & 0.270$\\pm$0.082 \\\\\n        News20-S5 & 0.882$\\pm$0.014 & 57 & 0.005 & 0.278 & 0.430$\\pm$0.076 & 0.893$\\pm$0.013 & 61 & 0.005 & 0.298 & 0.613$\\pm$0.269 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\n\\subsection{Attack and Defense Performance} \\label{subsec:attack_defense_performance}\n\n\\begin{figure}[!htb]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/cifar10/ALL_agg.pdf}\n  \\caption{MPs and APs for different attacks under defenses [CIFAR10 dataset, aggVFL, FedSGD]}\n  \\label{fig:cifar10_MPAP}\n\\end{figure}\n\nWe demonstrate attack and defense results of VFL on the AP-MP graph for each attack on MNIST, CIFAR10 and NUSWIDE datasets under aggVFL setting using FedSGD protocol in \\cref{fig:mnist_MPAP,fig:cifar10_MPAP,fig:nuswide_MPAP}.\nEach point in the figure represents a $(\\text{MP},\\text{AP})$ pair with the size of markers representing the relative magnitude of the corresponding defense hyper-parameter listed in \\cref{tab:defense_param}.\nNote that although we try to provide comprehensive evaluation for various defenses, we do not force defense onto attacks, meaning that if %we evaluate defense methods only on the specific attacks they were designed for. In other words, if \na defense mechanism is designed for mitigating label inference attacks only, we do not assess its effectiveness against FR attacks or backdoor attacks. \n\nWe further rank all the defenses of different hyper-parameters based on their C-DCS. Due to space limitation, we show representative results for NUSWIDE dataset in \\cref{tab:tiny_nuswide_dcs_ranking}.%Consequently, our experimental evaluations do not include assessments of GPer, CAE, and DCAE against FR attacks and NTB attacks, while GS is excluded from evaluations involving FR attacks.}}\n\n\n\\textbf{Attacks pose great threat to VFL.} Comparing the black squares illustrating the MP and AP of the attack against a VFL system without any defense in the sub-figures, we can observe that DS, DLI, BLI and TBM attacks are strong attacks with AP higher than $0.97$, while MF attacks are quite weak with AP below $0.1$.\n\n\\textbf{Defenses exhibit trade-offs between MP and AP.} For most of the attacks and defenses, we can observe an apparent trade-off between MP and AP, i.e. a lower AP is often gained with increasing harm of MP as defense strength grows, which can be controlled by adjusting defense hyper-parameters. \nAn increase of noise level in DP-G and DP-L, sparsification rate in GS, regularization hyper-parameter $\\alpha_d$ in dCor, confusional strength $\\lambda_2$ in CAE and DCAE, regularization hyper-parameter $\\lambda$ in MID or a decrease of DP budget $\\epsilon$ in GPer will lead to lower MP and AP.\n\n\\textbf{DCS rankings are consistent across various datasets and settings.} As shown in \\cref{tab:mnist_dcs_ranking,tab:nuswide_dcs_ranking,tab:cifar10_dcs_ranking}, the results of the C-DCS rankings are generally consistent across all $3$ datasets. %% namely MNIST, CIAFR10 and NUSWIDE, under the same aggVFL setting with FedSGD communication. Also, \nAs summarised in \\cref{fig:dcs_ranking_mean_std}, these results demonstrate the robustness of the proposed DCS metrics, as well as the stableness of relative performance of different defense methods.Note that, T-DCS$_{\\text{FR}}$ values are much lower than the T-DCS of other types, indicating that FR attacks are harder to defend than other attacks, which are consistent with human observation (see \\cref{fig:visualization_mnist_tbm} in \\cref{subsec:visualization_tbm}). %and more effective defense methods against FR attacks are in need.\n\n\\textbf{MID, L-DP and G-DP are effective on a wide spectrum of attacks.} MID demonstrates its capability of achieving a relatively lower AP while maintaining a higher MP compared to most other defenses as shown in \\cref{fig:mnist_MPAP,fig:nuswide_MPAP,fig:cifar10_MPAP} and  \\cref{tab:mnist_dcs_ranking,tab:nuswide_dcs_ranking,tab:cifar10_dcs_ranking}; L-DP and G-DP are also generally effective under most attacks with above average T-DCS and C-DCS; DCAE is effective in defending against LI attacks; %\\textbf{CAE is effective only against LI attacks that utilize merely the information of the current sample}%\\textbf{CAE consistently performs well across when defending against LI attacks that utilize merely the information of the current sample} by disguising label directly; \nGS demonstrates strong defense ability for most of the LI attacks but performs less than satisfactory on LRB attacks; GPer performs similar to DP-G and DP-L in defending against label related attacks; %as it targets at defending LI attacks and guarantees label-DP \\yang{what about other attac}ks?} \\tianyuan{Not evaluated... Do we need to include the explanation?}\ndCor is less effective in limiting AP under NTB attacks but is largely effective against PMC and AMC attacks as shown in \\cref{fig:mnist_MPAP,fig:nuswide_MPAP,fig:cifar10_MPAP}. \n\n\n\\begin{wrapfigure}{l}{0.55\\textwidth}\n  \\centering\n  \\includegraphics[width=0.99\\linewidth]{figures/mnist/mnist_rank_beta.pdf}\n  \\caption{Change of C-DCS ranking with the change of $\\beta$. [MNIST dataset, aggVFL, FedSGD]}\n  \\label{fig:change_beta_dcs_mnist}\n\\end{wrapfigure}\n\\textbf{Change in $\\beta$ does not significantly impact the C-DCS ranking.} $\\beta$ in \\cref{eq:dcs} represents users' trade-off preference on AP and MP when evaluating defenses, and can be adjusted. Here we use $\\beta =0.5$ for our main results. %If the user wants to place more attention on maintaining a high MP, hyper-parameter $\\beta$ in \\cref{eq:dcs} can be adjusted to a value larger than $0.5$. \n\\cref{fig:change_beta_dcs_mnist,fig:change_beta_dcs_cifar10,fig:change_beta_dcs_nuswide} show the change of the ranking results with the change of $\\beta$. Overall the relative rankings are not significantly impacted by $\\beta$, demonstrating the stableness of the comparison results among various defenses. As $\\beta$ grows to large values, e.g. $\\geq 0.9$, the metric places overly strong weight on MP, resulting in more variations on the rankings. Specifically, dCor ranks higher with the increase of $\\beta$ thanks to its better MP preservation at the cost of a weaker AP limitation.\n\\begin{table}[!tb]\n\\caption{T-DCS and C-DCS for All Defenses [NUSWIDE dataset, aggVFL, FedSGD]}\n\\label{tab:tiny_nuswide_dcs_ranking}\n\\resizebox{0.998\\linewidth}{!}{\n  \\centering\n   \\begin{tabular}{cc|cccccc|c}\n    \\toprule\n    \\textbf{\\makecell{Defense\\\\Name}} & \\textbf{\\makecell{Defense\\\\Parameter}} & \\bm{$T\\text{-}DCS_{LI_{2}}$} &\n    \\bm{$T\\text{-}DCS_{LI_{5}}$} & \\bm{$T\\text{-}DCS_{LI}$} & \\bm{$T\\text{-}DCS_{FR}$} & \\bm{$T\\text{-}DCS_{TB}$} & \\bm{$T\\text{-}DCS_{NTB}$} & \\bm{$C\\text{-}DCS$} \\\\ \n    \\midrule \n    MID  & 10000  & 0.7358 & 0.8559 & \\textbf{0.8159} & 0.5833 & \\textbf{0.7333} & 0.8707 & 0.7508  \\\\\n    MID  & 1.0    & 0.7476 & 0.8472 & 0.8140 & 0.5833 & 0.7331 & 0.8700 & 0.7501  \\\\\n    MID  & 100    & 0.7320 & 0.8536 & 0.8130 & 0.5833 & 0.7326 & \\textbf{0.8711} & 0.7500  \\\\\n    G-DP & 0.1    & 0.7375 & 0.8262 & 0.7966 & 0.5863 & 0.7282 & 0.8675 & 0.7447  \\\\\n    L-DP & 0.1    & 0.7389 & 0.8177 & 0.7915 & 0.5863 & 0.7258 & 0.8603 & 0.7410  \\\\\n    MID  & 0.1    & 0.7516 & 0.8259 & 0.8011 & 0.5833 & 0.7172 & 0.8563 & 0.7395  \\\\\n    MID  & 0.01   & 0.7280 & 0.8092 & 0.7822 & 0.5844 & 0.7151 & 0.8627 & 0.7361  \\\\\n    dCor & 0.3    & \\textbf{0.7641} & 0.8411 & 0.8155 & 0.5834 & 0.7289 & 0.8051 & 0.7332  \\\\\n    dCor & 0.0001 & 0.6496 & 0.6340 & 0.6392 & \\textbf{0.5864} & 0.6307 & 0.8287 & 0.6712 \\\\\n    \\hline\n    \\\\[-1em]\n    GS   & 99.0   & 0.7404 & 0.8060 & 0.7841 & -      & 0.6415 & 0.8408 & -      \\\\\n    CAE  & 1.0    & 0.6863 & 0.7822 & 0.7502 & -      & 0.6830  & -           \\\\\n    DCAE & 0.0    & 0.6669 & \\textbf{0.8660} & 0.7996 & -      & 0.6816 & -      & -       \\\\\n    GPer & 0.01   & 0.7386 & 0.8412 & 0.8070 & -      & 0.7193 & -      & -       \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\\textbf{splitVFL is less vulnerable to attacks than aggVFL.}\nUsing DCS metrics, we directly compare all the aforementioned attacks and defenses under aggVFL and splitVFL settings to understand the impact of changing the model partition strategy on VFL's vulnerability against attacks. We mainly use the DCS gap, defined as $\\text{DCS}^{\\text{splitVFL}}-\\text{DCS}^{\\text{aggVFL}}$ for each attack-defense point. \\cref{fig:mnist_splitVFL_DCS_gap,fig:nuswide_split_DCS_gap} \\begin{wrapfigure}{l}{0.55\\textwidth}\n  \\centering\n  \\includegraphics[width=0.99\\linewidth]{figures/split_agg_compare/splitagg_hist_total.pdf}\n  \\caption{DCS gap Distribution, y-axis represents density [MNIST dataset, splitVFL/aggVFL, FedSGD]}   \n  \\label{fig:mnist_splitVFL_DCS_hist}           \n\\end{wrapfigure}\n \n\\cref{fig:mnist_splitVFL_DCS_hist,fig:nuswide_split_hist} displays the distribution of the DCS gaps depicted in \\cref{fig:mnist_splitVFL_DCS_gap,fig:nuswide_split_DCS_gap} respectively.\nAs all the black square points in \\cref{fig:mnist_splitVFL_DCS_gap} appear above or close to the red horizontal line at a value of $0.0$ (see also the blue histograms that appear mostly at the right of the vertical line at a value of $0.0$ in \\cref{fig:mnist_splitVFL_DCS_hist}), we can conclude that splitVFL is less vulnerable to attacks than aggVFL when no defense is applied. In addition, splitVFL has an overall positive effect on boosting defense performance against attacks as well, as most of the DCS gap is positive in the last subplot of \\cref{fig:mnist_splitVFL_DCS_gap} when no attack is applied. \n\n\\begin{wrapfigure}{l}{0.55\\textwidth}\n      \\centering\n      \\includegraphics[width=0.99\\linewidth]{figures/bcd_compare/BCD_hist_total.pdf}\n    \\caption{DCS gap Distribution, y-axis represents density [MNIST dataset, aggVFL, FedBCD/FedSGD]}\n  \\label{fig:mnist_FedBCD_DCS_hist}           \n\\end{wrapfigure}\n\n\\textbf{FedBCD is less vulnerable to attacks than FedSGD.} In addition, we compare DCS gap under FedSGD setting and FedBCD with $Q=5$ to assess the impact of different communication protocols on model's vulnerability to attacks. DCS gap is defined as $\\text{DCS}^{\\text{FedBCD}}-\\text{DCS}^{\\text{FedSGD}}$ for each attack-defense point. \n\nAs shown in \\cref{fig:mnist_FedBCD_DCS_hist}, the blue histograms generally appear on the right of the vertical line of value $0.0$, indicating that a system with FedBCD protocol is less vulnerable to attacks when no defense method is applied. In addition, a system with FedBCD also has an overall positive effect on boosting defense performance against FR and NTB attacks. This is evidenced by the fact that that the majority of DCS gaps are positive for FR and NTB attacks as shown in \\cref{fig:mnist_FedBCD_DCS_hist}.\nSimilar conclusions can be drawn from \\cref{fig:nuswide_FedBCD_hist,fig:nuswide_FedBCD_DCS_gap}.\n\n\n\\section{Conclusions and Limitations}\n\n\nIn this work, we introduce a light-weight VFL framework \\verb|VFLAIR| that implements basic VFL training and evaluation flow under multiple model partition, model architectures,communication protocols and attacks and defenses algorithms using datasets of different modality. We also introduce unified evaluation metrics and benchmark model utility, communication and computation efficiency, and defense performance under various VFL settings, which sheds lights on choosing partition, communication and defense techniques in practical deployment. \nCurrently, the library has limited implementations on cryptographic techniques. Combination of non-cryptograhic and cryptographic techniques would be an interesting next step and we plan to add more advanced privacy-preserving and communication-efficient methods to our library. %encryption implementation, since is not implemented for NN-based VFL, we will add this part in the near future.}}\n\n\n\n\\section{Reproducibility Statement}\n\n Our code is also available at\n\n\n\n\n", "DeepReviewer": "\n## Summary:\n\nThis paper introduces VFLAIR, a lightweight and extensible framework for Vertical Federated Learning (VFL), designed to facilitate research and practical deployment. The core contribution of this work lies in the creation of a comprehensive benchmark for VFL, encompassing a wide range of models, datasets, communication protocols, and, crucially, various attack and defense strategies. The authors implement 29 local model architectures, 6 global models, and 5 communication protocols, alongside 11 different attack methods and 8 defense mechanisms. The framework supports 13 datasets, including image, text, and tabular data, allowing for a diverse set of experiments. The authors benchmark the performance of these components under different communication and model partition settings, aiming to provide insights and recommendations on the choice of defense strategies for different practical VFL deployment scenarios. The empirical findings include evaluations of model utility, defense performance, and the computational overhead of different configurations. The paper introduces new evaluation metrics, such as Defense Capability Score (DCS), Type-level DCS (T-DCS), and Comprehensive DCS (C-DCS), to assess the effectiveness of defense methods. The authors also explore the trade-offs between model utility and defense effectiveness, providing concrete recommendations for practitioners. The paper's significance lies in its attempt to provide a unified platform for VFL research, addressing the need for a comprehensive benchmark that includes a wide range of methods and scenarios. However, the paper's primary focus is on implementation and empirical evaluation, with limited theoretical contributions. The authors acknowledge the limitations of the current implementation, particularly in terms of cryptographic techniques, and suggest future directions for improvement. Overall, the paper presents a valuable contribution to the VFL community by providing a practical framework and benchmark, but it also highlights areas where further research is needed.\n\n\n## Soundness:\n\n3.0\n\n\n## Presentation:\n\n3.0\n\n\n## Contribution:\n\n3.0\n\n\n## Strengths:\n\nI find several strengths in this paper, primarily centered around its contribution to the practical application and benchmarking of Vertical Federated Learning (VFL). The most significant strength is the development of VFLAIR, a lightweight and extensible VFL framework. This framework, as the authors describe, includes a wide array of VFL components, such as 29 local model architectures, 6 global models, and 5 communication protocols. This comprehensive collection of components, coupled with the support for 13 diverse datasets, makes VFLAIR a valuable resource for researchers and practitioners alike. The inclusion of 11 attack methods and 8 defense mechanisms further enhances the framework's utility as a benchmarking tool. The authors have clearly put considerable effort into implementing state-of-the-art VFL methods, which is a non-trivial undertaking. The empirical evaluation is another strong point. The authors conduct extensive experiments, benchmarking the performance of various VFL settings, including model partitioning, communication protocols, and defense strategies. The introduction of new evaluation metrics, such as DCS, T-DCS, and C-DCS, is a positive step towards a more nuanced understanding of defense effectiveness. These metrics allow for a more comprehensive assessment of the trade-offs between model utility and defense performance. The paper also provides concrete recommendations on the choice of defense strategies for different practical VFL deployment scenarios, which is particularly helpful for practitioners. The authors have clearly identified a need for a comprehensive VFL benchmark and have made a commendable effort to address this gap. The framework's extensibility is also a notable strength, as it allows for future additions and modifications, ensuring its continued relevance to the VFL community. The paper's focus on practical deployment scenarios is also a positive aspect, as it highlights the real-world applicability of the research. The authors have successfully created a valuable tool that can facilitate further research and development in the field of VFL.\n\n\n## Weaknesses:\n\nDespite the strengths of this paper, I have identified several weaknesses that warrant careful consideration. First, the paper's novelty is limited. While the implementation of a comprehensive VFL framework is a valuable contribution, the paper does not introduce any novel defense techniques. As I've verified, the authors explicitly state that they implement and benchmark existing methods, rather than proposing new ones. This focus on implementation and empirical evaluation, while useful, positions the paper more as a system paper than a theoretical contribution. The lack of detailed implementation specifics further reinforces this perception. Second, the paper lacks a significant theoretical contribution. The focus is primarily on empirical evaluation and benchmarking, with no in-depth theoretical analysis or proofs. While the introduction of new evaluation metrics is a contribution, it is still within the realm of empirical analysis. The paper does not delve into the theoretical underpinnings of the observed empirical results, such as the relationship between the properties of the defense mechanisms and their ability to mitigate specific types of attacks. Third, the paper does not provide a detailed analysis of the computational overhead of the proposed defense methods. While the authors include 'Execution Time' as a metric in some experiments, this is primarily for comparing the overall training time with and without encryption for tree-based models. There is no detailed breakdown of the computational overhead introduced by each individual defense method. This is a significant omission, as the computational cost of defense methods is a crucial factor to consider in practical deployments, especially in resource-constrained environments. Fourth, the paper does not explore the potential vulnerabilities of the proposed defense methods to more advanced attacks. The evaluation is limited to a set of 11 specific attacks, which are established attacks but not necessarily adaptive attacks designed to circumvent these specific defenses. The paper does not consider the robustness of these defenses against sophisticated adversaries who might develop new attack strategies to target the weaknesses of the evaluated defenses. Fifth, the evaluation of the framework is limited to a relatively small number of datasets and model architectures. While the paper includes 13 datasets and 29 local model architectures, a wider range of datasets and models would further demonstrate the framework's generalizability. The current selection, while diverse, could be expanded to include more complex data modalities and tasks. Finally, the paper lacks a detailed discussion of the limitations of the proposed framework and potential future research directions. While the 'Conclusions and Limitations' section briefly mentions some limitations, it does not provide a comprehensive analysis of the challenges of deploying VFL in real-world scenarios, such as the need for robust security mechanisms and the difficulty of handling heterogeneous data distributions. The paper also does not thoroughly outline potential future research directions, such as the development of more efficient and robust defense methods or the exploration of new VFL architectures. These limitations, taken together, suggest that while the paper makes a valuable contribution in terms of a practical framework, it falls short in terms of theoretical depth, computational analysis, and exploration of advanced attack scenarios. My confidence in these identified weaknesses is high, as they are directly supported by the paper's content and lack of specific details in the areas mentioned.\n\n\n## Suggestions:\n\nBased on the identified weaknesses, I recommend several concrete improvements to enhance the paper's contribution. First, the authors should conduct a more in-depth analysis of the computational overhead associated with the proposed VFL framework, particularly the defense methods. This analysis should include a breakdown of the time spent on different stages of the training process, such as forward and backward propagation, communication, and encryption/decryption. This analysis should be conducted for various model architectures and datasets to understand how these factors influence the overall training time. Furthermore, the impact of different defense methods on the computational cost should be quantified. For example, the authors could measure the increase in training time and communication costs when using techniques like gradient perturbation or homomorphic encryption. This detailed analysis would allow practitioners to make informed decisions about the trade-offs between security and efficiency when deploying the framework in real-world scenarios. It would also be beneficial to explore optimization techniques to mitigate the computational overhead of the defense methods. Second, the authors should expand the evaluation of the framework to include a wider range of datasets and model architectures. While the current evaluation includes 13 datasets and 29 local model architectures, it would be beneficial to include datasets with varying characteristics, such as different data modalities (e.g., images, text, time series) and different data sizes. This would help to assess the generalizability of the framework and identify potential limitations. Similarly, the evaluation should include a wider range of model architectures, such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers, to demonstrate the framework's versatility. Furthermore, the authors should consider evaluating the framework's performance on more complex tasks, such as multi-label classification or regression problems. This would provide a more comprehensive understanding of the framework's capabilities and limitations. Third, the authors should explore the potential vulnerabilities of the proposed defense methods to more advanced attacks. This could involve considering adaptive attacks that are specifically designed to circumvent the defenses. The authors should also consider exploring the use of adversarial training techniques to improve the robustness of the defenses against such advanced attacks. This could involve training the models on adversarial examples generated by more sophisticated attack methods, or using techniques such as robust optimization to minimize the impact of adversarial perturbations. Furthermore, the authors should investigate the potential for attacks that exploit the communication protocols used in VFL, such as man-in-the-middle attacks or replay attacks. Fourth, the authors should include a more detailed discussion of the limitations of the proposed framework and potential future research directions. The authors should acknowledge the challenges of deploying VFL in real-world scenarios, such as the need for robust security mechanisms and the difficulty of handling heterogeneous data distributions. They should also discuss the limitations of the current evaluation, such as the limited number of datasets and model architectures. Furthermore, the authors should outline potential future research directions, such as the development of more efficient and robust defense methods, the exploration of new VFL architectures, and the application of VFL to new domains. This discussion would provide valuable insights for researchers in the field and help to guide future research efforts. Finally, the authors should consider adding more implementation details to the paper, such as code snippets or architectural diagrams of the framework itself. This would make the paper more useful for researchers who want to use the framework and would also help to clarify the technical contributions of the work. These suggestions, if implemented, would significantly strengthen the paper and make it a more valuable contribution to the VFL community.\n\n\n## Questions:\n\nBased on my analysis, I have several questions that I believe are crucial for a deeper understanding of the paper's contributions and limitations. First, how do the proposed defense methods perform against more advanced attacks that are specifically designed to target these defenses? The current evaluation focuses on a set of existing attacks, but it does not consider the potential for adaptive adversaries to circumvent these defenses. Understanding the robustness of the defenses against such advanced attacks is crucial for assessing their practical utility. Second, what is the computational overhead of the proposed defense methods, and how does it impact the scalability of VFL systems? The paper does not provide a detailed analysis of the computational cost of each defense method, which is a critical factor to consider in practical deployments, especially in resource-constrained environments. A detailed breakdown of the time spent on different stages of the training process, such as forward and backward propagation, communication, and encryption/decryption, would be beneficial. Third, how do the proposed defense methods affect the convergence rate and stability of VFL training? The paper does not explore the impact of the defense methods on the training dynamics, which is an important aspect to consider. Understanding how these methods affect the convergence rate and stability of the training process is crucial for ensuring the reliability of the framework. Fourth, how do the proposed defense methods perform in real-world VFL deployment scenarios, and what are the practical challenges of implementing these defenses in such settings? The paper focuses on benchmarking in a controlled environment, but it does not address the practical challenges of deploying these defenses in real-world scenarios, such as handling heterogeneous data distributions and dealing with communication constraints. Fifth, what are the potential limitations of the proposed framework, and what are the future research directions that the authors envision? The paper briefly mentions some limitations, but a more detailed discussion of the challenges of deploying VFL in real-world scenarios and the potential future research directions would be valuable. Finally, how does the proposed framework compare to other existing VFL frameworks in terms of performance, scalability, and security? A comparative analysis with other existing frameworks would provide a better understanding of the strengths and weaknesses of VFLAIR. These questions, I believe, are essential for a comprehensive understanding of the paper's contributions and limitations and would help to guide future research in this area.\n\n\n## Rating:\n\n6.0\n\n\n## Confidence:\n\n3.5\n\n\n## Decision:\n\nAccept", "other": "\n## Summary:\n\nThis paper introduces VFLAIR, a novel framework designed for research in Vertical Federated Learning (VFL). The authors aim to address the gap between academic research and industrial applications by providing a lightweight, modular, and extensible platform. VFLAIR supports a wide range of VFL algorithms, including various model architectures (linear regression, tree-based models, and neural networks), communication protocols (FedSGD, FedBCD, CELU-VFL, Top-k, and Quantize), and defense methods against data inference and backdoor attacks. The framework is designed to facilitate research by providing a standardized pipeline for VFL training and validation. A key contribution of this work is the extensive benchmarking of 11 different attacks and 8 defense strategies under diverse settings, which the authors use to derive insights and recommendations for practical VFL deployment. The empirical evaluation includes experiments on 13 datasets, assessing the impact of different model partition strategies, communication protocols, and defense mechanisms on model performance, communication efficiency, and robustness to attacks. The authors also introduce new evaluation metrics, such as Attack Performance (AP), Main Task Performance (MP), and Defense Capability Score (DCS), to quantify the effectiveness of different VFL configurations. The paper's findings highlight the trade-offs between model utility and security in VFL systems and provide practical guidance for selecting appropriate defense strategies. Overall, the paper presents a significant contribution to the VFL research community by providing a comprehensive framework and benchmark for evaluating the performance and security of VFL systems. However, as I will discuss in the weaknesses section, there are some areas where the paper could be improved to further enhance its impact and utility.\n\n\n## Soundness:\n\n2.75\n\n\n## Presentation:\n\n3.0\n\n\n## Contribution:\n\n2.75\n\n\n## Strengths:\n\nI find several aspects of this paper to be particularly strong. First, the development of VFLAIR as a lightweight, modular, and extensible framework is a significant contribution. The authors have clearly identified a need for a research-focused VFL platform that bridges the gap between academic research and industrial applications. By providing a standardized pipeline for VFL training and validation, VFLAIR has the potential to accelerate research in this area. The framework's support for a wide range of VFL algorithms, including various model architectures, communication protocols, and defense methods, makes it a versatile tool for researchers. Second, the extensive benchmarking of 11 attacks and 8 defenses under diverse settings is a valuable contribution. The authors have conducted a thorough evaluation of the performance and robustness of different VFL configurations, providing practical insights into the trade-offs between model utility and security. The use of multiple datasets and the exploration of different model partition strategies further enhance the generalizability of the findings. The introduction of new evaluation metrics, such as AP, MP, and DCS, is also a positive aspect, as these metrics provide a standardized way to quantify the effectiveness of different VFL configurations. Finally, the paper is well-written and easy to follow, with clear explanations of the methods and experimental results. The authors have made a commendable effort to make the paper accessible to a broad audience, which will undoubtedly increase its impact on the VFL research community. The inclusion of a comprehensive appendix further enhances the paper's value, providing additional details and context for the readers.\n\n\n## Weaknesses:\n\nDespite the strengths of this paper, I have identified several weaknesses that warrant further discussion. First, while the paper introduces VFLAIR as a lightweight and modular framework, it lacks a direct comparison with existing VFL frameworks, such as FATE. Although the introduction mentions that mainstream VFL projects like FATE are industrial-grade and not designed for keeping up with research advances, a quantitative comparison of system characteristics, such as system requirements, execution time, and resource usage, would be beneficial. This comparison would help potential users understand the trade-offs between VFLAIR and other frameworks and would further justify the need for a new framework. The absence of such a comparison makes it difficult to assess the practical advantages of VFLAIR over existing solutions. Second, the paper's evaluation of defense methods lacks a comprehensive analysis of their applicability to different attack types. While the paper benchmarks 11 attacks and 8 defenses, it does not explicitly state which defenses are applicable to which attacks in the main text, although this information is available in Table 1. This lack of clarity makes it difficult to understand the effectiveness of each defense method against specific attacks. For example, it is not immediately clear whether a defense designed for label inference attacks would be effective against feature reconstruction attacks. This is a crucial point for researchers who want to use VFLAIR to evaluate specific defense strategies. Third, the paper does not provide a detailed analysis of the computational overhead associated with the implemented defense methods. While the paper reports the execution time for tree-based models with and without encryption, it does not provide a similar analysis for the other defense methods. This is a significant limitation, as the computational cost of defense methods is a critical factor in practical VFL deployments. Researchers need to know the computational overhead of each defense method to make informed decisions about which methods to use in their specific contexts. Fourth, the paper's experimental evaluation is limited to a specific set of datasets and model architectures. While the paper includes experiments on 13 datasets and various model architectures, it does not explicitly test the framework's performance with larger models, such as ResNet or Inception, which are commonly used in computer vision tasks. This is a limitation, as the performance and scalability of VFL frameworks can vary significantly depending on the model architecture. The paper also does not include experiments on datasets with highly imbalanced classes or high-dimensional sparse features, which are common in real-world applications. This limits the generalizability of the findings and raises questions about the framework's robustness in diverse scenarios. Fifth, the paper does not address the issue of fairness in VFL. While the paper focuses on privacy and security, fairness is another critical aspect of VFL that needs to be considered. The paper does not include any discussion of fairness-related metrics or defense methods, which is a significant limitation, especially given the increasing awareness of fairness issues in machine learning. Finally, the paper does not provide a detailed analysis of the scalability of VFLAIR with an increasing number of participants. While the paper includes experiments with 2 and 4 parties, it does not explicitly test the framework's performance with a significantly larger number of participants. This is a limitation, as scalability is a crucial factor in practical VFL deployments. The paper also does not discuss the potential challenges of deploying VFLAIR in real-world scenarios, such as network latency, data heterogeneity, and the need for secure multi-party computation. These issues need to be addressed to make VFLAIR a more practical and useful tool for researchers and practitioners.\n\n\n## Suggestions:\n\nBased on the identified weaknesses, I recommend several improvements to enhance the paper's impact and utility. First, I suggest that the authors include a direct comparison of VFLAIR with existing VFL frameworks, such as FATE. This comparison should include quantitative metrics, such as system requirements, execution time, and resource usage, to provide a clear understanding of the trade-offs between VFLAIR and other frameworks. This would help potential users make informed decisions about which framework to use for their specific needs. Second, I recommend that the authors provide a more detailed analysis of the applicability of each defense method to different attack types. This analysis should be included in the main text, not just in the appendix or tables, and should clearly state which defenses are effective against which attacks. This would help researchers quickly identify the most appropriate defense methods for their specific concerns. Third, I suggest that the authors include a detailed analysis of the computational overhead associated with each defense method. This analysis should include metrics such as training time, communication cost, and memory usage. This would help researchers understand the practical implications of using different defense methods and make informed decisions about which methods to use in their specific contexts. Fourth, I recommend that the authors expand the experimental evaluation to include a wider range of datasets and model architectures. This should include experiments with larger models, such as ResNet or Inception, as well as datasets with highly imbalanced classes or high-dimensional sparse features. This would enhance the generalizability of the findings and demonstrate the robustness of VFLAIR in diverse scenarios. Fifth, I suggest that the authors incorporate fairness-related metrics and defense methods into VFLAIR. This would make the framework more comprehensive and address an important aspect of VFL that is currently missing. This could include metrics such as demographic parity, equal opportunity, and predictive parity, as well as defense methods that aim to mitigate bias in VFL models. Sixth, I recommend that the authors conduct more extensive experiments to evaluate the scalability of VFLAIR with an increasing number of participants. This should include experiments with a significantly larger number of parties to assess the framework's performance in more realistic scenarios. Finally, I suggest that the authors discuss the potential challenges of deploying VFLAIR in real-world scenarios, such as network latency, data heterogeneity, and the need for secure multi-party computation. This would make the paper more relevant to practitioners and would help bridge the gap between academic research and industrial applications.\n\n\n## Questions:\n\nBased on my analysis, I have several questions that I believe would be beneficial for the authors to address. First, given the focus on label inference attacks, I am curious about the specific scenarios where the other attack types (feature reconstruction, backdoor, and non-targeted attacks) are most relevant. What are the practical situations in which these attacks would be a primary concern, and how do the defense methods perform in those specific contexts? Second, regarding the computational overhead of the defense methods, what are the specific bottlenecks that contribute to the increased execution time? Are there any optimization techniques that could be used to reduce the computational cost of these methods? Third, concerning the scalability of VFLAIR, what are the main challenges in deploying the framework with a large number of participants? Are there any specific architectural limitations that need to be addressed to improve scalability? Fourth, given the lack of explicit fairness considerations, what are the authors' plans for incorporating fairness-related metrics and defense methods into VFLAIR? Are there any specific fairness challenges in VFL that the authors are particularly interested in addressing? Fifth, regarding the choice of datasets, what are the authors' criteria for selecting the datasets used in the experiments? Are there any specific characteristics of the datasets that make them representative of real-world VFL scenarios? Finally, given the focus on benchmarking, what are the authors' plans for maintaining and updating the benchmark results? How will they ensure that the benchmark remains relevant and up-to-date with the latest advancements in VFL research?\n\n\n## Rating:\n\n6.0\n\n\n## Confidence:\n\n3.75\n\n\n## Decision:\n\nAccept", "other_type": "DeepReviewer_fast", "v.s.": "B", "result": "```\n**Technical Accuracy - Evidence Analysis:**\n- From Assistant A:\n  [Direct quotes and specific examples]\n  - \"This paper introduces VFLAIR, a novel framework designed for research in Vertical Federated Learning (VFL).\" - Accurate summary of the paper's main contribution.\n  - \"VFLAIR supports a wide range of VFL algorithms, including various model architectures (linear regression, tree-based models, and neural networks), communication protocols (FedSGD, FedBCD, CELU-VFL, Top-k, and Quantize), and defense methods against data inference and backdoor attacks.\" - Accurately lists key features of VFLAIR as described in the paper (Introduction and Section 4).\n  - \"A key contribution of this work is the extensive benchmarking of 11 different attacks and 8 defense strategies under diverse settings...\" - Correctly identifies the benchmarking as a major contribution (Abstract and Section 5).\n  - \"The empirical evaluation includes experiments on 13 datasets...\" - Accurate number of datasets used in experiments (Section 5.1).\n  - \"The introduction of new evaluation metrics, such as AP, MP, and DCS, is also a positive aspect...\" - Correctly points out the new metrics (Section 5.3).\n- From Assistant B:\n  [Direct quotes and specific examples]\n  - \"This paper introduces VFLAIR, a lightweight and extensible framework for Vertical Federated Learning (VFL), designed to facilitate research and practical deployment.\" - Accurate summary of the paper's purpose.\n  - \"The core contribution of this work lies in the creation of a comprehensive benchmark for VFL, encompassing a wide range of models, datasets, communication protocols, and, crucially, various attack and defense strategies.\" - Correctly identifies the benchmark as the core contribution.\n  - \"The authors implement 29 local model architectures, 6 global models, and 5 communication protocols, alongside 11 different attack methods and 8 defense mechanisms.\" - Accurately lists the components of VFLAIR (Section 4 and Section 5.1, 5.2).\n  - \"The framework supports 13 datasets, including image, text, and tabular data, allowing for a diverse set of experiments.\" - Correctly states the number of datasets and their types (Section 5.1).\n  - \"The paper introduces new evaluation metrics, such as Defense Capability Score (DCS), Type-level DCS (T-DCS), and Comprehensive DCS (C-DCS), to assess the effectiveness of defense methods.\" - Correctly lists the new evaluation metrics (Section 5.3).\n- Comparative Assessment:\n  Both assistants demonstrate excellent technical accuracy. They both correctly identify the key contributions, features, and experimental setup of the paper.  Both accurately list the number of datasets, attacks, defenses, and other components. There are no factual errors in either review regarding the paper's content.\n\n**Technical Accuracy - Judgment:**\n**Evidence-Based Reason:** Both Assistant A and Assistant B accurately and comprehensively summarize the paper's technical details, including the framework's components, experimental setup, and key contributions. They both demonstrate a strong understanding of the paper's content and present factually correct information.\n**Better Assistant:** Tie\n\n---\n**Constructive Value - Evidence Analysis:**\n- From Assistant A:\n  [Direct quotes and specific examples]\n  - \"First, while the paper introduces VFLAIR as a lightweight and modular framework, it lacks a direct comparison with existing VFL frameworks, such as FATE... This comparison would help potential users understand the trade-offs between VFLAIR and other frameworks and would further justify the need for a new framework.\" - Suggestion for improvement is actionable and specific. Authors could add a table comparing VFLAIR to FATE on metrics like system requirements, ease of use, features, etc.\n  - \"Second, the paper's evaluation of defense methods lacks a comprehensive analysis of their applicability to different attack types... This lack of clarity makes it difficult to understand the effectiveness of each defense method against specific attacks.\" - Actionable suggestion. Authors could add a table or section explicitly mapping defenses to attack types and discussing their effectiveness.\n  - \"Third, the paper does not provide a detailed analysis of the computational overhead associated with the implemented defense methods... Researchers need to know the computational overhead of each defense method to make informed decisions...\" - Actionable suggestion. Authors could add experiments and analysis focusing on the computational cost of defenses.\n  - \"Fourth, the paper's experimental evaluation is limited to a specific set of datasets and model architectures... This limits the generalizability of the findings...\" - Actionable suggestion. Authors could expand experiments to include larger models, more diverse datasets.\n  - \"Fifth, the paper does not address the issue of fairness in VFL... The paper does not include any discussion of fairness-related metrics or defense methods...\" - Actionable suggestion. Authors could add a section discussing fairness and potentially incorporate fairness metrics into VFLAIR.\n  - \"Finally, the paper does not provide a detailed analysis of the scalability of VFLAIR with an increasing number of participants... This is a limitation, as scalability is a crucial factor in practical VFL deployments.\" - Actionable suggestion. Authors could add scalability experiments with more parties.\n- From Assistant B:\n  [Direct quotes and specific examples]\n  - \"First, the paper's novelty is limited... the paper does not introduce any novel defense techniques... This focus on implementation and empirical evaluation, while useful, positions the paper more as a system paper than a theoretical contribution.\" - Less actionable as it's a general comment on novelty rather than a specific improvement.\n  - \"Second, the paper lacks a significant theoretical contribution... The paper does not delve into the theoretical underpinnings of the observed empirical results...\" - Less actionable, similar to the first point, more of a characterization.\n  - \"Third, the paper does not provide a detailed analysis of the computational overhead of the proposed defense methods... This is a significant omission, as the computational cost of defense methods is a crucial factor to consider in practical deployments...\" - Similar to Assistant A's third point, actionable. Authors could add computational overhead analysis.\n  - \"Fourth, the paper does not explore the potential vulnerabilities of the proposed defense methods to more advanced attacks... The paper does not consider the robustness of these defenses against sophisticated adversaries...\" - Actionable suggestion. Authors could explore robustness against adaptive attacks.\n  - \"Fifth, the evaluation of the framework is limited to a relatively small number of datasets and model architectures... a wider range of datasets and models would further demonstrate the framework's generalizability.\" - Similar to Assistant A's fourth point, actionable. Authors could expand experiments.\n  - \"Finally, the paper lacks a detailed discussion of the limitations of the proposed framework and potential future research directions... The paper also does not thoroughly outline potential future research directions...\" - Actionable suggestion. Authors could expand the conclusion and future work section.\n- Comparative Assessment:\n  Both assistants provide constructive feedback. Assistant A's suggestions are generally more specific and actionable. For example, suggesting a direct comparison with FATE, or explicitly mapping defenses to attack types. Assistant B's suggestions are also valuable, but sometimes more general, like \"the paper lacks a significant theoretical contribution\" or \"novelty is limited\". While valid points, they are less directly actionable than Assistant A's detailed suggestions for experiments and analyses. Both identify the lack of computational overhead analysis and limited dataset/model range as weaknesses, which are important and constructive.\n\n**Constructive Value - Judgment:**\n**Evidence-Based Reason:** Both assistants offer constructive criticism and suggestions for improvement. However, Assistant A's feedback is more specific and directly actionable. Suggestions like comparing VFLAIR to FATE quantitatively, explicitly mapping defenses to attacks, and expanding experiments to larger models and diverse datasets provide clearer directions for the authors to improve their paper. While Assistant B also provides valuable points, they are sometimes more general and less immediately translatable into concrete actions for revision.\n**Better Assistant:** Assistant A\n\n---\n**Analytical Depth - Evidence Analysis:**\n- From Assistant A:\n  [Direct quotes and specific examples]\n  - Analyzes the paper's contribution in the context of existing frameworks (FATE).\n  - Identifies the lack of clarity in defense applicability as a weakness.\n  - Points out the missing computational overhead analysis as a significant limitation for practical deployment.\n  - Critiques the limited scope of experimental evaluation regarding model size and dataset diversity.\n  - Raises the issue of fairness, a relevant and important aspect in federated learning.\n  - Discusses the scalability issue with increasing participants.\n  - Questions the practical challenges of real-world deployment.\n  - Questions about attack scenarios, computational bottlenecks, scalability challenges, fairness considerations, dataset selection criteria, and benchmark maintenance. These questions demonstrate a deep engagement with the paper's topic and identify areas needing further clarification or research.\n- From Assistant B:\n  [Direct quotes and specific examples]\n  - Analyzes the paper's novelty and theoretical contribution, categorizing it as more of a system paper.\n  - Identifies the lack of computational overhead analysis.\n  - Points out the limited exploration of advanced attacks.\n  - Critiques the limited range of datasets and models in evaluation.\n  - Discusses the lack of detailed limitation discussion and future directions.\n  - Questions about advanced attacks against defenses, computational overhead impact on scalability, defense impact on convergence, performance in real-world scenarios, limitations and future directions, and comparison to other frameworks. These questions also show a good level of analytical depth and identify key areas for further consideration.\n- Comparative Assessment:\n  Both assistants demonstrate good analytical depth. They both identify key weaknesses and limitations of the paper, such as the lack of computational overhead analysis, limited experimental scope, and missing discussion of real-world deployment challenges. Assistant A's analysis is slightly more detailed in some areas, for example, in breaking down the suggestions into specific points and in the range of questions asked, which cover a slightly broader spectrum of concerns (fairness, benchmark maintenance). Assistant B focuses more on the theoretical and novelty aspects, which is also a valid analytical perspective. Both reviews are comprehensive in their assessment of the paper's strengths and weaknesses.\n\n**Analytical Depth - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate a strong analytical depth by thoroughly examining various aspects of the paper, including its contributions, limitations, experimental design, and practical implications. They both identify crucial weaknesses and ask insightful questions that probe deeper into the paper's methodology and findings. Assistant A's analysis is slightly more granular and covers a broader range of relevant aspects, including fairness and benchmark maintenance, giving it a marginal edge in depth. However, Assistant B's focus on theoretical contributions and advanced attacks also demonstrates a strong analytical understanding.\n**Better Assistant:** Assistant A\n\n---\n**Communication Clarity - Evidence Analysis:**\n- From Assistant A:\n  [Direct quotes and specific examples]\n  - Uses clear and concise language.\n  - Organizes feedback into strengths, weaknesses, and suggestions with bullet points, making it easy to read and understand.\n  - Provides specific examples and justifications for each point.\n  - Questions are clearly formulated and directly relevant to the paper's content.\n- From Assistant B:\n  [Direct quotes and specific examples]\n  - Uses clear and concise language.\n  - Organizes feedback into strengths, weaknesses, suggestions, and questions, similar to Assistant A, enhancing readability.\n  - Provides specific examples and justifications for each point.\n  - Questions are clearly formulated and relevant.\n- Comparative Assessment:\n  Both assistants communicate their feedback with excellent clarity. Both reviews are well-structured, using headings and bullet points to organize their points effectively. The language used is clear, concise, and professional. Both assistants provide specific examples and reasoning to support their claims, making their feedback easy to understand and follow. There is no significant difference in communication clarity between the two reviews.\n\n**Communication Clarity - Judgment:**\n**Evidence-Based Reason:** Both Assistant A and Assistant B demonstrate excellent communication clarity. Their reviews are well-organized, use clear and concise language, and provide specific examples and justifications. Both reviews are easy to read and understand, effectively conveying their feedback to the authors.\n**Better Assistant:** Tie\n\n---\n**Comprehensive Analysis:**\nAssistant A and Assistant B both provide strong and insightful reviews of the paper. Both assistants accurately summarize the paper's contributions, identify key strengths, and point out important weaknesses. They both offer constructive suggestions for improvement and ask relevant questions.\n\nAssistant A's review is slightly stronger in terms of **Constructive Value** and **Analytical Depth**. Assistant A's suggestions are more specific and actionable, providing clearer directions for the authors to revise their paper.  Assistant A also covers a slightly broader range of analytical points, including fairness and benchmark maintenance, and asks questions that delve into a wider spectrum of concerns.\n\nAssistant B's review is also excellent, particularly in its identification of the limitations regarding theoretical contributions and advanced attack scenarios. Assistant B's feedback is also highly constructive and clearly articulated.\n\nIn **Technical Accuracy** and **Communication Clarity**, both assistants are equally strong.\n\n**Overall Judgment:**\n**Evidence-Based Reason:** Both assistants provide high-quality reviews that are technically accurate, constructive, analytically deep, and clearly communicated. While both reviews are commendable, Assistant A's review is marginally better due to its slightly more specific and actionable suggestions and slightly broader analytical scope. This makes Assistant A's feedback potentially more immediately useful for the authors in revising their paper. However, the difference is not substantial, and both reviews are of high caliber.\n**Better Assistant:** Assistant A\n"}
{"id": "syoLhUJmth", "year": 2024, "paper_context": "\\title{From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models}\n\n\\begin{abstract}\n  Multi-modal Large Language Models (MLLMs) have made significant strides in expanding the capabilities of Large Language Models (LLMs) through the incorporation of visual perception interfaces. Despite the emergence of exciting applications and the availability of diverse instruction tuning data, existing approaches often rely on CLIP or its variants as the visual branch, and merely extract features from the deep layers. However, these methods lack a comprehensive analysis of the visual encoders in MLLMs. In this paper, we conduct an extensive investigation into the effectiveness of different vision encoders within MLLMs. Our findings reveal that the shallow layer features of CLIP offer particular advantages for fine-grained tasks such as grounding and region understanding. Surprisingly, the vision-only model DINO, which is not pretrained with text-image alignment, demonstrates promising performance as a visual branch within MLLMs. By simply equipping it with an MLP layer for alignment, DINO surpasses CLIP in fine-grained related perception tasks. Building upon these observations, we propose a simple yet effective feature merging strategy, named \\textbf{COMM}, that integrates \\textbf{C}LIP and DIN\\textbf{O} with \\textbf{M}ulti-level features \\textbf{M}erging, to enhance the visual capabilities of MLLMs. We evaluate \\textbf{COMM} through comprehensive experiments on a wide range of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination. Experimental results demonstrate the superior performance of \\textbf{COMM} compared to existing methods, showcasing its enhanced visual capabilities within MLLMs.\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\n\n\\label{sec:intro}\nLarge Language Models (LLMs)~\\cite{chatgpt, openai2023gpt4, touvron2023llama, llama2, taori2023stanford, chiangvicuna} have made significant strides in the domains of language understanding and generation, achieving remarkable progress recently. Through instruction tuning~\\cite{wei2021finetuned,wang2022self}, existing LLMs demonstrate their versatility as general-purpose models capable of handling a wide range of tasks. This capability unlocks their potential zero-shot learning ability, enabling seamless task switching guided by instructions. Building upon the promising performance of LLMs, researchers are now motivated to enhance their capabilities by incorporating visual signals as inputs. This extension allows the generation of textual outputs that are closely related to visual content, opening up exciting possibilities in the realm of vision-language understanding.\n\nTo this end, Flamingo~\\cite{alayrac2022flamingo} and BLIP2~\\cite{li2023blip2} align the powerful LLMs with a frozen visual encoder to understand visual inputs and perform various vision-language tasks. A series of following works, LLaVA~\\cite{liu2023llava}, InstructBLIP~\\cite{dai2023instructblip}, MiniGPT-4~\\cite{zhu2023minigpt} and mPLUG-OWL~\\cite{ye2023mplug} further improve the ability to follow human instructions by constructing multi-modal instruction-following datasets for training. However, these methods are built on image-level alignments, which suffer from the limited fine-grained understanding (such as region description~\\cite{liu2017referring} and reasoning~\\cite{zellers2019recognition}) and severe object hallucination problem~\\cite{li2023obj_Hallucination}. To this end, GPT4ROI~\\cite{zhang2023gpt4roi} proposes instruction tuning on region-of-interest and unlocks the region-level multimodal capacities. Kosmos-2~\\cite{peng2023kosmos} and Shikra~\\cite{chen2023shikra} further integrate the grounding abilities into LLMs and unlock the referential ability in dialogue, \\emph{i.e.}, enable the user to point to the object or region as input and the model responds with spatial coordinates of bounding boxes. Such grounding capacity can fulfill numerous vision-language tasks, which is a great progress in MLLMs.  \n\nDespite a wide variety of exciting methods and applications, most of existing multi-modal LLMs employ CLIP~\\cite{radford2021learning} or its variants ~\\cite{sun2023eva} as the visual branch, where the features output from the deep layers (\\emph{e.g.}, the penultimate layer) are usually employed as inputs to the language decoders. However, it still lacks analysis that: \\emph{Whether using the Vanilla CLIP features as visual encoder is the best way for MLLMs?} Though the visual encoder of CLIP is apparently well aligned with the word embedding space by image-text contrastive learning, it fails to learn more detailed pixel-level information such as color and positioning due to the global supervision of image captions, which might hinder the fine-grained perception ability in MLLMs. Besides, existing MLLMs have quite unbalanced visual and language encoders (\\emph{e.g.}, ViT-Large-300M vs. Vicuna-7B/13B). Since the language models have succeeded in scaling up the model size with progressively powerful language abilities, the short plate of the Buckets Effect for MLLMs lies in the visual models, which fails to demonstrate emerging capabilities, and suffer from domain gap and limited zero-shot ability. Consequently, it is critical to enhance the visual capabilities for boosting MLLMs. \n\nThis paper presents an extensive investigation into different visual encoders for MLLMs. Four typical visual foundation models are considered, \\emph{i.e.}, image-text contrastive learning CLIP, image-only contrastive learning DINOv2~\\cite{oquab2023dinov2}, masked image modeling MAE~\\cite{he2022masked} and supervised learning DeiT~\\cite{touvron2021training}. We evaluate the performance on commonly-used vision-language tasks including visual grounding, object hallucination, visual question answering, image captioning and MME benchmark. \nOur analysis reveals that different layers of features exhibit varying biases towards local and global patterns. Shallow layer features containing low-level detailed information prove beneficial for fine-grained perception tasks such as grounding and positioning ability, while deep layer features are superior at global understanding. To enhance representation, we propose a multi-level feature merging strategy that incorporates both low-level and high-level features. Surprisingly, when equipped with an MLP layer for alignment, the vision-only model DINOv2 shows promise as a visual branch for MLLMs. We attribute this to the fine-grained localization information captured by DINOv2. Conversely, MAE and DeiT perform inferiorly as visual branches for MLLMs. MAE learns limited semantic information, while DeiT's strong supervised training makes the alignment with the textual space challenging. Based on the above observations, we propose a fusion strategy that integrates \\textbf{C}LIP and DIN\\textbf{O} with \\textbf{M}ulti-level features \\textbf{M}erging), dubbed as \\textbf{COMM}, for boosting the visual branches of MLLMs.\nExperimental results demonstrate clear advantages of the proposed model over existing approaches and highlight the enhanced visual capabilities brought by \\textbf{COMM}. In a nutshell, the contributions of this paper are summarized as follows:\n\\begin{itemize}\n    \\item We are the first to extensively investigate the effectiveness of different visual encoders for MLLMs. Based on the analysis that shallow layer features contain low-level detailed information which is helpful for fine-grained tasks, we propose a multi-level feature fusion strategy to incorporate low-level and high-level features for improving representation.\n    \\item Our analysis indicates that vision-only DINOv2 achieves promising results in MLLMs with only an MLP layer for alignment. Considering fine-grained pixel information in DINOv2 and global semantic information in CLIP, we propose \\textbf{COMM} to fuse the visual embeddings of these two models to enhance visual capabilities for boosting MLLMs.\n    \\item Extensive experiments on a wide range of tasks including visual grounding, referring expression generation, object hallucination, visual question answering and image captioning demonstrate the superiority of \\textbf{COMM} over existing works.\n\\end{itemize}\n\n\n\\section{Related Work}\n\n\\textbf{Multi-modal Large Language Model.} LLMs~\\cite{dai2019transformer,brown2020language} have garnered significant attention in both academia and industry due to their remarkable understanding and generative abilities. The success of LLMs has motivated researchers to explore the integration of vision into these models, leading to the development of powerful multi-modal LLMs (MLLMs). Flamingo~\\cite{alayrac2022flamingo} employs a cross-attention module to extract visual contexts, which are concatenated with text token as input for LLMs. LLaVA~\\cite{liu2023visual} and FROMAGe~\\cite{koh2023grounding} leverage the vision encoder of CLIP to extract visual features, which is aligned to text features using a single linear layer and then input to LLMs. Models such as BLIP-2~\\cite{li2023blip}, mPLUG-OWL~\\cite{ye2023mplug}, MiniGPT-4~\\cite{zhu2023minigpt} and InstructBLIP~\\cite{dai2023instructblip} employ Q-former to extract text-aligned visual features for LLMs. Recently, some interesting works extend LLMs to image retrieval~\\cite{koh2023grounding}, video understanding~\\cite{zhang2023video}, audio~\\cite{su2023pandagpt}, biomedical analysis~\\cite{li2023llava}, control systems~\\cite{driess2023palme}. \n\nIn recent studies, there has been a growing interest in extending MLLMs to improve their fine-grained understanding abilities through region-level image-text alignment. Kosmos-2~\\cite{peng2023kosmos} addresses this by constructing a large-scale dataset of grounded region-text pairs, enabling the integration of grounding abilities into LLMs. GPT4RoI~\\cite{zhang2023gpt4roi} reformulates the bounding box as a spatial instruction format and extracts visual features based on region-of-interest, facilitating region-level multi-modal understanding. Shikra~\\cite{chen2023shikra} proposes a unified model that handles spatial coordinates to possess referential abilities in dialogue contexts. Ferret~\\cite{you2023ferret} and ViP-LLaVA~\\cite{cai2023making} further extends with a broader range of free-form shapes for referring, including points, boxes, sketches and scribbles.\nAdditionally, Qwen~\\cite{Qwen-VL} presents a set of MLLMs that demonstrate remarkable performance across various tasks. However, previous works have predominantly focused on extracting visual features solely from the last few layers of the CLIP model, resulting in an emphasis on global image properties. In this study, we draw attention to the fact that features extracted from shallower layers exhibit a stronger focus on localized properties, which we argue can be more potent in comprehending object locations and image details. Additionally, while CLIP primarily learns globally aligned features, advanced vision-alone models such as DINOv2 excel in capturing more fine-grained vision features. We posit that leveraging these fine-grained vision features can effectively enhance the capabilities of MLLMs, as demonstrated in our analysis. To further advance this line of research, we introduce a novel fusion module that expands and enhances the visual branches, thereby aiming to significantly improve the performance of MLLMs.\n\n\\textbf{Large Vision Foundation Model.} Recent progresses in training vision foundation models with large-scale image data focus on contrastive learning, masked image modeling and supervised training. For one thing, contrastive learning can be conducted in an image-only or image-text manner. DINOv2~\\cite{oquab2023dinov2} pretrains the image encoder on large curated image data, which shows a superior understanding of object parts and scene geometry across image domains. Image-text contrastive learning as CLIP~\\cite{radford2021learning} and EVA-CLIP~\\cite{sun2023eva} employs the natural language as weak supervision to guide the learning of visual features. \nFor another, BEiT~\\cite{HangboBao2021BEiT} predicts discrete tokens based on a pre-trained image tokenizer while iBOT~\\cite{zhou2021ibot} proposes an online image tokenizer. MAE~\\cite{he2022masked} proposes a masked autoencoder for reconstructing image pixels. Besides, DeiT III~\\cite{Touvron2022DeiTIR} proposes a training recipe to achieve promising performance.\nRecent MLLMs employ the vision encoder of CLIP/EVA-CLIP without considering the properties of specific visual models. \nIn this paper, we are the first to re-examine the effectiveness of existing visual models in MLLMs and propose a simple yet effective fusion strategy for boosting visual capabilities.\n\n\n\\section{Analysis of the Visual Branch in MLLMs}\n\nPrevious MLLMs~\\cite{liu2023llava,liu2023improvedllava,zhu2023minigpt,dai2023instructblip,ye2023mplug,peng2023kosmos,chen2023shikra,Qwen-VL,you2023ferret} usually utilize the vision encoder of CLIP as their visual branch. Typically, these models extract features from the last few layers, such as the penultimate layer, which are then fed into an alignment network. Subsequently, the aligned features are concatenated with text tokens to serve as input for the LLMs. While the image-text pretraining of CLIP aligns well with the language model, it primarily learns image-level features but overlooks the richer pixel-level features due to the constraint of limited fine-grained information in the guided captions. Moreover, the deep-layer features primarily focus on global image properties and inadequately explore the intricate details of local object parts. As depicted in Fig.~\\ref{fig:feature}, the visual features extracted from the shallow layers of CLIP and the deep visual features obtained from the visual-only model DINOv2 contain more detailed information regarding local objects, such as shape or texture. Leveraging these detailed features may enhance the MLLMs' fine-grained perception abilities. \n\n\\textbf{Evaluation Settings.} For further analysis, we conduct a series of quantitative experiments using different kinds of visual models, \\emph{i.e.}, image-text contrastive learning CLIP, image-only contrastive learning DINOv2, masked image modeling MAE and supervised learning DeiT. In specific, the visual features extracted from different layers of visual models (based on ViT-Large) are aligned using a linear projection layer and then concatenated with text tokens as the input for LLMs (here we use Vicuna-7B~\\cite{chiangvicuna}). The overall architecture and training process follow Shikra~\\cite{chen2023shikra} but with fewer iterations (9400 iterations, batch size 16 on 4 A800) to save the computation cost. Then, we measure the capability of the trained MLLMs on referring expression comprehension (REC)~\\cite{chen2023shikra}, referring expression generation (REG)~\\cite{peng2023kosmos} and object hallucination benchmark (POPE)~\\cite{li2023obj_Hallucination}. \nDetailed descriptions of these tasks can be referred to Sec.~\\ref{sec:experiments}.\n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=1\\textwidth]{feature.png}\n\\caption{Feature correspondence visualization by computing the cosine similarity of different visual tokens extracted from the shallow and deep layers of CLIP and DINOv2. }\\label{fig:feature}\n\\end{figure}\n\n\\textbf{CLIP as the Visual Branch of MLLMs.} As depicted in Fig.~\\ref{fig:rec}, we observe that different layers of features exhibit varying biases towards grounding and understanding abilities. For instance, the shallow features demonstrate relatively higher accuracy in terms of REC and reach their optimal value at layer 12. Conversely, the deep features achieve higher accuracy in terms of POPE, indicating superior understanding ability. Notably, the relatively deep features (layer 16) display the best REG CIDEr score, showcasing promising region understanding capabilities. Consequently, instead of solely relying on deep features as done in previous works, we argue that integrating both shallow and deep features is crucial for MLLMs with improved overall performance.\n\nWe further explore various merging modes of low-level and high-level features. Denoting the output features from each transformer layer of ViT with a depth of $N$ as $\\mathbf{z}=[ z_1,..,z_i,...,z_N ]$, we discuss several multi-level feature merging (MFM) strategies for combining shallow and deep features, namely: \n\n$\\bullet$ \\emph{Mean(half)}: averaging output patch token features in the second half of the backbone as $z=(z_{N/2}+\\cdots+z_{N})/(N/2)$. \n\n$\\bullet$ \\emph{Mean(all)}: averaging features output by all layers as $z=(z_{1}+\\cdots+z_{N})/N$. \n\n$\\bullet$ \\emph{Layerscale(all)}: learning a scale parameter as the weight to sum features output by all layers as $z=w_1z_{1}+\\cdots+w_Nz_{N}$, where $w_i$ refers to the weight assigned to the $i$-th layer feature and all these weights are dynamically updated and summed up to 1. \n\n$\\bullet$ \\emph{LLN-Layerscale(all)}: using a linear-layernorm module to align the feature space between different layers’ features and then summed by \\emph{Layerscale} as $z=w_1\\mathrm{LLN}(z_{1})+\\cdots+w_N\\mathrm{LLN}(z_{N})$.\n\n$\\bullet$ \\emph{Conv-Layerscale(all)}: using a convolution and bn module to align the feature space between different layers’ features and then summed by \\emph{Layerscale} as $z=w_1\\mathrm{Conv}(z_{1})+\\cdots+w_N\\mathrm{Conv}(z_{N})$. \n\nFig.~\\ref{fig:clipanddino} (a) and (b) shows that simply averaging all shallow and deep features of CLIP can \\emph{de facto} achieve a satisfactory accuracy and \\emph{LLN-Layerscale} strategy further improves performance. With \\emph{LLN-Layerscale} as MFM module, the performance of CLIP can be evidently improved on commonly-used vision-language tasks as shown in Table~\\ref{tab:fusion}. \n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n \\subfloat[Average REC accuracy.]{\n\\includegraphics[width=0.33\\columnwidth]{analysis_rec2.png}}\n\\subfloat[Average POPE accuracy.]{\\includegraphics[width=0.33\\columnwidth]{analysis_pope2.png}}\n\\subfloat[Average REG CIDEr.]{\\includegraphics[width=0.33\\columnwidth]{analysis_reg2.png}}\n\\caption{Average REC, POPE accuracy and REG CIDEr for using different layers of features extracted from various vision models (CLIP, DINOv2 and MAE), as input to MLLMs. Shikra uses the 23rd layer features of CLIP and we reproduce its results with fewer iterations (denoted as Shikra$^*$).}\n\\label{fig:rec}\n\\end{figure}\n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n \\subfloat[ REC acc for CLIP.]{\n\\includegraphics[width=0.26\\columnwidth]{recclip.png}}\n\\subfloat[ POPE acc for CLIP.]{\n\\includegraphics[width=0.248\\columnwidth]{popeclip.png}}\n\\subfloat[REC acc for DINO.]{\\includegraphics[width=0.247\\columnwidth,height=0.222\\columnwidth]{recdino.png}}\n\\subfloat[POPE acc for DINO.]{\\includegraphics[width=0.248\\columnwidth,height=0.222\\columnwidth]{popedino.png}}\n\\caption{Average REC and POPE accuracy for merging different layers of features with mutli-feature merging (MFM) strategies as input to MLLMs for visual backbones of CLIP and DINOv2. }\n\\label{fig:clipanddino}\n\\end{figure}\n\n\\textbf{DINOv2 as the Visual Branch of MLLMs.} \nTo leverage the rich fine-grained visual information present in DINOv2, but not inherently aligned with text, we employ a non-linear Multi-Layer Perceptron (MLP) module to align the image features with the word embedding space. Fig.~\\ref{fig:rec}  demonstrates that the deep-layer features of DINOv2 exhibit superior grounding abilities, as evidenced by higher REC accuracy, and display satisfactory understanding abilities, as indicated by favorable POPE and REG results. Additionally, we explore the efficacy of multi-level feature merging to enhance performance. In contrast to CLIP, the merging of shallow features from DINOv2 leads to a significant performance degradation. Specifically, in Fig.~\\ref{fig:clipanddino}(c) and (d), it is evident that \\emph{Mean(all)} performs notably worse than \\emph{Mean(19-24)} in terms of both REC and POPE accuracy, indicating that the shallow representations lack sufficient semantic information. Building upon the \\emph{LLN-Layerscale} approach, the incorporation of the MLP module for a more potent connection between the visual and text spaces demonstrates a clear improvement in performance. Table~\\ref{tab:fusion} showcases the substantial performance gains achieved by employing \\emph{LLN-Layerscale-MLP} as Multi-Level Feature Merging (MFM) module across various vision language tasks. Further detailed ablation studies on the MLP module are in Section~\\ref{sec:mae}. \n\\begin{table}[!t]\n\\centering\n\\setlength{\\tabcolsep}{2.8pt}\n\\caption{Comparison of the visual model using CLIP, DINOv2 with Multi-level Feature Merging (MFM) and \\textbf{COMM} to incorporate visual embeddings of both models on VL tasks. CLIP baseline use the 23rd layer features, which follows Shikra but with fewer training iterations. DINOv2 baseline is w/o MLP module.\nMME CS and PS indicate cognition and perception score, respectively.\n}\n\\label{tab:fusion}\n\\scalebox{0.82}{\n\\begin{tabular}{l|cccccccc}\n\\toprule\nVisual Model  &Avg REC& Avg POPE &COCO & Flickr30k &MME CS&MME PS   & VQAv2&OK-VQA  \\\\\n\\cmidrule(lr){1-9}\nCLIP&47.3&82.3&125.0&80.7&209.6&1107.8&68.8&44.2 \\\\\nDINOv2&54.8&78.3&118.0&68.9&261.8&930.5&63.1&41.9\\\\\n\\cmidrule(lr){1-9}\nCLIP w/ MFM&  70.0    & 83.4 & 125.8 & 81.0&296.6&1164.4 &69.5  & 44.7 \\\\\nDINOv2 w/ MFM\n& \\textbf{72.8}&83.3&123.4&76.3 &  252.9 &1086.8&68.0 & 42.1 \\\\\n \\cmidrule(lr){1-9}\n\\textbf{COMM}\n& \\textbf{72.8} & \\textbf{83.6}&\\textbf{127.3}&\\textbf{81.9}  & \\textbf{360.4}&  \\textbf{1234.9}&  \\textbf{70.1}& \t\\textbf{45.0}\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{table}\n\n\\textbf{MAE and DeiT as the Visual Branch of MLLMs.} Fig.~\\ref{fig:rec} shows that MAE features achieve acceptable REC accuracy, but suffers large performance drop on POPE and REG evaluation. This is because MAE features lack sufficient semantic information for global or regional understanding. Therefore, MAE is not suitable as the visual branch for MLLMs. DeiT performs even worse than MAE (details in Section~\\ref{sec:mae}). We speculate that this is because supervised training is too strong, which learns a specialized visual space that is difficult to align with the word embedding space.\n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=1\\columnwidth]{overall2.pdf}\n\\caption{Overview of our \\textbf{COMM}. The image is input to the vision encoder of CLIP and DINOv2, and the features from shallow and deep layers are incorporated by multi-level feature merging. The features of DINOv2 are aligned with an MLP and concatenated with features of CLIP, which are input to a linear layer. Then the fused features are concatenated with text tokens as input to LLMs.}\n\\label{fig:overall}\n\\end{figure}\n\n\n\n\\section{\\textbf{COMM}\n\n\\textbf{Architecture Overview.} In this section, we introduce the proposed \\textbf{COMM}, that integrates CLIP and DINO with Multi-level features Merging to enhance the visual capabilities of MLLMs. The overall framework is illustrated in Fig.~\\ref{fig:overall}, \\textbf{COMM} is incorporated into a vision-language instruction following model built upon the\nrecent advanced language and vision-language foundation models. Following the input instructions, our model takes vision and language as inputs to generate text responses following the input instructions.\nSpecifically, we adopt the visual encoder of CLIP and DINOv2 (based on ViT-Large) with our proposed fusion strategy as the visual branch, and Vicuna~\\cite{chiangvicuna} (7B/13B) as language decoder. The visual encoder is downsampled with rate 14, meaning that an image with resolution $H\\times W$ will be represented by a sequence of $\\frac{H}{14}\\times \\frac{W}{14}$ tokens.\nThe fused token features are projected using a linear layer and then concatenated with the instruction tokens as inputs to the language decoder, which is a generic interface to unify various vision-language tasks as text generation task.\n\nSpecifically, denote the visual encoder of CLIP and DINOv2 (ViT Large used) as $f_1$ and $f_2$, respectively. Given an input image $x$, we extract the patch token features output by all layers of CLIP as $f_1(x)=[v_1^{1},...,v_1^{i},...,v_1^{24}]$, where $v_1^i \\in \\mathbf{R}^{N\\times D}$, $N$ is the number of patch tokens and $D$ is the embedding dimension. The features output by the deep layers of DINOv2 are $f_2(x)=[v_2^{19},...,v_2^{i},...,v_2^{24}]$. Then we concatenate the features output by these two models as $\\mathbf{v}=[v_1^{1},...,v_1^{24},v_2^{19},...,v_2^{24}]$. A linear-layernorm module is employed to align the feature space between different layers’ features and layerscale is used to merge multiple layer features as\n\\begin{equation}\n    \\overline{v}_1 = \\sum_{i=1}^{24} \\alpha_i \\cdot \\mathrm{Linear}(\\mathrm{LN}(v_1^{i})), \\qquad \\overline{v}_2 = \\sum_{j=19}^{24} \\beta_j \\cdot \\mathrm{Linear}(\\mathrm{LN}(v_2^{j}))\n\\end{equation}\nwhere $\\alpha$ and $\\beta$ are the learnable scaling parameter. Then, we employ an MLP layer to project the features of DINOv2 and concatenate the output features with that of CLIP as $\\mathbf{\\overline{v}}=[\\overline{v}_1,\\mathrm{MLP}(\\overline{v}_2)]$. Then, a linear layer is employed to match the dimension of visual features to that of text features as $\\mathbf{\\hat{v}}=\\mathrm{Linear}(\\mathbf{\\overline{v}})$. Finally, fused visual features $\\mathbf{\\hat{v}}$ are concatenated with text tokens as input to LLMs.\n\n\n\\section{Experiments}\n\n\\label{sec:experiments}\nIn this section, we conduct extensive evaluation on four kinds of vision-language tasks to comprehensively evaluate the visual understanding ability of our model, namely, Referring Expression Comprehension, Referring Expression Generation, Object Hallucination Benchmark, and Visual Question Answering and Image Captioning.\n\n\\textbf{Training Details.} Similar to previous MLLM methods, \\textbf{COMM} is trained in two stages. In the first pretraining stage, we train the model on the reorganized vision-language dataset as~\\cite{chen2023shikra}, including public VQA, Image Captioning datset and several datasets containing positional annotation RefCOCO, visual gemone~\\cite{krishna2017visualgenome} and Visual-7W~\\cite{mani2020pointqa}. The first pretraining stage is conducted for 100K steps. In the second instruction tuning stage, we set the sampling ratio to 50\\% on LLaVA-Instruct-150K~\\cite{liu2023llava} and Shikra-RD~\\cite{chen2023shikra}. Instead of 224 × 224 resolution currently used by existing MLLMs, we use 336 × 336 resolution to reduce the information loss caused by image down-sampling and promote the fine-grained perception ability. In both stages, we freeze the visual encoder and tune all parameters in LLMs, alignment layer and multi-level feature fusion module. We adopt AdamW~\\cite{DBLP:conf/iclr/LoshchilovH19adamw} as the optimizer and cosine annealing\nscheduler~\\cite{DBLP:conf/iclr/LoshchilovH17cos} as learning rate scheduler with an initial learning rate of\n2e-5 and global batch size of 64. All training runs on 8 NVIDIA A800 GPUs. It takes around 100h for stage one training and 20h for stage two.\n\n\\begin{table*}[!t]\n\\centering\n\\caption{Results on standard referring expression comprehension (REC) task. Generalist VL models can perform various vision-language tasks.\nSpecialist models are designed specifically for localization tasks or generalist pretraining models that undergone finetuning. The results of Shikra, Qwen, Ferret and Griffon are from their papers.\n}\n\\label{tab:rec}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{l|l|cccccccc}\n\\toprule\n\\multirow{2}{*}{Model type}\n& \\multirow{2}{*}{Model}  & \\multicolumn{3}{c}{RefCOCO} & \\multicolumn{3}{c}{RefCOCO+} & \\multicolumn{2}{c}{RefCOCOg} \\\\\n &  & val & test-A & test-B & val & test-A & test-B & val-u & test-u \\\\\n\\cmidrule(lr){1-2}\\cmidrule(lr){3-10}\n\\multirow{9}{*}{\\tabincell{l}{Generalist VL SOTAs \\\\ (w/o finetuning)}}\n& OFA-L*      & 79.96 & 83.67 & 76.39 & 68.29 & 76.00 & 61.75 & 67.57 & 67.58  \\\\\n& VisionLLM-H   & - & 86.70     & - & - & - & - & - & - \\\\\n& Shikra-7B      & 87.01 & 90.61 & 80.24 & 81.60 & 87.36 & 72.12 & 82.27 & 82.19 \\\\\n& Shikra-13B     & 87.83 & 91.11 & 81.81 & 82.89 & 87.79 & 74.41 & 82.64 & 83.16  \\\\\n&Ferret-7B&87.49&91.35&82.45&80.78&87.38&73.14&83.93&84.76\\\\\n&Ferret-13B&89.48&92.41&84.36&82.81&88.14&75.17&85.83&86.34\\\\\n&Griffon-13B&88.00&92.10&81.90&81.50&88.20&73.30&82.90&84.30\\\\\n&Qwen-VL-7B&89.36& 92.26 &85.34 &83.12& 88.25& 77.21& 85.58& 85.48\\\\\n&Qwen-VL-7B-Chat& 88.55& 92.27& 84.51 &82.82& 88.59 &76.79& 85.96 &86.32\\\\\n& \\textbf{COMM-7B (Ours)}     &\\textbf{91.73}& \\textbf{94.06}&\\textbf{88.85}&\\textbf{87.21}&\\textbf{91.74}&\\textbf{81.39}&\\textbf{87.32}&\\textbf{88.33}\\\\\n\\cmidrule(lr){1-2}\\cmidrule(lr){3-10}\n\\multirow{3}{*}{\\tabincell{l}{Specialist SOTAs \\\\ (Specialist/Finetuned)}}\n& G-DINO-L     & 90.56 & 93.19 & 88.24 & 82.75 & 88.95 & 75.92 & 86.13 & 87.02 \\\\\n& UNINEXT-H    & 92.64 & 94.33 & 91.46 & 85.24 & 89.63 & 79.79 & 88.73 & 89.37 \\\\\n& ONE-PEACE    & 92.58 & 94.18 & 89.26 & 88.77 & 92.21 & 83.23 & 89.22 & 89.27 \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table*}\n\\begin{table*}[!t]\n\\centering\n\\setlength{\\tabcolsep}{4.5pt}\n\\caption{Results on standard referring expression generation (REG) task in CIDEr score. We reproduce the results of Shikra-7B using its officially released checkpoint. SLR is a finetuned listener-speaker model with an added reward-based module (SLR).\n}\n\\label{tab:reg}\n\\scalebox{0.95}{\n\\begin{tabular}{l|cccccccc}\n\\toprule\n \\multirow{2}{*}{Model}  & \\multicolumn{3}{c}{RefCOCO} & \\multicolumn{3}{c}{RefCOCO+} & \\multicolumn{2}{c}{RefCOCOg} \\\\\n  & val & test-A & test-B & val & test-A & test-B & val-u & test-u \\\\\n\\cmidrule(lr){1-9}\nSLR~\\cite{yu2017joint}&-&69.7&132.3&-&49.4&70.9&59.2&-\\\\\nSLR+Rerank~\\cite{yu2017joint}&-&77.5&132.0&-&52.0&73.5&66.2&-\\\\\n\\cmidrule(lr){1-9}\nShikra     &75.61&44.26&104.83&56.42& 40.98&68.25&62.71&65.58\\\\\nKosmos-2    & - & - & - & - & - & - & 62.3 & - \\\\\n\\textbf{COMM} (Ours)   &\\textbf{93.35}&\\textbf{54.95}&\\textbf{131.13}&\\textbf{70.00}&\\textbf{52.27}&\\textbf{79.05}&\\textbf{79.22}&\\textbf{77.96}\\\\ \n\\bottomrule\n\\end{tabular}}\n\\end{table*}\n\\subsection{Referring Expression Comprehension}\nTo evaluate the fine-grained understanding and positioning capability of our model, we investigate the referring expression comprehension task on benchmarks as RefCOCO~\\cite{kazemzadeh2014referitgame}, RefCOCO+~\\cite{mao2016refcocog} and RefCOCOg~\\cite{mao2016refcocog}, where models are asked to localize the object described with an expression. As shown in Table~\\ref{tab:rec}, compared with generalist VL models and previous SOTA MLLMs, \\textbf{COMM} achieves significant performance gain on all benchmarks, \\emph{i.e.}, \\textbf{COMM}-7B outperforms Shikra-13B and Qwen-VL-7B-Chat by 4.87\\% and 3.10\\% accuracy on average, respectively. With more powerful visual capabilities of our proposed fusion model, we can evidently surpass recent SOTA MLLMs in a more efficient way, \\emph{e.g.}, using a smaller LLM than Shikra (7B vs. 13B) and less training data than Qwen (3.6M vs. 1.4B). Besides, our generalist model even achieves comparable results with specialist SOTA methods, showing the superior grounding ability of our MLLMs.\n\n\\subsection{Referring Expression Generation}\nMoreover, we evaluate the ability to understand image regions or objects referred via inputting bounding boxes. Instead of referring image regions or objects via detailed text descriptions, directly referring to image regions via its bounding boxes is more effective and can reduce the ambiguity.\nThe experiments are conducted on the referring expression generation task with RefCOCO, RefCOCO+ and RefCOCOg, aiming to generate text descriptions of specific regions in the bounding box. Table~\\ref{tab:reg} shows that our model outperforms Shikra and Kosmos-2 by a considerable margin of 16.51 CIDEr and 16.92 CIDEr gain on RefCOCOg, demonstrating the effectiveness of our model for fine-grained understanding. Besides, \\textbf{COMM} even outperforms finetuned SLR on RefCOCO+ and RefCOCOg.\n\n\\begin{table}[!t]\n\\centering\n\\setlength{\\tabcolsep}{1.5pt}\n\\caption{Object hallucination benchmark using POPE evaluation pipeline \\cite{li2023obj_Hallucination}. The results of Shikra-7B are taken from its paper. Except for Shikra-7B, the other results are obtained from \\cite{li2023obj_Hallucination}.\n}\n\\label{tab:pope_results}\n\\scalebox{0.95}{\n\\begin{tabular}{l|ccccccc}\n\\toprule\nDatasets  &\\textbf{COMM}& Shikra & InstructBLIP  & MiniGPT-4 & LLaVA&MM-GPT & mPLUG-Owl \\\\\n\\cmidrule(lr){1-8}\nRandom\n&   87.29   & 86.90 & \\textbf{88.57} & 79.67 &50.37 &50.10 & 53.97 \\\\\nPopular\n&   \\textbf{86.50}    & 83.97  & 82.77 &69.73  &49.87 &50.00 &50.90  \\\\\nAdversarial\n&   \\textbf{84.50}    & 83.10  & 72.10  &65.17  &  49.70 &50.00& 50.67\\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\\begin{table*}[!t]\n\\centering\n\\renewcommand{\\tabcolsep}{0.9mm}\n\\caption{Results on visual question answering (VQA) and image captioning.\nFor VQA, we evaluate SOTA generalist models and our \\textbf{COMM} onVQAv2 and OK-VQA following the normalization rules. Shikra and LLaVA-1.5~\\cite{liu2023improvedllava} is based on the 13B variant.\nFor image captioning, we evaluate them on COCO and Flickr30k in CIDEr score.\nWe call Flamingo as FM for short.\n}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{l|l|cccccccc}\n\\toprule\n\\multicolumn{2}{c|}{Datasets} &\\textbf{COMM}&LLaVA-1.5&Qwen& Shikra  & FM-80B& BLIP-2 & Unified-IO  & VPGTrans\\\\\n\\cmidrule(lr){1-2}\\cmidrule(lr){3-10}\n\\multirow{4}{*}{VQA}\n& VQAv2$^\\text{val}$     &\\textbf{79.05}&-&-& 75.33  &   -   & 65.2 & -  & 65.2 \\\\\n& VQAv2$^\\text{dev}$     &\\textbf{81.04}&80.0&79.5& 77.36  & 56.3 & 65.0 & 77.9  & - \\\\\n& VQAv2$^\\text{std}$     &\\textbf{81.17}&-&-& 77.51  &  -  &  -  & -  & - \\\\\n& OK-VQA                 &\\textbf{59.18}&-&58.6& 47.16  & 50.6 & 45.9 & 54.0  & 45.0 \\\\\n\\cmidrule(lr){1-2}\\cmidrule(lr){3-10}\n\\multirow{2}{*}{Caption}\n& Flickr30k      &\\textbf{88.2}&-&85.8 & 73.9 &  67.2   &   -  &   -   &   -   \\\\\n& COCO           &\\textbf{132.7}&-&- & 117.5 & 84.3  &   -  & 122.3 &   -  \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\label{tab:vl}\n\\end{table*}\n\\subsection{Object Hallucination Benchmark}\nWe compare our model against the baseline models\non the hallucination evaluation dataset recently introduced by POPE~\\cite{li2023obj_Hallucination}, which randomly selects 500 images from COCO~\\cite{caesar2018coco}. Table~\\ref{tab:pope_results} shows that \\textbf{COMM} surpasses recent popular MLLMs with 1.44\\% and 4.95\\% higher accuracy on average than Shikra and InstrutBLIP, respectively. By enhancing the fine-grained visual capabilities, \\textbf{COMM} can effectively alleviate the object hallucination problem.\n\n\\subsection{Visual Question Answering and Image Captioning}\nWe evaluate \\textbf{COMM} on conventional VL tasks of VQA and Image Captioning. Specifically, image captioning requires the model to generate description for the given image and VQA asks the model to generate answer for the given image-question pair. For image captioning, we choose COCO~\\cite{chen2015cococap} and Flickr30K~\\cite{plummer2015flickr30ke} as benchmarks and report the CIDEr score. For VQA task, we experiment on VQAv2~\\cite{antol2015vqav2} and OK-VQA~\\cite{marino2019ok}. \nAs shown in Table~\\ref{tab:vl}, \\textbf{COMM} achieves state-of-the-art performance on image captioning task, \\emph{i.e.}, 88.2 CIDEr score on Flickr30K and 132.7 CIDEr score on COCO, even outperforms previous SOTA models with much more parameters (\\emph{e.g.}, Shikra-13B with 13B parameters) or much more training data (\\emph{e.g.}, Qwen with 1.4B data). For VQA task, our model also shows significant advantages compared to other MLLMs. On VQAv2 val, dev and std, our model achieves 79.05, 81.04 and 81.17 accuracy respectively, which surpasses recent proposed Shikra with the same training data and procedure by a large margin, demonstrating the effectiveness of merging visual embeddings of DINOv2 and CLIP for enhancing visual capabilities. Besides, our \\textbf{COMM} model outperforms Qwen with 1.54 and 0.58 accuracy gain on VQAv2 dev and OK-VQA respectively with less VQA training data, \\emph{i.e.}, we use 0.6M and Qwen with 3.6M. Training with more VQA data might further improve performance and we leave it as future work.\n\\subsection{Ablation Study}\n\\label{sec:mae}\n\\textbf{Ablation on the MLP of DINOv2.} We conduct ablation study on the design of the MLP module in DINOv2 for aligning visual and text embedding space. \nWe ablate on the number and the expanding ratio of MLP module.\nTable~\\ref{tab:mlp} shows that increasing the number of MLP to 2 can evidently improve performance, demonstrate the effectiveness of using a more powerful network to align the vision only model DINOv2 to the word embedding space. However, increasing the number beyond 2 suffers the degraded performance. For the expanding ratio, increasing to 8 can improve performance, while increasing to 16 does not achieve significant performance gain. Moreover, we experiment with one linear layer, which suffers severe performance degradation. Thus, non-linear MLP is necessary for aligning the features of vision-only DINOv2 to the word embedding space.\n\n\\textbf{Ablation on the visual model of MAE and DeiT.} As shown in Table~\\ref{tab:deit}, MAE and DeiT suffers from evident performance degradation. For one thing, the visual features of MAE lack sufficient semantic information for global or regional understanding. For another, the supervised training of DeiT is so strong that it learns specialized visual space, making it difficult to align with the word embedding space. \n\\begin{table}[!t]\n\\centering\n\\setlength{\\tabcolsep}{2pt}\n\\caption{Ablation study on the number and expanding ratio of MLP module. Experiments are conducted on referring expression comprehension and object hallucination benchmark on Random (R), Adversarial (A), and Popular (P).\n}\n\\label{tab:mlp}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|ccccccccc}\n\\toprule\n\\multirow{2}{*}{Visual Model}  & \\multicolumn{3}{c}{RefCOCO+} & \\multicolumn{2}{c}{RefCOCOg}& \\multicolumn{3}{c}{RefCOCO}  &POPE\\\\\n& test-A & test-B &val& test-u&  val-u  &  test-A & test-B &val  &A/P/R\\\\\n \\cmidrule(lr){1-10}\nDINOv2 w/ MLP Ratio 4\n& 75.3 &59.3&67.0&73.0&71.8&84.4&74.1&79.6&80.3/84.2/85.5  \\\\\n\\cmidrule(lr){1-10}\nDINOv2 w/ 2MLP Ratio 4\n& \\textbf{77.5}& \t\\textbf{60.3} &69.2&\t\\textbf{74.6} &\\textbf{74.7} &\\textbf{86.5} &75.3 &81.4 &\\textbf{82.4}/84.5/86.2 \\\\\nDINOv2 w/ 4MLP Ratio 4\n& 53.7& \t34.4 &45.3&\t49.0 &48.8&\t65.4 &48.0 &57.9 &79.2/82.9/84.6 \\\\\nDINOv2 w/ 8MLP Ratio 4\n& 8.2 & \t6.5 & \t7.4 & \t6.8 & 6.7 & 14.8 & 12.9 & 14.9& 56.0/55.3/59.0\\\\ \n\\cmidrule(lr){1-10}\nDINOv2 w/ MLP Ratio 8\n&77.4 &59.9 &\\textbf{69.7} &73.7 &73.3 &85.7&74.1 &80.9 &81.5/\\textbf{85.8}/\\textbf{86.7}\\\\\nDINOv2 w/ MLP Ratio 16\n&76.2 &60.2 &\\textbf{69.7} &74.5&74.6&85.7 &\\textbf{75.5} &\\textbf{81.5} &80.4/83.7/85.7\\\\ \n\\cmidrule(lr){1-10}\nDINOv2 w/ Linear\n&61.8& \t48.8 &\t55.1 &\t64.1 &\t62.9 &\t76.5 &\t67.0 &\t71.9& \t75.6/79.3/83.7 \\\\\n\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\\begin{table}[!t]\n\\centering\n\\setlength{\\tabcolsep}{5.3pt}\n\\caption{Comparison of the visual model using CLIP, DINOv2 with our multi-level feature merging (MFM), MAE and DeiT. MAE-20 denotes using the features output by the 20-th layer of MAE. DeiT-20 denotes using the features output by 20-th layer.\n}\n\\label{tab:deit}\n\\scalebox{1}{\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{l|ccccccccc}\n\\toprule\n\\multirow{2}{*}{Visual Model}  & \\multicolumn{3}{c}{RefCOCO+} & \\multicolumn{2}{c}{RefCOCOg}& \\multicolumn{3}{c}{RefCOCO}  &POPE\\\\\n& test-A & test-B &val& test-u&  val-u  &  test-A & test-B &val  &A/P/R\\\\\n\\cmidrule(lr){1-10}\nCLIP w/ MFM&  73.7&53.8 &64.3 &69.1 &70.3&83.8 &68.4&76.4&\\textbf{80.7}/\\textbf{84.2}/\\textbf{85.8} \n \\\\\n \\cmidrule(lr){1-10}\nDINOv2 w/ MFM\n& \\textbf{75.3} &\\textbf{59.3}&\\textbf{67.0}&\\textbf{73.0}&\\textbf{71.8}&\\textbf{84.4}&\\textbf{74.1}&\\textbf{79.6}&80.3/\\textbf{84.2}/85.5  \\\\\n\\cmidrule(lr){1-10}\nMAE-20&64.7&\t49.4&\t56.8\t&63.7&\t62.8&\t77.9&\t68.6\t&73.6\t&66.8/71.1/76.7\\\\\nMAE-22&65.9& \t50.0 &\t58.5 &\t64.2 &\t63.2& \t79.3& \t69.8& \t74.9 &\t68.0/71.2/77.5\\\\ \n\\cmidrule(lr){1-10}\nDeiT-20&18.4&\t13.0&\t15.9&\t17.0\t&16.2\t&29.0&\t21.6&\t25.7&\t66.2/69.6/77.9\\\\\nDeiT-22&25.3&\t15.4&\t19.4&\t22.6&\t21.8\t&36.9&\t25.3&\t32.0&\t67.9/71.6/78.7\\\\\n\\bottomrule\n\\end{tabular}%\n}}\n\\end{table}\n\\subsection{Demonstrations}\nAs shown in Fig.~\\ref{fig:case}, our \\textbf{COMM} model exhibits a multitude of promising capabilities including visual grounding, fine-grained region understanding and robustness to object hallucination. The first example showcases our strong fine-grained perception ability, which identifies implicit strawberries in a blender. The second example exhibits our strong visual grounding ability to successfully locates the object of sugar. The third case demonstrates our robustness to object hallucination. In contrast, Shikra fails on these challenging cases, showing the superior capabilities of our model. We provide additional demonstrations of our \\textbf{COMM} model in this section to demonstrate a multitude of promising capabilities including visual grounding, fine-grained region understanding and robustness to object hallucination. For instance, we showcase Referring Expression Comprehension in Fig.~\\ref{fig:caserec} and Object Hallucination in Fig.~\\ref{fig:caseobj}.\n\\begin{figure}[h!]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=1\\columnwidth]{rec_case.png}\n\\caption{Referring Expression Comprehension (REC) using our \\textbf{COMM}-7B. The task intends to localize a target object in an image described by a referring expression.}\n\\label{fig:caserec}\n\\end{figure}\n\\begin{figure}[!h]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=1\\columnwidth]{object.png}\n\\caption{Object hallucination using our \\textbf{COMM}-7B. This task aims to evaluate the robustness to object hallucination, \\emph{i.e.}, answer yes or no for the existence of questioned object.}\n\\label{fig:caseobj}\n\\end{figure}\n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=0.9\\columnwidth]{case2.png}\n\\caption{Qualitative comparison between Shikra with its official checkpoint and our \\textbf{COMM}.}\n\\label{fig:case}\n\\end{figure}\n\n\n\\section{Conclusion}\n\nThis paper presented an extensive investigation into the efficacy of different visual models when employed as the visual branch in MLLMs. Through a systematic analysis, we highlight the significance of shallow layer features, which capture low-level details that prove beneficial for grounding and positioning tasks. Furthermore, we recognize the potential of the vision-only model DINOv2, which leverages its inherent fine-grained pixel-level information for enhanced fine-grained perception in MLLMs when combined with an MLP layer for alignment purposes. Motivated by our analysis, we introduce a fusion approach to combine the visual features obtained from CLIP and DINOv2, thereby further augmenting the visual capabilities and performance of MLLMs. Through qualitative analysis and extensive quantitative experiments, we demonstrate the effectiveness of our proposed method, surpassing the performance of existing MLLM models across diverse benchmark datasets. Looking ahead, we encourage future research to explore the integration of more powerful vision models to enhance the capabilities of visual branches in MLLMs. We believe that this avenue of investigation holds the key to unlocking the potential of the next generation of MLLMs.\n\n\n\n\n", "DeepReviewer": "\n## Summary:\n\nThis paper introduces COMM, a novel approach to enhance the visual capabilities of Multimodal Large Language Models (MLLMs) by integrating the strengths of CLIP and DINOv2 through a multi-level feature merging strategy. The authors' core contribution lies in their investigation of different visual encoders within MLLMs, demonstrating that combining the global semantic understanding of CLIP with the fine-grained localization information from DINOv2 leads to improved performance across a range of vision-language tasks. The COMM method employs a multi-level feature merging (MFM) strategy, which aligns and combines features from different layers of both CLIP and DINOv2 using a linear layer, layer normalization, and learnable scaling parameters. For DINOv2, which lacks inherent text alignment, an MLP layer is used to project its features into the word embedding space. The authors evaluate their method on a variety of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination, demonstrating superior performance compared to existing methods. The empirical findings suggest that the proposed feature merging strategy effectively leverages the complementary strengths of CLIP and DINOv2, resulting in enhanced visual understanding within MLLMs. The paper's overall significance lies in its exploration of visual encoder effectiveness and the introduction of a practical method for improving visual capabilities in MLLMs. However, the paper also presents several limitations, particularly in terms of the practical applicability of the method due to the large feature vector size, the lack of a strong theoretical foundation, and the limited generalizability of the results. Despite these limitations, the paper provides valuable insights into the potential of combining different visual encoders for MLLMs and offers a promising direction for future research.\n\n\n## Soundness:\n\n2.75\n\n\n## Presentation:\n\n2.75\n\n\n## Contribution:\n\n2.75\n\n\n## Strengths:\n\nI find several aspects of this paper to be commendable. Firstly, the paper's core idea of combining CLIP and DINOv2 to leverage their complementary strengths is both intuitive and effective. The authors' analysis of the different biases of CLIP and DINOv2, with CLIP excelling in global understanding and DINOv2 capturing fine-grained details, provides a solid rationale for their approach. The empirical results, demonstrating superior performance on a range of vision-language tasks, validate the effectiveness of the proposed COMM method. Specifically, the performance gains on tasks like Referring Expression Comprehension (REC), Object Hallucination Benchmark (POPE), and Visual Question Answering (VQA) are significant and showcase the enhanced visual capabilities of MLLMs when using COMM. The authors' exploration of different feature merging strategies, including the use of linear-layernorm and learnable scaling parameters, is a valuable contribution. The use of an MLP layer to align DINOv2's features with the word embedding space is also a practical and effective solution. Furthermore, the paper is well-written and easy to follow, making the methodology and results accessible to a broad audience. The authors' focus on practical application and empirical validation is a strength, as it demonstrates the real-world potential of their approach. The paper's exploration of visual encoder effectiveness within MLLMs is a significant contribution, as it highlights the importance of carefully selecting and combining visual encoders to achieve optimal performance. Finally, the paper's demonstration of the COMM method's ability to enhance visual capabilities in MLLMs is a promising direction for future research in this field.\n\n\n## Weaknesses:\n\nDespite the strengths of this paper, I have identified several significant weaknesses that warrant careful consideration. The most pressing limitation is the practical applicability of the COMM method due to the large visual feature length. The authors explicitly state that they concatenate features from both CLIP and DINOv2, which are both based on ViT-Large. Given the image resolution of 336x336 and the downsampling rate of 14, each encoder outputs 576 tokens per layer. Concatenating features from multiple layers of both encoders results in a feature vector that can easily exceed the context window of standard LLMs like LLaMA2, which has a maximum input length of 4096 tokens. This is a critical issue, as it makes it difficult to apply the COMM method to most existing MLLMs without significant modifications or feature compression techniques. This limitation is explicitly stated in the paper, where the authors mention the concatenation of features and the resulting token sequence (lines 296-298). My analysis confirms that this concatenation, even with multi-level merging, leads to a feature vector that could easily exceed the typical context window of transformer-based LLMs, creating a significant bottleneck for practical deployment. This is a high-confidence concern, as it is directly supported by the paper's methodology and the known limitations of standard LLMs. \n\nFurthermore, the reliance on two distinct visual encoders introduces a computational overhead that may not be feasible for resource-constrained environments or real-time applications. The paper confirms the use of both CLIP and DINOv2 as visual encoders (lines 291-292). Processing two large visual encoders and handling the resulting large feature vector will undoubtedly increase the computational cost, making the method less practical for deployment in resource-constrained environments. This is another high-confidence concern, as it is a direct consequence of the method's design. The paper lacks a strong theoretical foundation to explain why the COMM method enhances the visual capabilities of MLLMs. While the authors provide some intuition about the complementary strengths of CLIP and DINOv2, there is no formal analysis of the feature spaces or a theoretical framework explaining the benefits of the specific merging strategy. The paper mentions that shallow layers capture local details and deep layers capture global semantics (lines 49-52), and that DINOv2 captures fine-grained localization (lines 53-55). However, there is no mathematical analysis of the feature spaces of CLIP and DINOv2, nor is there a formal justification for the chosen merging strategy. This lack of theoretical grounding makes it difficult to understand the underlying mechanisms of the method and limits the generalizability of the findings. This is a high-confidence concern, as it is evident from the absence of a formal theoretical framework in the paper. The COMM method's generalizability is also limited, as it is primarily evaluated within the Shikra framework. The paper explicitly mentions using Vicuna as the language decoder and following the training setup of Shikra (lines 291-294, 311-313). While the architecture description is general, the experimental setup heavily relies on the Shikra framework's training data and procedure. This makes it difficult to assess the robustness of the proposed method across different MLLM designs. This is a high-confidence concern, as it is directly supported by the paper's experimental setup. The COMM method also lacks significant innovation in terms of feature merging techniques. The paper describes the MFM strategy as involving linear-layernorm for alignment and learnable scaling parameters for merging (lines 59-61). For DINOv2, an MLP is used for alignment (lines 61-63). While effective, these techniques are relatively standard and do not represent a significant architectural innovation. This is a high-confidence concern, as it is evident from the description of the MFM strategy in the paper. The paper's evaluation is limited to a few general visual tasks, such as REC, REG, POPE, VQA, and Image Captioning (lines 320-322). It does not include more specialized domains like medical imaging or video understanding. This lack of evaluation on specialized tasks limits the understanding of COMM's performance in more complex and domain-specific scenarios. This is a high-confidence concern, as it is evident from the list of evaluated tasks in the paper. Finally, the paper lacks sufficient interpretability, as it does not provide a detailed analysis of how the different feature levels contribute to the overall performance. While the paper provides some high-level analysis of feature layer contributions (lines 137-138, 142-144) and shows performance improvements, there is no detailed analysis of how the learnable scaling parameters in the MFM strategy affect the contribution of different feature levels, or a deeper dive into the interaction between CLIP and DINOv2 features. This lack of interpretability makes it harder to understand the specific mechanisms behind the performance gains and limits the ability to further refine the method. This is a high-confidence concern, as it is evident from the lack of detailed analysis in the paper. The paper also fails to convincingly demonstrate the unique advantages of the proposed method over existing approaches, particularly given the increased computational cost and complexity. While the paper shows performance improvements, the simplicity of the method and lack of deep interpretability, coupled with potential computational costs, might not fully justify the claimed advantages. This is a medium-confidence concern, as it is a more subjective assessment of the paper's overall contribution. In summary, while the paper presents a promising approach, these limitations significantly impact its practical applicability and generalizability.\n\n\n## Suggestions:\n\nTo address the identified weaknesses, I recommend several concrete improvements. Firstly, the authors should explore techniques to reduce the dimensionality of the combined feature space. Given the concatenation of features from both CLIP and DINOv2, the resulting feature vector can be quite large, potentially exceeding the context window of standard LLMs. To mitigate this, the authors could investigate methods such as Principal Component Analysis (PCA) or autoencoders to compress the feature representation while preserving the most informative components. This would make the method more applicable to a wider range of MLLMs and deployment scenarios. Furthermore, the authors should investigate the possibility of using more efficient feature merging strategies that do not rely on the concatenation of all feature levels. For example, they could explore attention-based mechanisms to selectively combine the most relevant features from CLIP and DINOv2. This would allow the model to focus on the most informative features, reducing the dimensionality of the combined feature space and potentially improving performance. Another approach could be to investigate knowledge distillation techniques to transfer the knowledge of the combined model into a more compact representation. This would allow the authors to leverage the performance of the combined model while reducing the computational cost of deployment. To enhance the theoretical grounding of the COMM method, the authors should provide a more detailed analysis of the feature spaces of CLIP and DINOv2. This could involve visualizing the feature distributions, analyzing the correlation between different feature levels, and investigating the impact of different merging strategies on the resulting feature representation. A formal analysis of the information content of the different feature levels would also be beneficial. The authors should also explore the possibility of using techniques such as information bottleneck to identify the most informative features for the downstream tasks. This would provide a more solid theoretical foundation for the proposed method and help to understand why it works. To address the limited generalizability of the method, the authors should significantly broaden the evaluation of the COMM method to include a more diverse set of MLLM architectures and visual tasks. This should include evaluating the method on different LLM backbones, such as LLaMA3, and on more specialized tasks such as medical image analysis or video understanding. The authors should also provide a more detailed analysis of the computational cost of the proposed method, including the memory footprint and inference time. This would help to assess the practical feasibility of the method for real-world applications. Furthermore, the authors should provide a more detailed analysis of the interpretability of the method, including visualizing the contribution of different feature levels to the final prediction. This would help to understand the underlying mechanisms of the proposed method and provide insights for further improvements. Finally, the authors should explore more sophisticated feature fusion techniques to enhance the innovation of the method. This could involve exploring cross-attention mechanisms that allow for more fine-grained interaction between the features of CLIP and DINOv2, or investigating adaptive layer selection mechanisms, perhaps guided by a task-specific objective. These improvements would not only address the identified weaknesses but also enhance the overall impact and significance of the paper.\n\n\n## Questions:\n\nI have several questions regarding the methodology and results presented in this paper. Firstly, given the significant increase in visual feature length due to the concatenation of CLIP and DINOv2 features, what specific strategies do the authors propose to mitigate the context window limitations of standard LLMs? Have they explored any feature compression techniques or alternative merging strategies that could reduce the dimensionality of the combined feature space without sacrificing performance? Secondly, considering the lack of a formal theoretical framework, what specific analyses have the authors conducted to understand the interaction between the feature spaces of CLIP and DINOv2? Have they visualized the feature distributions or analyzed the correlation between different feature levels? What is the rationale behind the specific merging strategy, and how does it impact the information content of the resulting feature representation? Thirdly, given the limited evaluation of the COMM method on the Shikra framework, what are the authors' plans to evaluate the method on a more diverse set of MLLM architectures? What specific challenges do they anticipate when applying the method to different LLM backbones, and how do they plan to address these challenges? Fourthly, considering the relatively straightforward nature of the feature merging strategy, what are the authors' plans to explore more sophisticated feature fusion techniques? Have they considered attention-based mechanisms or adaptive layer selection mechanisms? What are the potential benefits and drawbacks of these more complex approaches? Fifthly, given the limited evaluation on general visual tasks, what are the authors' plans to evaluate the method on more specialized domains, such as medical image analysis or video understanding? What specific challenges do they anticipate when applying the method to these more complex and domain-specific scenarios? Finally, considering the lack of interpretability of the method, what specific analyses have the authors conducted to understand the contribution of different feature levels to the final prediction? Have they visualized the feature maps or used techniques like t-SNE to project the high-dimensional features into a lower-dimensional space? What are the specific mechanisms behind the performance gains observed with the COMM method?\n\n\n## Rating:\n\n4.5\n\n\n## Confidence:\n\n5.0\n\n\n## Decision:\n\nReject", "other": "\n## Summary:\n\nThis paper introduces COMM, a novel multi-modal large language model (MLLM) that leverages the strengths of both CLIP and DINOv2 visual encoders. The core idea is to combine CLIP's image-text alignment capabilities with DINOv2's fine-grained visual representations. The authors propose a multi-level feature merging (MFM) strategy to integrate features from different layers of both encoders, aiming to enhance the model's performance on various vision-language tasks. Specifically, COMM employs a linear projection layer with layer normalization (LLN) and layer scaling to align and merge features from CLIP and DINOv2. For DINOv2, which lacks inherent text alignment, a non-linear multi-layer perceptron (MLP) module is used to align its features with the text embedding space. The authors conduct extensive experiments on a range of tasks, including visual question answering (VQA), image captioning, visual grounding, and object hallucination. The empirical results demonstrate that COMM generally outperforms existing MLLMs, particularly in tasks requiring fine-grained visual understanding. The paper also includes ablation studies to analyze the impact of different feature merging strategies and the contribution of each visual encoder. The authors emphasize the importance of combining both low-level and high-level features for improved representation learning. Overall, the paper presents a compelling approach to integrating diverse visual encoders into MLLMs, showcasing the potential of combining contrastive and self-supervised learning for enhanced vision-language understanding. The findings suggest that leveraging the complementary strengths of different visual models can lead to significant performance gains in multimodal tasks. However, the paper also has some limitations, particularly in terms of the novelty of the approach and the scope of the experimental evaluation, which I will discuss in detail in the following sections.\n\n\n## Soundness:\n\n2.75\n\n\n## Presentation:\n\n2.75\n\n\n## Contribution:\n\n2.0\n\n\n## Strengths:\n\nI found several aspects of this paper to be commendable. First, the core idea of combining CLIP and DINOv2 visual encoders is well-motivated. The authors correctly identify the strengths of each model: CLIP's strong image-text alignment and DINOv2's fine-grained visual representations. The proposed multi-level feature merging (MFM) strategy, while not entirely novel in its individual components, is a reasonable approach to integrate features from different layers of both encoders. The use of a non-linear MLP to align DINOv2's features with the text embedding space is a necessary step, given DINOv2's lack of inherent text alignment. The empirical results presented in the paper are generally strong, demonstrating that COMM outperforms existing MLLMs on a variety of tasks, including VQA, image captioning, visual grounding, and object hallucination. The ablation studies provide valuable insights into the impact of different feature merging strategies and the contribution of each visual encoder. The authors also highlight the importance of combining both low-level and high-level features for improved representation learning, which is a well-established concept in computer vision. The paper is generally well-written and easy to follow, with clear explanations of the proposed method and experimental setup. The authors also provide a detailed analysis of the performance of COMM on different tasks, which helps to understand the strengths and weaknesses of the proposed approach. The inclusion of experiments on visual grounding and object hallucination is particularly valuable, as these tasks are often challenging for MLLMs. The paper also includes a detailed analysis of the performance of COMM on different tasks, which helps to understand the strengths and weaknesses of the proposed approach. The inclusion of experiments on visual grounding and object hallucination is particularly valuable, as these tasks are often challenging for MLLMs. Finally, the paper's focus on fine-grained visual understanding is timely and relevant, given the increasing importance of this capability in many real-world applications.\n\n\n## Weaknesses:\n\nDespite the strengths of this paper, I have identified several weaknesses that warrant careful consideration. First, the novelty of the proposed method is somewhat limited. While the authors combine CLIP and DINOv2, the core idea of merging multiple features from different encoders is not entirely new, as similar approaches have been explored in models like Qwen-VL. The paper acknowledges Qwen-VL but doesn't provide a direct comparison of the feature merging strategies, which would have been beneficial. The use of a simple MLP for aligning DINOv2 features, while effective, is also not a groundbreaking contribution. The paper's claim of being the 'first to extensively investigate the effectiveness of different visual encoders' is also debatable, as other works like Qwen-VL have also explored this area. This lack of clear novelty is a significant concern, as it diminishes the overall contribution of the paper. My confidence in this assessment is high, as the paper itself acknowledges the existence of similar approaches, and the lack of a direct comparison makes it difficult to ascertain the unique contribution of COMM's feature merging strategy. Second, the paper lacks a comprehensive comparison with other state-of-the-art MLLMs. While the authors compare COMM with several models, the selection is not always consistent across different tasks, and some recent models, such as MiniCPM-V, are missing. This makes it difficult to assess the true performance of COMM relative to the current state-of-the-art. Furthermore, the paper does not include comparisons with models that use the same ViT-Large backbone, which would have provided a more controlled comparison. This is a significant limitation, as it makes it difficult to isolate the impact of the proposed feature merging strategy from the impact of the backbone architecture. My confidence in this assessment is high, as the absence of key models and consistent comparisons is evident from the paper's tables. Third, the paper lacks a detailed analysis of the computational cost of COMM. The authors do not provide information on the number of parameters, FLOPs, or inference time for COMM or the compared models. This is a critical omission, as the computational cost is a crucial factor in the practical applicability of any model. Without this information, it is difficult to assess the trade-off between performance and computational cost for COMM. My confidence in this assessment is high, as the absence of computational cost information is clear from the paper. Fourth, the paper does not adequately address the potential for redundancy when combining two powerful visual encoders. While the authors highlight the complementary strengths of CLIP and DINOv2, they do not provide a detailed analysis of the feature space to demonstrate that the combined features are indeed capturing distinct aspects of the visual input. This is a concern, as simply concatenating features from two models can lead to redundancy and increased computational cost without a corresponding increase in performance. My confidence in this assessment is medium, as the paper does provide some justification for the combination, but lacks a detailed analysis of feature redundancy. Fifth, the paper's claim that DINOv2 is not inherently aligned with text is not fully supported by the evidence. While DINOv2 is not trained with text-image pairs, it has been shown to possess emergent localization capabilities that could be leveraged for alignment. The paper does not explore the possibility of fine-tuning the MLP alignment layer with text-image pairs to leverage DINOv2's localization capabilities for better text alignment. This is a missed opportunity, as it could have led to improved performance. My confidence in this assessment is high, as the paper itself acknowledges DINOv2's localization capabilities, but does not explore their potential for text alignment. Finally, the paper does not explicitly discuss the limitations of the COMM method. This is a significant omission, as it is important to understand the potential drawbacks and challenges of any proposed approach. My confidence in this assessment is high, as the absence of a dedicated limitations section is clear from the paper.\n\n\n## Suggestions:\n\nBased on the identified weaknesses, I recommend several concrete improvements for this paper. First, the authors should conduct a more thorough comparison with state-of-the-art MLLMs, including models like MiniCPM-V. The comparison should be consistent across all tasks, and the authors should try to control for the backbone architecture when comparing different feature merging strategies. This would provide a more accurate assessment of the performance of COMM relative to the current state-of-the-art. Second, the authors should provide a detailed analysis of the computational cost of COMM, including the number of parameters, FLOPs, and inference time. This information is crucial for assessing the practical applicability of the model. The authors should also compare the computational cost of COMM with other MLLMs. Third, the authors should conduct a more detailed analysis of the feature space to demonstrate that the combined features from CLIP and DINOv2 are indeed capturing distinct aspects of the visual input. This could involve visualizing the feature maps or using other techniques to analyze the feature representations. This would help to address the concern about potential redundancy. Fourth, the authors should explore the possibility of fine-tuning the MLP alignment layer with text-image pairs to leverage DINOv2's emergent localization capabilities for better text alignment. This could potentially lead to improved performance. The authors should also compare the performance of COMM with a version that uses a CLIP backbone with an expanded MLP module. Fifth, the authors should include a dedicated section discussing the limitations of the COMM method. This would provide a more balanced and realistic assessment of the proposed approach. Sixth, the authors should consider exploring more sophisticated feature fusion techniques beyond simple concatenation and linear projection. This could involve exploring attention-based fusion mechanisms or other non-linear methods. This could potentially lead to further performance improvements. Seventh, the authors should provide a more detailed explanation of the differences between COMM and other multi-branch MLLMs, such as Qwen-VL. This would help to clarify the unique contribution of COMM. Finally, the authors should consider conducting experiments on additional tasks, such as long-text captioning and multi-modal in-context learning, to further demonstrate the capabilities of COMM. These improvements would address the identified weaknesses and strengthen the overall contribution of the paper.\n\n\n## Questions:\n\nI have several questions that arise from my analysis of this paper. First, what is the specific rationale for choosing the particular layers of CLIP and DINOv2 for feature merging? The paper mentions that shallow layers of CLIP are beneficial for grounding and positioning tasks, but a more detailed explanation of the layer selection process would be helpful. Second, how sensitive is the performance of COMM to the choice of the MLP architecture used for aligning DINOv2 features? The paper includes some ablation studies on the number and expanding ratio of the MLP layers, but a more thorough analysis of the MLP architecture would be beneficial. Third, what is the impact of the proposed feature merging strategy on the interpretability of the model? Does the combination of features from different encoders make it more difficult to understand how the model is making its predictions? Fourth, how does the performance of COMM scale with the size of the training data? Does the model benefit from larger training datasets, and how does this compare to other MLLMs? Fifth, what is the impact of the image resolution on the performance of COMM? Does the model benefit from higher resolution images, and how does this affect the computational cost? Sixth, how does the performance of COMM compare to other methods on more challenging visual reasoning tasks? Does the model exhibit any limitations in its ability to perform complex visual reasoning? Finally, what are the potential ethical implications of using a model like COMM, which combines multiple powerful visual encoders? Are there any potential biases or risks associated with the use of such a model? These questions target key uncertainties in the paper's methodology and seek clarification of critical assumptions, which would help to further understand the proposed approach.\n\n\n## Rating:\n\n5.25\n\n\n## Confidence:\n\n4.25\n\n\n## Decision:\n\nReject", "other_type": "DeepReviewer_fast", "v.s.": "B", "result": "**Technical Accuracy - Evidence Analysis:**\n- From Assistant A:\n  - Claim: \"The core idea of combining CLIP and DINOv2 visual encoders is well-motivated.\"\n    - Evidence: \"The authors correctly identify the strengths of each model: CLIP's strong image-text alignment and DINOv2's fine-grained visual representations.\"\n    - Analysis: This claim is technically accurate. The paper explicitly states the motivation for combining CLIP and DINOv2 based on their complementary strengths (Introduction and Section 3).\n  - Claim: \"The proposed multi-level feature merging (MFM) strategy... is a reasonable approach to integrate features from different layers of both encoders.\"\n    - Evidence: \"The use of a non-linear MLP to align DINOv2's features with the text embedding space is a necessary step, given DINOv2's lack of inherent text alignment.\"\n    - Analysis: This is also technically accurate. The paper describes MFM in Section 4 and explains the necessity of MLP for DINOv2 alignment due to its vision-only pre-training.\n  - Claim: \"The empirical results presented in the paper are generally strong, demonstrating that COMM outperforms existing MLLMs on a variety of tasks...\"\n    - Evidence: \"VQA, image captioning, visual grounding, and object hallucination.\"\n    - Analysis: This is consistent with the results presented in Tables 1, 2, 3, 4, and 5 in the paper.\n  - Claim: \"The novelty of the proposed method is somewhat limited... the core idea of merging multiple features from different encoders is not entirely new, as similar approaches have been explored in models like Qwen-VL.\"\n    - Evidence: \"The paper acknowledges Qwen-VL but doesn't provide a direct comparison of the feature merging strategies...\"\n    - Analysis: This is a valid technical assessment. While the paper introduces COMM, the general concept of multi-modal fusion and using different feature levels is not entirely novel, and Qwen-VL is a relevant example. The paper mentions Qwen-VL in Related Work but doesn't deeply compare their feature merging strategies.\n  - Claim: \"The paper lacks a comprehensive comparison with other state-of-the-art MLLMs... some recent models, such as MiniCPM-V, are missing.\"\n    - Evidence: \"While the authors compare COMM with several models, the selection is not always consistent across different tasks, and some recent models... are missing.\"\n    - Analysis: This is a reasonable technical critique. The paper compares against several models, but the landscape of MLLMs is rapidly evolving, and including more recent or relevant baselines would strengthen the evaluation. MiniCPM-V is a relevant recent model.\n\n- From Assistant B:\n  - Claim: \"The authors' core contribution lies in their investigation of different visual encoders within MLLMs, demonstrating that combining the global semantic understanding of CLIP with the fine-grained localization information from DINOv2 leads to improved performance...\"\n    - Evidence: \"The COMM method employs a multi-level feature merging (MFM) strategy, which aligns and combines features from different layers of both CLIP and DINOv2...\"\n    - Analysis: This accurately reflects the paper's focus and contributions as stated in the Abstract and Introduction.\n  - Claim: \"The empirical results, demonstrating superior performance on a range of vision-language tasks, validate the effectiveness of the proposed COMM method.\"\n    - Evidence: \"Specifically, the performance gains on tasks like Referring Expression Comprehension (REC), Object Hallucination Benchmark (POPE), and Visual Question Answering (VQA) are significant...\"\n    - Analysis: This is consistent with the experimental results presented in the paper (Tables 1, 2, 3, 4, and 5).\n  - Claim: \"The most pressing limitation is the practical applicability of the COMM method due to the large visual feature length... Concatenating features from multiple layers of both encoders results in a feature vector that can easily exceed the context window of standard LLMs like LLaMA2...\"\n    - Evidence: \"The authors explicitly state that they concatenate features from both CLIP and DINOv2... Given the image resolution of 336x336 and the downsampling rate of 14, each encoder outputs 576 tokens per layer. Concatenating features from multiple layers of both encoders results in a feature vector that can easily exceed the context window of standard LLMs like LLaMA2...\"\n    - Analysis: This is a technically valid and important critique. Assistant B correctly points out a potential practical limitation related to the input length of LLMs.  The paper describes concatenating features from multiple layers of both ViT-Large models.  ViT-Large with a 14x14 patch size on a 336x336 image does indeed produce 576 patch tokens (24x24). Concatenating features from multiple layers of two such models could lead to a very long sequence.\n  - Claim: \"The reliance on two distinct visual encoders introduces a computational overhead that may not be feasible for resource-constrained environments or real-time applications.\"\n    - Evidence: \"The paper confirms the use of both CLIP and DINOv2 as visual encoders...\"\n    - Analysis: This is a valid technical concern. Using two large visual encoders will naturally increase computational cost compared to using a single encoder.\n  - Claim: \"The COMM method also lacks significant innovation in terms of feature merging techniques. The paper describes the MFM strategy as involving linear-layernorm for alignment and learnable scaling parameters for merging... While effective, these techniques are relatively standard and do not represent a significant architectural innovation.\"\n    - Evidence: \"The paper describes the MFM strategy as involving linear-layernorm for alignment and learnable scaling parameters for merging (lines 59-61). For DINOv2, an MLP is used for alignment (lines 61-63).\"\n    - Analysis: This is a fair technical assessment. The MFM components (linear layers, layer norm, scaling, MLP) are standard techniques in deep learning.\n\n- Comparative Assessment:\n  - Both assistants demonstrate good technical accuracy in understanding the paper's method and results.\n  - Assistant B raises a critical technical point about the potential issue of large feature vector length exceeding LLM context windows, which is a more insightful and practically relevant technical critique compared to Assistant A's points about novelty and baseline comparisons.\n  - Both assistants correctly identify the strengths and limitations of the paper's approach based on the paper's content.\n\n**Technical Accuracy - Judgment:**\n**Evidence-Based Reason:** Both assistants accurately represent the paper's technical details and identify valid technical strengths and weaknesses. However, Assistant B's critique regarding the feature vector length and LLM context window is a more critical and insightful technical point directly related to the practical applicability of the proposed method. Assistant A's points about novelty and baseline comparisons are also valid but less technically critical in terms of the method's core functionality and potential limitations.\n**Better Assistant:** B\n\n---\n**Constructive Value - Evidence Analysis:**\n- From Assistant A:\n  - Suggestion: \"conduct a more thorough comparison with state-of-the-art MLLMs, including models like MiniCPM-V.\"\n    - Analysis: This is a constructive suggestion. Including more recent and relevant baselines would strengthen the paper's empirical evaluation and contextualize its performance.\n  - Suggestion: \"provide a detailed analysis of the computational cost of COMM, including the number of parameters, FLOPs, and inference time.\"\n    - Analysis: This is a highly constructive and actionable suggestion. Computational cost is crucial for practical applicability.\n  - Suggestion: \"conduct a more detailed analysis of the feature space to demonstrate that the combined features from CLIP and DINOv2 are indeed capturing distinct aspects of the visual input.\"\n    - Analysis: This is a constructive suggestion for deeper analysis and understanding of the method.\n  - Suggestion: \"explore the possibility of fine-tuning the MLP alignment layer with text-image pairs to leverage DINOv2's emergent localization capabilities for better text alignment.\"\n    - Analysis: This is a good, specific, and potentially impactful suggestion for improvement.\n  - Suggestion: \"include a dedicated section discussing the limitations of the COMM method.\"\n    - Analysis: Standard practice and valuable for a balanced paper.\n  - Suggestion: \"consider exploring more sophisticated feature fusion techniques beyond simple concatenation and linear projection.\"\n    - Analysis: Suggests exploring more advanced techniques for potential future work.\n  - Suggestion: \"provide a more detailed explanation of the differences between COMM and other multi-branch MLLMs, such as Qwen-VL.\"\n    - Analysis: Clarifies novelty and contribution.\n  - Suggestion: \"consider conducting experiments on additional tasks, such as long-text captioning and multi-modal in-context learning...\"\n    - Analysis: Broadens evaluation scope.\n    - Overall, Assistant A provides a good range of actionable and specific suggestions for improvement, covering baselines, computational analysis, feature analysis, method refinement, limitations, and broader evaluation.\n\n- From Assistant B:\n  - Suggestion: \"explore techniques to reduce the dimensionality of the combined feature space... methods such as Principal Component Analysis (PCA) or autoencoders to compress the feature representation...\"\n    - Analysis: Highly constructive and directly addresses the practical limitation identified by Assistant B regarding feature length. PCA and autoencoders are specific and feasible techniques.\n  - Suggestion: \"investigate the possibility of using more efficient feature merging strategies that do not rely on the concatenation of all feature levels... attention-based mechanisms to selectively combine the most relevant features...\"\n    - Analysis: Constructive and suggests specific alternative approaches to feature merging.\n  - Suggestion: \"investigate knowledge distillation techniques to transfer the knowledge of the combined model into a more compact representation.\"\n    - Analysis: Another concrete and valuable suggestion for improving efficiency.\n  - Suggestion: \"provide a more detailed analysis of the feature spaces of CLIP and DINOv2... visualizing the feature distributions, analyzing the correlation between different feature levels...\"\n    - Analysis: Constructive suggestion for deeper analysis and theoretical grounding.\n  - Suggestion: \"broaden the evaluation of the COMM method to include a more diverse set of MLLM architectures and visual tasks... evaluating the method on different LLM backbones, such as LLaMA3, and on more specialized tasks...\"\n    - Analysis: Suggests broader evaluation for generalizability.\n  - Suggestion: \"provide a more detailed analysis of the computational cost of the proposed method, including the memory footprint and inference time.\"\n    - Analysis:  Actionable and important for practical assessment.\n  - Suggestion: \"provide a more detailed analysis of the interpretability of the method, including visualizing the contribution of different feature levels to the final prediction.\"\n    - Analysis: Suggests improving understanding of the method's inner workings.\n  - Suggestion: \"explore more sophisticated feature fusion techniques to enhance the innovation of the method... cross-attention mechanisms... adaptive layer selection mechanisms...\"\n    - Analysis: Suggests exploring more advanced techniques for future work and innovation.\n    - Overall, Assistant B provides highly constructive and actionable feedback, particularly focused on addressing the practical limitations related to feature dimensionality and computational cost.  The suggestions are specific and feasible, offering concrete directions for improvement.\n\n- Comparative Assessment:\n  - Both assistants provide valuable and constructive feedback.\n  - Assistant B's suggestions are slightly more focused and directly address the most critical practical limitation they identified (feature length).  Assistant B's suggestions are also very specific and actionable (PCA, autoencoders, attention, distillation).\n  - Assistant A's suggestions are also good and cover a broader range of aspects, including baselines, limitations, and broader evaluation.\n  - Both reviews are balanced and aim to help the authors improve their paper.\n\n**Constructive Value - Judgment:**\n**Evidence-Based Reason:** Both assistants offer valuable and constructive feedback. However, Assistant B's suggestions are slightly more targeted and actionable, particularly in addressing the practical limitations of the proposed method related to feature dimensionality and computational cost. Assistant B's suggestions are also more specific and offer concrete techniques (PCA, autoencoders, knowledge distillation, attention) for improvement. While Assistant A's suggestions are also helpful, Assistant B's feedback is marginally more impactful in terms of directly addressing the most significant weaknesses and providing concrete paths forward.\n**Better Assistant:** B\n\n---\n**Analytical Depth - Evidence Analysis:**\n- From Assistant A:\n  - Analyzes motivation: \"The authors correctly identify the strengths of each model: CLIP's strong image-text alignment and DINOv2's fine-grained visual representations.\"\n  - Analyzes method: \"The proposed multi-level feature merging (MFM) strategy... is a reasonable approach... The use of a non-linear MLP to align DINOv2's features...\"\n  - Analyzes results: \"The empirical results presented in the paper are generally strong, demonstrating that COMM outperforms existing MLLMs on a variety of tasks...\"\n  - Critiques novelty: \"the novelty of the proposed method is somewhat limited... the core idea of merging multiple features from different encoders is not entirely new...\"\n  - Critiques baselines: \"The paper lacks a comprehensive comparison with other state-of-the-art MLLMs... some recent models, such as MiniCPM-V, are missing.\"\n  - Critiques computational cost: \"the paper lacks a detailed analysis of the computational cost of COMM.\"\n  - Critiques redundancy: \"the paper does not adequately address the potential for redundancy when combining two powerful visual encoders.\"\n  - Critiques DINOv2 alignment claim: \"the paper's claim that DINOv2 is not inherently aligned with text is not fully supported by the evidence.\"\n  - Critiques limitations discussion: \"the paper does not explicitly discuss the limitations of the COMM method.\"\n  - Assistant A covers a good range of analytical points, including motivation, method, results, novelty, baselines, computational aspects, potential redundancy, claims about DINOv2, and limitations.\n\n- From Assistant B:\n  - Analyzes motivation: \"The authors' core idea of combining CLIP and DINOv2 to leverage their complementary strengths is both intuitive and effective.\"\n  - Analyzes results: \"The empirical results, demonstrating superior performance on a range of vision-language tasks, validate the effectiveness of the proposed COMM method.\"\n  - Critiques practical applicability (feature length): \"The most pressing limitation is the practical applicability of the COMM method due to the large visual feature length.\"\n  - Critiques computational overhead: \"the reliance on two distinct visual encoders introduces a computational overhead...\"\n  - Critiques theoretical foundation: \"The paper lacks a strong theoretical foundation to explain why the COMM method enhances the visual capabilities of MLLMs.\"\n  - Critiques generalizability: \"The COMM method's generalizability is also limited, as it is primarily evaluated within the Shikra framework.\"\n  - Critiques innovation (feature merging): \"The COMM method also lacks significant innovation in terms of feature merging techniques.\"\n  - Critiques evaluation scope: \"The paper's evaluation is limited to a few general visual tasks... It does not include more specialized domains...\"\n  - Critiques interpretability: \"the paper lacks sufficient interpretability, as it does not provide a detailed analysis of how the different feature levels contribute to the overall performance.\"\n  - Critiques justification of advantages: \"The paper also fails to convincingly demonstrate the unique advantages of the proposed method over existing approaches, particularly given the increased computational cost and complexity.\"\n  - Assistant B also covers a broad range of analytical points, including motivation, results, practical applicability (feature length, computational cost), theoretical foundation, generalizability, innovation, evaluation scope, interpretability, and justification of advantages.\n\n- Comparative Assessment:\n  - Both assistants demonstrate a good level of analytical depth, thoroughly examining various aspects of the paper.\n  - Assistant B's analysis is slightly more in-depth on the practical limitations (feature length, computational cost) and theoretical grounding, which are critical aspects for evaluating the significance and impact of a research paper.\n  - Assistant B also raises the important point about the justification of advantages relative to the increased complexity, which is a more nuanced analytical point.\n  - Both reviews are comprehensive and address important elements of the paper.\n\n**Analytical Depth - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate strong analytical depth, thoroughly examining key aspects of the paper, including motivation, methodology, results, novelty, practical limitations, and theoretical grounding. However, Assistant B's analysis delves slightly deeper into the practical implications, particularly concerning feature length and computational cost, and also raises a more nuanced point about justifying the advantages relative to the increased complexity. Assistant B's critique of the lack of theoretical foundation is also a more profound analytical point compared to some of Assistant A's critiques.\n**Better Assistant:** B\n\n---\n**Communication Clarity - Evidence Analysis:**\n- From Assistant A:\n  - Summary is clear and concise.\n  - Strengths and Weaknesses are clearly separated and listed with bullet points.\n  - Suggestions are numbered and specific.\n  - Questions are listed and relevant.\n  - Language is clear and easy to understand.\n  - Reasoning is generally well-explained for each point.\n  - Uses direct quotes and specific examples effectively.\n\n- From Assistant B:\n  - Summary is clear and concise.\n  - Strengths and Weaknesses are clearly separated and listed with bullet points.\n  - Suggestions are numbered and specific.\n  - Questions are numbered and relevant.\n  - Language is clear and easy to understand.\n  - Reasoning is very well-explained for each point, often with detailed justifications and references to line numbers in the paper.\n  - Uses direct quotes and specific examples effectively, especially in the weaknesses section, referencing line numbers for evidence.\n  - Structure is logical and easy to follow.\n\n- Comparative Assessment:\n  - Both assistants communicate their points with excellent clarity.\n  - Both use clear language, logical structure, and effectively present their reasoning.\n  - Assistant B is slightly more explicit in referencing specific parts of the paper (line numbers) to support their claims, which enhances clarity and verifiability.\n  - Both reviews are well-organized and easy to read.\n\n**Communication Clarity - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate excellent communication clarity. They both articulate their points clearly, use specific examples, and provide well-explained reasoning. Assistant B is marginally better in communication clarity due to the more frequent and explicit use of references to line numbers in the paper to support their claims, which further enhances the clarity and verifiability of their feedback. This makes it slightly easier to trace their critiques back to the source material.\n**Better Assistant:** B\n\n---\n**Comprehensive Analysis:**\nAssistant A provides a balanced review, identifying both strengths and weaknesses of the paper.  Assistant A's strengths lie in its clear and well-structured presentation, covering a good range of aspects from novelty to computational cost.  However, some of the weaknesses identified by Assistant A, while valid, are somewhat generic (e.g., lack of limitations section, need for more baselines).\n\nAssistant B provides a more critical and in-depth review, particularly focusing on the practical limitations and theoretical underpinnings of the proposed method. Assistant B's review is characterized by its strong focus on the feature vector length issue and its implications for LLM context windows, which is a highly relevant and potentially significant limitation. Assistant B also provides more specific and actionable suggestions, often suggesting concrete techniques for improvement.  Assistant B's review is also very well-evidenced, frequently referencing line numbers in the paper to support its claims.\n\nKey Differences:\n- **Focus of Critique:** Assistant A's critique is broader, covering novelty, baselines, and computational cost. Assistant B's critique is more focused on practical limitations related to feature length and theoretical depth.\n- **Specificity of Suggestions:** Assistant B's suggestions are generally more specific and actionable, often suggesting concrete techniques (PCA, autoencoders, distillation, attention).\n- **Depth of Analysis:** Assistant B's analysis is slightly deeper, particularly in its examination of practical limitations and theoretical grounding.\n- **Evidence and Referencing:** Assistant B more explicitly and frequently references specific parts of the paper (line numbers) to support its claims, enhancing verifiability and clarity.\n\nRelative Strengths:\n- Assistant B is stronger in Technical Accuracy (due to the feature length critique), Constructive Value (more actionable and specific suggestions), Analytical Depth (deeper analysis of practical limitations and theoretical grounding), and Communication Clarity (more explicit referencing).\n- Assistant A is also strong in all aspects, but slightly less impactful and specific compared to Assistant B.\n\n**Overall Judgment:**\n**Evidence-Based Reason:** Assistant B provides a more insightful, critical, and practically relevant review. Assistant B's identification of the feature vector length issue and its implications for LLM context windows is a crucial technical critique. Assistant B's suggestions are more actionable and specific, offering concrete techniques for improvement.  Assistant B's analysis is also slightly deeper and more well-evidenced. While Assistant A provides a good and balanced review, Assistant B's review is more impactful and ultimately more helpful for the authors in improving their paper.\n**Better Assistant:** B"}
{"id": "tc1jaWpi7M", "year": 2024, "paper_context": "\\title{Completing Visual Objects via Bridging Generation and Segmentation}\n\n\\begin{abstract}\n\nThis paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named \\textbf{\\textsl{MaskComp}}, delineates the completion process through iterative stages of generation and segmentation. \nIn each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images.\nWe demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, \\eg, ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.\n\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\n\\def\\tabdesignchoice#1{\n\\begin{table*}[#1]\n\t\\centering\n \t\\subfloat[\n\t\\textbf{Segmentation model}.\n\t\\label{tab:segm model}\n\t]{\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|ccc}\n                        Model & CLIPSeg & SEEM & SAM \\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 19.9 & 18.1 & 16.9\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t\\subfloat[\n\t\\textbf{IMD step number}.\n\t\\label{tab:step number}\n\t]{\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccccc}\n        $T$ & 1 & 3 & 5 & 7 \\\\ [.1em]\n        \\shline\n        FID & 24.7 & 19.4 & 16.9 & 16.1  \\\\\n    \\end{tabular}\n\\end{center}}\\end{minipage}\n}\n\\subfloat[\n\t\\textbf{\\# of sampled images}.\n\t\\label{tab:image number}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccc}\n        N & 4 & 5 & 6 \\\\ [.1em]\n        \\shline\n    FID & 17.4 & 16.9 & 16.8 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n    \\subfloat[\n\t\\textbf{Condition gating}.\n\t\\label{tab:gating}\n\t]{\n\t\t\\begin{minipage}{0.2\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        Gating & \\Checkmark & \\XSolidBrush \\\\ [.1em]\n                        \\shline\n\t\t\t\t\t\tFID & 16.9 & 18.2\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t\\caption{\\textbf{Design choices for IMD on AHP dataset.} We ablate (a) the impact of different segmentation networks, (b) IMD step number, (c) the number of sampled images in the segmentation stage, and (d) the gating operation in the CompNet.}\n\t\\label{tab:design choices}\n\\end{table*}}\n\\def\\tableablation#1{\n\\begin{table*}[#1]\n\t\\centering\n\t\\subfloat[\n\t\\textbf{Conditioned mask}.\n\t\\label{tab:conditioned mask}\n\t]{\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccccc}\n        Mask & Partial & Intermed. & Complete \\\\ [.1em]\n        \\shline\n        FID & 16.9 & 15.3 & 12.7   \\\\\n    \\end{tabular}\n\\end{center}}\\end{minipage}\n}\n\t\\subfloat[\n\t\\textbf{Occlusion rate}.\n\t\\label{tab:occ}\n\t]{\n\t\t\\begin{minipage}{0.27\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        Occ. & 20\\% & 40 \\% & 60 \\% & 80\\%\\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 13.4 & 15.7 & 17.2 & 29.9\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n \\subfloat[\n\t\\textbf{Inference time}.\n\t\\label{tab:time}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.2\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccc}\n        Comp. & Gen. & Segm. & Total \\\\ [.1em]\n        \\shline\n    Second &  14.3 & 1.2 & 15.5 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n  \\subfloat[\n\t\\textbf{Amodal baseline}.\n\t\\label{tab:amodal}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccc}\n        Model & Baseline & MaskComp \\\\ [.1em]\n        \\shline\n    FID & 29.4 & 16.9 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t\\caption{\\textbf{Ablation of MaskComp on AHP dataset}. We ablate (a) the different conditioning masks during inference, (b) the occlusion rate during inference, (c) the inference time of each component in an IMD step, and (d) the performance compared with the amodal baseline.}\n\t\\label{tab:ablation}\n\\end{table*}}\n\\def\\tablemoreablation#1{\n\\begin{table*}[#1]\n\t\\centering\n\t\\subfloat[\n\t\\textbf{Iteration for diffusion}.\n\t\\label{tab:iter diff}\n\t]{\n\t\t\\begin{minipage}{0.3\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        Iter & 20 & 40 & 50 \\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 16.9 & 15.7 & 15.1\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t \\subfloat[\n\t\\textbf{Occlusion type}.\n\t\\label{tab:occ type}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.3\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccc}\n        Occ. & Rectangle & Oval & Object \\\\ [.1em]\n        \\shline\n    FID & 15.3 & 15.1 & 16.9 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n \\subfloat[\n\t\\textbf{Availablility of complete object}.\n\t\\label{tab:complete data}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.3\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|cc}\n        Comp. & \\Checkmark & \\XSolidBrush \\\\[.1em]\n        \\shline\n    FID & 16.9 & 19.4 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\\\\\n \\subfloat[\n\t\\textbf{Voting strategies}.\n\t\\label{tab:voting}\n\t]{\n\t\t\\begin{minipage}{0.45\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        Strategy & Logits (V) & Logits (M)  & Mask (V)  & Mask (M)\\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 16.9 & 17.2 & 17.6 & 17.0\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n  \\subfloat[\n\t\\textbf{Mask loss}.\n\t\\label{tab:mask loss}\n\t]{\n\t\t\\begin{minipage}{0.3\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        $\\mathcal{L}_{mask}$ & \\Checkmark & \\XSolidBrush \\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 16.9 & 17.7 \\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t\\caption{\\textbf{More ablation of MaskComp.} We report the performance with the AHP dataset. (a) We ablate the iteration number of the diffusion model. (b) We report the performance with different types of occlusion. (c) We report the performance of MaskComp trained with or without the complete objects. (d) We ablate voting strategies. V: voting. M: Mean. (e) We ablate the effectiveness of adding intermediate supervision to predict the complete mask.}\n\t\\label{tab:more ablation}\n\\end{table*}}\n\n\\begin{figure*}[t]\n    \\centering    \n    \\includegraphics[width=\\linewidth]{figs/teaser.pdf}\n    \\caption{\\textbf{Illustration of iterative mask denoising (IMD).} Starting from an initial partial object and its corresponding mask, IMD utilizes alternating generation and segmentation stages to progressively refine the partial mask until it converges to the complete mask. With the complete mask as the condition, the final complete object can be seamlessly generated.\n    }\n    \\label{fig:teaser}\n\\end{figure*}\n\n\n\nIn recent years, creative image editing has attracted substantial attention and seen significant advancements. Recent breakthroughs in image generation techniques have delivered impressive results across various image editing tasks, including image inpainting \\citep{Xie_2023_CVPR}, composition \\citep{yang2023paint} and colorization \\citep{Chang_2023_CVPR}. However, another intriguing challenge lies in the domain of object completion (\\cref{fig:teaser0}). This task involves the restoration of partially occluded objects within an image, representing the image as a layered stack of objects and background, which can potentially enable a number of more complicated editing tasks such as object layer switching. Unlike other conditional generation tasks, \\eg, image inpainting, which only generates and integrates complete objects into images, object completion requires seamless alignment between the generated content and the given partial object, which imposes more challenges to recover realistic and comprehensive object shapes.\n \nTo guide the generative model in producing images according to a specific shape, additional conditions can be incorporated \\citep{Koley_2023_CVPR,Yang_2023_CVPR}. Image segmentation has been shown to be a critical technique for enhancing the realism and stability of generative models by providing pixel-level guidance during the synthesis process. Recent research, as exemplified in the latest study by Zhang et al. \\citep{zhang2023adding}, showcases that, by supplying object segmentations as additional high-quality masks for shaping the objects, it becomes possible to generate complex images of remarkable fidelity.\nIn this paper, we present MaskComp, a novel approach that bridges image generation and segmentation for effective object completion. MaskComp is rooted in a fundamental observation: the quality of the resulting image in the mask-conditioned generation is directly influenced by the quality of the conditioned mask \\citep{zhang2023adding}. That says the more detailed the conditioned mask, the more realistic the generated image. \nBased on this observation, unlike prior object completion methods that solely rely on partially visible objects for generating complete objects, MaskComp introduces an additional mask condition combined with an iterative mask denoising (IMD) process, progressively refining the incomplete mask to provide comprehensive shape guidance to object completion.\n\nOur approach formulates the partial mask as a noisy form of the complete mask and the IMD process is designed to iteratively denoise this noisy partial mask, eventually leading to the attainment of the complete mask. As illustrated in \\cref{fig:teaser}, each IMD step comprises two crucial stages: generation and segmentation. The generation stage's objective is to produce complete object images conditioning on the visible portion of the target object and an object mask. Meanwhile, the segmentation stage is geared towards segmenting the object mask within the generated images and aggregating these segmented masks to obtain a superior mask that serves as the condition for the subsequent IMD step. By seamlessly integrating the generation and segmentation stages, we demonstrate that each IMD step effectively operates as a mask-denoising mechanism, taking a partially observed mask as input and yielding a progressively more complete mask as output. Consequently, through this iterative mask denoising process, the originally incomplete mask evolves into a satisfactory complete object mask, enabling the generation of complete objects guided by this refined mask.\n\nThe effectiveness of MaskComp is demonstrated by its capacity to address scenarios involving heavily occluded objects and its ability to generate realistic object representations through the utilization of mask guidance. In contrast to recent progress in the field of image generation research, our contributions can be succinctly outlined as follows:\n\n\\begin{itemize}\n    \\item We explore and unveil the benefits of incorporating object masks into the object completion task. A novel approach, MaskComp, is proposed to seamlessly bridge the generation and segmentation.\n    \n    \\item We formulate the partial mask as a form of noisy complete mask and introduce an iterative mask denoising (IMD) process, consisting of alternating generation and segmentation stages, to refine the object mask and thus improve the object completion. \n    \n    \\item We conduct extensive experiments for analysis and comparison, the results of which indicate the strength and robustness of MaskComp against previous methods, \\eg, Stable Diffusion. \n\\end{itemize}\n\n\n\\section{Related Works}\n\n\\noindent\\textbf{Conditional image generation.}\nConditional image generation \\cite{lee2022autoregressive,gafni2022make,li2023gligen} involves the process of creating images based on specific conditions. These conditions can take various forms, such as layout \\citep{li2020bachgan,sun2019image,zhao2019image}, sketch \\citep{Koley_2023_CVPR}, or semantic masks \\citep{gu2019mask}. For instance, Cascaded Diffusion Models \\citep{ho2022cascaded} utilize ImageNet class labels as conditions, employing a two-stage pipeline of multiple diffusion models to generate high-resolution images. Meanwhile, in the work by \\citep{sehwag2022generating}, diffusion models are guided to produce novel images from low-density regions within the data manifold. Another noteworthy approach is CLIP \\citep{radford2021clip}, which has gained widespread adoption in guiding image generation in GANs using text prompts \\citep{galatolo2021generating, gal2022stylegan, zhou2021lafite}. In the realm of diffusion models, Semantic Diffusion Guidance \\citep{liu2023more} explores a unified framework for diffusion-based image generation with language, image, or multi-modal conditions. Dhariwal et al. \\citep{dhariwal2021diffusion} employ an ablated diffusion model that utilizes the gradients of a classifier to guide the diffusion process, balancing diversity and fidelity. Furthermore, Ho et al. \\citep{ho2022classifier} introduce classifier-free guidance in conditional diffusion models, incorporating score estimates from both a conditional diffusion model and a jointly trained unconditional diffusion model. \n\n\\noindent\\textbf{Object segmentation.}\nIn the realm of segmentation, traditional approaches have traditionally leaned on domain-specific network architectures to tackle various segmentation tasks, including semantic, instance, and panoptic segmentation \\citep{fcn, deeplabv1, mask-rcnn, spatial-instance, associate-instance, solo, panoptic-deeplab, max-deeplab, li2023towards,li2023panoramic,li2023robust,li2023rethinking,li2023paintseg,li2022video,li2022hybrid}. However, recent strides in transformer-based methodologies, have highlighted the effectiveness of treating these tasks as mask classification challenges \\citep{maskformer, knet, mask2former, detr}. MaskFormer \\citep{maskformer} and its enhanced variant \\citep{mask2former} have introduced transformer-based architectures, coupling each mask prediction with a learnable query. Unlike prior techniques that learn semantic labels at the pixel level, they directly link semantic labels with mask predictions through query-based prediction. Notably, the Segment Anything Model (SAM) \\citep{kirillov2023segment} represents a cutting-edge segmentation model that accommodates diverse visual and textual cues for zero-shot object segmentation. Similarly, SEEM \\citep{zou2023segment} is another universal segmentation model that extends its capabilities to include object referencing through audio and scribble inputs. By leveraging those foundation segmentation models, \\eg, SAM and SEEM, a number of downstream tasks can be boosted \\citep{ma2023segment,cen2023segment,yu2023inpaint}.\n\n\n\n\\section{MaskComp}\n\n\\label{sec:IMD}\n\n\\subsection{Problem Definition and Key Insight}\nWe address the object completion task, wherein the objective is to predict the image of a complete object $I_c\\in\\mathbb{R}^{3\\times H\\times W}$, based on its visible (non-occluded) part $I_p\\in\\mathbb{R}^{3\\times H\\times W}$. \n\nWe first discuss the high-level idea of the proposed \\textbf{I}terative \\textbf{M}ask \\textbf{D}enoising (IMD) and then illustrate the module details in \\cref{sec:object completion} and \\cref{sec:object segmentation}. The core of IMD is based on an essential observation: In the mask-conditioned generation, the quality of the generated object is intricately tied to the quality of the conditioned mask. As shown in \\cref{fig:mask_condition}, we visualize the completion result of the same partial object but with different conditioning masks. We notice a more complete object mask condition will result in a more complete and realistic object image. \nBased on this observation, high-quality occluded object completion can be achieved by providing a complete object mask as the condition.\n\n\\begin{figure}[t]\n\\includegraphics[width=\\linewidth]{figs/mask_condition.pdf}\n\\caption{Object completion with different mask conditions.}\n\\label{fig:mask_condition}\n\\end{figure}\n\n\\begin{figure*}[t]\n    \\centering    \n    \\includegraphics[width=\\linewidth]{figs/pipeline.pdf}\n    \\caption{\\textbf{Illustration of CompNet (generation stage of MaskComp).} The CompNet aims to recover the complete object $I_c$ from the partial object $I_p$ and a mask $M$. An object encoder is utilized to extract partial token $c_p$ which is gated and fed to the ControlNet to form the complete token $c_c$. The complete token $c_c$ serves as the condition to the diffusion U-Net to guide the conditional denoising process. In addition, a pre-diffusion mask is predicted from the partial token to encourage the object encoder to capture shape information.}\n    \\label{fig:pipeline}\n\\end{figure*}\n\\subsection{Iterative Mask Denoising}\nHowever, in real-world scenarios, the complete object mask is not available. To address this problem, we propose the IMD process which leverages intertwined generation and segmentation processes to approach the partial mask to the complete mask gradually. \nGiven a partially visible object $I_p$ and its corresponding partial mask $M_p$, the conventional object completion task aims to find a generative model $\\mathcal{G}$ such that $I_c\\leftarrow\\mathcal{G}(I_p)$, where $I_c$ is the complete object. Here, we additionally add the partial mask $M_p$ to the condition  $I_c\\leftarrow\\mathcal{G}(I_p, M_p)$, where $M_p$ can be assumed as an addition of the complete mask and a noise $M_p=M_c+\\Delta$. By introducing a segmentation model $\\mathcal{S}$, we can find a mask denoiser $\\mathcal{S}\\circ\\mathcal{G}$ from the object completion model:\n\\begin{equation}\nM_c\\leftarrow\\mathcal{S}\\circ\\mathcal{G}(I_p, M_c+\\Delta)\n\\label{equ:mask denoiser}\n\\end{equation}\nwhere $M_c=\\mathcal{S}(I_c)$.\nStarting from the visible mask $M_0=M_p$, as shown in \\cref{fig:teaser}, we repeatedly apply the mask denoiser $\\mathcal{S}\\circ\\mathcal{G}$ to gradually approach the visible mask $M_p$ to complete mask $M_c$. In each step, the input mask is denoised with a stack of generation and segmentation stages. Specifically, as the $\\mathcal{S}\\circ\\mathcal{G}(\\cdot)$ includes a generative process, we can obtain a set of estimations of denoised mask $\\{M_t^{(k)}\\}$. Here, we utilize a function $\\mathcal{V}(\\cdot)$ to find a more complete and reasonable mask from the $N$ sampled masks and leverage it as the input mask for the next iteration to further denoise. The updating rule can be written as:\n\\begin{equation}\nM_{t}^{(k)}=\\mathcal{S}\\circ\\mathcal{G}(I_p,\\hat{M}_{t-1}),\\,\\,\\,\n\\hat{M_t}=\\mathcal{V}(M_t^{(1)},\\cdots,M_t^{(N)})\n\\end{equation}\nwhere $N$ is the number of sampled images in each iteration. With a satisfactory complete mask $\\hat{M}_T$ after $T$ iterations, the object completion can be achieved accordingly by $\\mathcal{G}(I_p,\\hat{M}_T)$. The mathematical explanation of the process will be discussed in \\cref{sec:discussion}. \n\n\n\\subsection{Generation Stage}\n\\label{sec:object completion}\nWe introduce \\textbf{\\textsl{CompNet}} as the generative model $\\mathcal{G}$ which aims to recover complete objects based on partial conditions. We build CompNet based on popular ControlNet \\cite{zhang2023adding} while making fundamental modifications to enable object completion. As shown in \\cref{tab:form diff}, the target of ControlNet is to generate images strictly based on the given conditions, \\ie, $I_p\\leftarrow\\mathcal{G}(I_p,M_p)$, making it unable to complete object. Differently, CompNet is designed to recover the object. With a segmentation network, it can act as a mask denoiser to refine the conditioned mask, \\ie, $M_c\\leftarrow\\mathcal{S}\\circ\\mathcal{G}(I_p,M_p)$.\n\n\\begin{table}[t]\n    \\centering\n    \\scalebox{0.77}{\n    \\begin{tabular}{c|ccc}\n    \\hline\n    Method  & Objective & Objective with Segm. &  Object Comp.\\\\\n    \\hline\n    ControlNet & $I_p\\leftarrow\\mathcal{G}(I_p,M_p)$ & $M_p\\leftarrow\\mathcal{S}\\circ\\mathcal{G}(I_p,M_p)$ & \\textcolor{red}{\\XSolidBrush}\\\\\n    CompNet & $I_c\\leftarrow\\mathcal{G}(I_p,M_p)$ & $M_c\\leftarrow\\mathcal{S}\\circ\\mathcal{G}(I_p,M_p)$ & \\textcolor{mygreen}{\\Checkmark}\\\\\n    \\hline\n    \\end{tabular}}\n    \\caption{Objective difference with ControlNet.}\n    \\label{tab:form diff}\n\\end{table}\n\n\\noindent\\textbf{Mask condition.}\nAs illustrated on the left side of \\cref{fig:pipeline}, we begin with a complete object $I_c$ and its corresponding mask $M_c$. Our approach commences by occluding the complete object, retaining only the partially visible portion as $I_p$. Recall that the mask-denoising procedure initiates with the partial mask $M_p$ and culminates with the complete mask $M_c$. To facilitate this iterative denoising, the model must effectively handle any mask that falls within the interpolation between the initial partial mask and the target complete mask. Consequently, we introduce a mask $M$ with an occlusion rate positioned between the partial and complete masks as a conditioning factor for the generative model. The partial mask $M_p$ is achieved by considering the occluded areas in both occlusion processes. The interpolated mask $M$ is generated by using one of the occlusions. \n\n\\noindent\\textbf{Diffusion model.}\nDiffusion models have achieved notable progress in synthesizing unprecedented image quality and have been successfully applied to many text-based image generation works \\citep{rombach2022high,zhang2023adding}. For our object completion task, the complete object can be generated by leveraging the diffusion process.\n\nSpecifically, the diffusion model generates image latent $x$ by gradually reversing a Markov forward process. As shown in Figure~\\ref{fig:pipeline}, starting from $x_0=\\mathcal{E}(I_c)$, the forward process yields a sequence of increasing noisy tokens $\\{x_\\tau|\\tau\\in[1,T_{\\mathcal{G}}]\\}$, where $x_\\tau=\\sqrt{\\Bar{\\alpha_\\tau}}y_0+\\sqrt{1-\\Bar{\\alpha_\\tau}}\\epsilon$, $\\epsilon$ is the Gaussian noise, and $\\alpha_\\tau$ decreases with the timestep $\\tau$. For the denoising process, the diffusion model progressively denoises a noisy token from the last step given the conditions $c=(I_p,M,E)$ by minimizing the following loss function: $\\mathcal{L}=\\mathbb{E}_{\\tau,x_0,\\epsilon}\\|\\epsilon_\\theta(x_\\tau,c,\\tau)-\\epsilon\\|_2^2$. $I_p$, $M$, and $E$ are the partial object, conditioned mask, and text prompt respectively.\n\\begin{figure*}[t]\n    \\centering    \n    \\includegraphics[width=\\linewidth]{figs/mask.pdf}\n    \\caption{\n    We calculate the mask probability map by averaging and normalizing the masks of sampled images. We show a cross-section of the lower leg to better visualize (shown as \\textcolor{darkyellow}{yellow}). \n    }\n    \\label{fig:mask}\n\\end{figure*}\n\n\\noindent\\textbf{CompNet architecture.}\nPrevious work \\citep{zhang2023adding} has demonstrated an effective way to add additional control to generative diffusion models. We follow this architecture and make necessary modifications to adapt the architecture to object completion. As shown in \\cref{fig:pipeline}, given the visible object $I_p$ and the conditioning mask $M$, we first concatenate them and extract the partial token $c_p$ with an object encoder. \nDifferent from ControlNet \\citep{zhang2023adding} assuming the condition is accurate, the object completion task relies on incomplete conditions. Specifically, in the early diffusion steps, the condition information is vital to complete the object. Nevertheless, in the later steps, inaccurate information in the condition can degrade the generated object. To tackle this problem, we introduce a time-variant gating operation to adjust the importance of conditions in the diffusion steps. We learn a linear transform $f: \\mathbb{R}^{C}\\rightarrow\\mathbb{R}^{1}$ upon the time embedding $e_t\\in\\mathbb{R}^{C}$ and then apply it to the partial token as $f(e_t)\\cdot c_p$ before feeding it to the ControlNet. In this way, the importance of visible features can be adjusted as the diffusion steps forward. The time embedding used for the gating operation is shared with the time embedding for encoding the diffusion step in the stable diffusion.\n\nTo encourage the object encoder to capture shape information, we introduce an auxiliary path to predict the complete object mask from the partial token $c_p$. Specifically, a feature pyramid network \\cite{lin2017feature} is leveraged as the mask decoder which takes $c_p$ and the multi-scale features from the object encoder as input and outputs a pre-diffusion mask $M_{pre}$. We encourage mask completion with supervision as\n\\begin{equation}\n    \\mathcal{L}_{mask}=\\mathcal{L}_{dice}(M_c,M_{pre})+\\lambda_{ce}\\mathcal{L}_{ce}(M_c,M_{pre})\n\\end{equation}\nwhere $\\mathcal{L}_{dice}$ and $\\mathcal{L}{ce}$ are Dice loss \\cite{li2019dice} and BCE loss respectively. $\\lambda_{ce}$ is a constant.\n\n\\subsection{Segmentation Stage}\n\\label{sec:object segmentation}\nIn the segmentation stage, illustrated in \\cref{fig:mask} (a), our approach initiates by sampling $N$ images denoted as $\\{I_t^{(k)}\\}^N_{k=1}$ from the generative model, where $t$ is the IMD step. Subsequently, we employ an off-the-shelf object segmentation model denoted as $\\mathcal{S}(\\cdot)$ to obtain the shapes (object masks) $\\{M_t^{(k)}\\}$ from these sampled images.\n\nTo derive an improved mask for the subsequent IMD step, we seek a function $\\mathcal{V}(\\cdot)$ that can produce a high-quality mask prediction from the set of $N$ generated masks. Interestingly, though the distribution of sampled images is complex, we notice the distribution of masks has good properties.\nIn \\cref{fig:mask} (b), we provide a visualization of the probability map associated with a set of object masks with the same conditions, which is computed by taking the normalized average of the masks.\nTo enhance the visualization of this probability distribution, we focus on a specific cross-section of the fully occluded portion in image $I_p$ (the lower leg, represented as a \\textcolor{darkyellow}{yellow} section) and visualize the probability as a function of the horizontal coordinate which demonstrates an obvious unimodal and symmetric property. Leveraging this observation, we can find an improved mask by taking the high-probability region. \nThe updating can be achieved by conducting a voting process across the $N$ estimated masks, as defined by the following equation:\n\\begin{equation}\n    \\hat{M}_t[i,j] = \\begin{cases}\n                1, & \\text{if}\\quad \\frac{\\sum_{k=1}^NM_t^{(k)}[i,j]}{N} \\geq \\tau \\\\\n                0, & \\text{otherwise}\n        \\end{cases}\n\\end{equation}\nwhere $[i,j]$ denotes the coordinate, and $\\tau$ is the threshold employed for the mask voting process.\n\n\n\n\n\n\\begin{figure}[t]\n\\centering   \n\\includegraphics[width=\\linewidth]{figs/dist.pdf}\n\\caption{\\textbf{Mutual-benificial sampling.}}\n\\label{fig:mutual}\n\\end{figure}\n\n\\begin{table*}[t]\n\\centering\n\\scalebox{0.9}{\n\\begin{tabular}{l|p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}|p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}} \n\\hline\n\\multirow{2}*{Method} & \\multicolumn{4}{c|}{AHP \\citep{zhou2021human}} &\n\\multicolumn{4}{c}{DYCE \\citep{ehsani2018segan}}\\\\\n\\cline{2-9}\n~ & FID-G $\\downarrow$ & FID-S $\\downarrow$ & Rank $\\downarrow$ & Best $\\uparrow$ & FID-G $\\downarrow$ & FID-S $\\downarrow$ & Rank $\\downarrow$ & Best $\\uparrow$\\\\\n\\hline\nControlNet & 40.2 & 45.4 & 3.4 & 0.10 & 42.4 & 49.4 & 3.4 & 0.08 \\\\\nKandinsky 2.1 & 43.9 & 39.2 & 3.2 & 0.11 & 44.3 & 47.7 &  3.4 & 0.06 \\\\\nStable Diffusion 1.5 & 35.7 & 41.4 & 3.2 & 0.12 & 31.2 & 43.4 & 3.4 & 0.11 \\\\\nStable Diffusion 2.1 & 30.8 & 39.9 & 3.1 & 0.14 & 30.0 & 41.1 & 3.0 & 0.12 \\\\\n\\bf MaskComp (Ours) & \\bf16.9 & \\bf21.3 &\\bf2.1 & \\bf0.53 & \\bf 20.0 & \\bf25.4 & \\bf1.9 & \\bf0.63  \\\\\n\\hline\n\\end{tabular}}\n\\caption{\\textbf{Quantitative evaluation on object completion task}. The computing of FID-G and FID-S only considers the object areas within ground truth and foreground regions segmented by SAM, respectively, to eliminate the influence of the generated background. The Rank denotes the average ranking in the user study. The Best denotes the percentage of samples that are ranked as the best. $\\downarrow$ and $\\uparrow$ denote the smaller the better and the larger the better respectively.}\n\\label{tab:main results}\n\\end{table*}\n\n\\subsection{Discussion}\n\\label{sec:discussion}\nIn this section, we will omit the conditioned partial image $I_p$ for simplicity.\n\n\\noindent\\textbf{Joint modeling of mask and object.}\nIn practical scenarios where the complete object mask $M_c$ is unavailable, modeling object completion through a marginal probability $p(I_c|M_c)$ becomes infeasible. Instead, it necessitates the more challenging joint modeling of objects and masks, denoted as $p(I,M)$, where the images and masks can range from partial to complete. Let us understand the joint distribution by exploring its marginals. Since the relation between mask and image is one-to-many (each object image only has one mask while the same mask can be segmented from multiple images), the $p(M|I)$ is actually a Dirac delta distribution $\\delta$ and only the $p(I|M)$ is a real distribution. This way, the joint distribution of mask and image is discrete and complex, making the modeling difficult. To address this issue, we introduce a slack condition to the joint distribution $p(I,M)$ that \\textit{the mask and image can follow a many-to-many relation}, which makes its marginal $p(M|I)$ a real distribution and permits $p(I|M)$ to predict image $I$ that has a different shape as the conditioned $M$ and vice versa. \n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/compare.pdf}\n    \\caption{\\textbf{Qualitative comparison against ControlNet, Kandinsky and Stable Diffusion}. The background is filtered out for better visualization.}\n    \\label{fig:compare}\n\\end{figure*}\n\n\\noindent\\textbf{Mutual-beneficial sampling.}\nAfter discussing the joint distribution that we are targeting, we introduce the mathematical explanation of MaskComp. MaskComp introduces the alternating modeling of two marginal distributions $p(I|M)$ (generation stage) and $p(M|I)$ (segmentation stage), which is actually a Markov Chain Monte Carlo-like (MCMC-like) process and more specifically Gibbs sampling-like. It samples the joint distribution $p(I,M)$ by iterative sampling from the marginal distributions. Two core insights are incorporated in MaskComp: (1) providing a mask as a condition can effectively enhance object generation and (2) fusing the mask of generated object images can result in a more accurate and complete object mask. Based on these insights, we train CompNet to maximize $p(I|M)$ and leverage mask voting to maximize the $p(M|I)$. As shown in \\cref{fig:mutual}, MaskComp develops a mutual-beneficial sampling process from the joint distribution $p(I,M)$, where the object mask is provided to boost the image generation and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. Through alternating sampling from the marginal distributions, we can effectively address the object completion task.\n\n\n\n\\section{Experiment}\n\n\\subsection{Experimental Settings}\n\\noindent\\textbf{Dataset.}\nWe evaluate MaskComp on two popular datasets: AHP \\citep{zhou2021human} and DYCE \\citep{ehsani2018segan}. \nAHP is an amodal human perception dataset with 56,302 images with annotations of integrated humans.\nDYCE is a synthetic dataset with photo-realistic images and the natural configuration of objects in indoor scenes.\nFor both datasets, the non-occluded object and its corresponding mask for each object are available. We train MaskComp on the AHP and a filtered subset of OpenImage v6 \\citep{kuznetsova2020open}. OpenImage is a large-scale dataset offering heterogeneous annotations. We select a subset of OpenImage that contains 429,358 objects as a training set of MaskComp.\n\\tableablation{t}\n\n\\noindent\\textbf{Evaluation metrics.}\nIn accordance with previous methods \\citep{zhou2021human}, we evaluate image generation quality Fréchet Inception Distance (FID). The background is removed with object masks before evaluation. \nAs the FID score cannot reflect the object completeness, we further conduct a user study, leveraging human assessment to compare the quality and completeness of images. During the assessment, given a partial object, the participants are required to rank the generated object from different methods based on their completeness and quality.\n\n\\noindent\\textbf{Implementation details.}\nFor the generation stage, we train the CompNet with frozen Stable Diffusion \\citep{rombach2022high} on the AHP dataset for 50 epochs. The learning rate is set for 1e-5. We adopt $\\mathrm{batchsize}=8$ and an Adam \\citep{loshchilov2017adamw} optimizer. \nThe image is resized to $512\\times 512$ for both training and inference. \nThe object is cropped and resized to have the longest side 360 before sticking on the image. \nFor a more generalized setting, we train the CompNet on a subset of the OpenImage \\citep{kuznetsova2020open} dataset for 36 epochs. We generate text prompts using BLIP \\citep{li2022blip} for all experiments (prompts are necessary to train ControlNet). For the segmentation stage, we leverage SAM \\citep{kirillov2023segment} as $\\mathcal{S}(\\cdot)$. We vote mask with a threshold of $\\tau=0.5$. During inference, if no other specification, we conduct the IMD process for 5 steps with $N=5$ images for each step. We give the class label as the text prompt to facilitate the CompNet to effectively generate objects. All baseline methods are given the same text prompts during the experiments. The code will be made publicly available.\n\n\\subsection{Main Results}\n\n\\noindent\\textbf{Quantitative results.}\nWe compare the MaskComp with state-of-the-art methods, ControlNet \\citep{zhang2023adding}, Kandinsky 2.1 \\citep{kandinsky2}, Stable Diffusion 1.5 \\citep{rombach2022high} and Stable Diffusion 2.1 \\citep{rombach2022high} on AHP \\citep{zhou2021human} and DYCE \\citep{ehsani2018segan} dataset. The results in \\cref{tab:main results} indicate that MaskComp consistently outperforms other methods, as evidenced by its notably lower FID scores, signifying the superior quality of its generated content. We conducted a user study to evaluate object completeness in which participants ranked images generated by different approaches. MaskComp achieved an impressive average ranking of 2.1 and 1.9 on the AHP and DYCE datasets respectively. Furthermore, MaskComp also generates the highest number of images ranked as the most complete and realistic compared to previous methods. We consider the introduced mask condition and the proposed IMD process benefits the performance of MaskComp, where the additional conditioned mask provides robust shape guidance to the generation process and the proposed iterative mask denoising process refines the initial conditioned mask to a more complete shape, further enhancing the generated image quality.   \n\n\\noindent\\textbf{Qualitative results.}\nWe present visual comparisons among ControlNet, Kandinsky 2.1, Stable Diffusion 1.5, and Stable Diffusion 2.1, illustrated in \\cref{fig:compare}. Our visualizations showcase MaskComp's ability to produce realistic and complete object images given partial images as the condition, whereas previous approaches exhibit noticeable artifacts and struggle to achieve realistic object completion. In addition, without mask guidance, it is common for previous methods to generate images that fail to align with the partial object.\n\n\\begin{figure*}[t]\n    \\centering    \n    \\includegraphics[width=\\linewidth]{figs/vis_imd.pdf}\n    \\caption{\\textbf{Visualization of the IMD process.} For each step, we randomly demonstrate one generated image and the averaged mask for all generated images. We omit the input mask which has the same shape as the input occluded object.\n    }\n    \\label{fig:vis_imd}\n\\end{figure*}\n\n\\tabdesignchoice{t}\n\n\\subsection{Analysis}\n\\label{sec:analysis}\nIn this section, we provide an experimental analysis of MaskComp. All the results are evaluated with GT masks to filter out the background, \\ie, FID-G.\n\n\\noindent\\textbf{Performance with different mask conditions.}\nWe evaluated the quality of generated images when conditioned on the same partial images along with three distinct types of masks: (1) partial mask (mask of the partial image), (2) intermediate mask (less occlusion than partial), and (3) complete mask. As shown in \\cref{tab:conditioned mask}, the model achieves its highest performance when it is conditioned with complete object masks, whereas relying solely on partial masks yields less optimal results. These results provide strong evidence that the quality of the conditioned mask significantly influences the quality of the generated images.\n\n\\noindent\\textbf{Performance with different occlusion rates.}\nWe perform ablation studies to assess the resilience of MaskComp under varying occlusion levels. As presented in \\cref{tab:occ}, we evaluate MaskComp at different occlusion levels (proportion of the obscured area relative to the complete object) ranging from 20\\% to 80\\%, and the results indicate that its performance does not degrade significantly up to 60\\% occlusion.\n\n\n\\noindent\\textbf{Inference time.} \\cref{tab:time} reports the inference time of each component in IMD (with a single NVIDIA V100 GPU). Although MaskComp's throughput is reduced due to the inclusion of multiple diffusion processes in each IMD step, it is capable of attaining a higher degree of accuracy in visual object completion. Based on our empirical experiments, reducing the number of diffusion steps during the first few IMD steps can increase model speed without sacrificing much performance. With this idea incorporated into MaskComp, the average running time could be reduced to 2/3 of the original time with FID slightly increasing by 0.50. While beyond the scope of this study, we expect more advanced techniques could be explored to optimize the tradeoff between model speed and performance.\n\n\\noindent\\textbf{Comparison to amodal segmentation baseline.}\nAmodal segmentation has a similar objective to the proposed IMD process. To demonstrate the effectiveness of MaskComp, we construct an amodal baseline that generates amodal masks from the SOTA amodal segmentation method \\cite{tran2022aisformer} and then utilize ControlNet to generate images based on the amodal masks. s shown in \\cref{tab:amodal}, we notice that our method outperforms the amodal baseline by a considerable margin, which could be attributed to the strong mask completion capability of the proposed IMD process.\n\n\\begin{table}[t]\n    \\centering\n    \\scalebox{0.95}{\n    \\begin{tabular}{c|ccccc}\n        Noise degree & Iter. 1 & Iter. 3 & Iter. 5 & Iter. 7 & Iter. 9 \\\\\n        \\toprule\n        15\\% area & 28.4 & 22.7 & 18.9 & 17.2 & 16.5 \\\\\n        10\\% area & 26.4 & 21.4 & 18.1 & 17.0 & 16.4 \\\\\n        5\\% area & 24.9 & 19.6 & 17.0 & 16.2 & 16.0 \\\\\n        No noise & 24.7 & 19.4 & 16.9 & 16.1 & 15.9 \n    \\end{tabular}}\n    \\caption{Performance against segmentation errors on AHP dataset.}\n    \\label{tab:robust}\n\\end{table}\n\n\\noindent\\textbf{Impact of different segmentation networks.} \nWe adopt SAM to obtain object masks at the segmentation stage. To study the impacts of different segmenters, we replace SAM with two smaller segmentation networks, CLIPSeg \\cite{luddecke2022image} and SEEM \\cite{zou2023segment}. \\cref{tab:segm model} shows that the FID score with CLIPSeg (19.9) is slightly higher than with SAM (16.9), but remains competitive against other state-of-the-art methods, e.g., Stable Diffusion 2.1 (30.8 reported in \\cref{tab:main results}). MaskComp is an iterative mask denoising (IMD) process that progressively refines a partial object mask to boost image generation. The results support our hypothesis that the impact of the segmenter is modest.\n\n\\noindent\\textbf{Design choices in IMD.}\nWe conduct experiments to ablate the design choices in IMD and their impacts on the completion performance. We first study the effect of IMD step number. With a larger step number, IMD can better advance the partial mask to the complete mask. As shown in \\cref{tab:step number}, we notice that the image quality keeps increasing and slows down at a step number of 5. In this way, we choose 5 as our IMD step number. After that, we ablate the number of sampled images in the segmentation stage in \\cref{tab:image number}. We notice more sampled images generally leading to a better performance. We leverage an image number of 5 with the efficiency consideration. As we leverage the diffusion-based method for image generation, we ablate the iterations for the diffusion process. As shown in \\cref{tab:gating}, we notice the gating operation improves the generation quality by 1.3 FID, indicating the necessity of conditional gating.\n\\noindent\\textbf{Robustness to segmentation errors.} \nWe conduct experiments to manually add random errors to masks. As shown in \\cref{tab:robust}, we ablate on the number of iterations and the degree of segmentation error. We observe that segmentation errors will increase the convergence iteration number while not affecting the final performance significantly. As IMD is a reciprocal process intended to provide effective control for later-generated masks to be refined based on adaptive feedback, mask errors are mitigated and not propagated.\n\\noindent\\textbf{Visualization of iterative mask denoising.}\nTo provide a clearer depiction of the IMD process, as depicted in \\cref{fig:vis_imd}, we present visualizations of the generated image and the averaged mask for each step. In the initial step, we observe the emergence of artifacts alongside the object. As we progress through the steps, both the image and mask quality exhibit continuous improvement.\n\n\n\n\n\\section{Conclusion}\n\nIn this paper, we introduce MaskComp, a novel approach for object completion. MaskComp addresses the object completion task by seamlessly integrating conditional generation and segmentation, capitalizing on the crucial observation that the quality of generated objects is intricately tied to the quality of the conditioned masks. We augment the object completion process with an additional mask condition and propose an iterative mask denoising (IMD) process. This iterative approach gradually refines the partial object mask, ultimately leading to the generation of satisfactory objects by leveraging the complete mask as a guiding condition. Our extensive experiments demonstrate the robustness and effectiveness of MaskComp, particularly in challenging scenarios involving heavily occluded objects.\n\n\n\n\n\n\n\n\n", "DeepReviewer": "\n## Summary:\n\nThis paper introduces MaskComp, a novel approach to object completion that iteratively refines object masks through alternating stages of image generation and segmentation. The core idea is to leverage the synergy between these two processes, where the generation stage, conditioned on a partial object and its mask, produces a completed object, and the segmentation stage, applied to the generated image, provides a more accurate mask. This iterative process, termed Iterative Mask Denoising (IMD), progressively improves the mask quality, leading to more precise object completion. The method builds upon ControlNet, a conditional image generation model, and utilizes a pre-trained Stable Diffusion model for the generative component. For the segmentation stage, MaskComp employs the Segment Anything Model (SAM). The authors demonstrate the effectiveness of MaskComp on two datasets, AHP and DYCE, showing superior performance compared to existing methods like ControlNet and Stable Diffusion. The method's key contribution lies in its iterative refinement strategy, which effectively combines generation and segmentation to achieve high-quality object completion. The paper presents a clear methodology, supported by experimental results, and highlights the potential of iterative approaches for addressing complex image editing tasks. However, the paper also acknowledges certain limitations, such as the computational cost of the iterative process and the reliance on paired data for training. Overall, the paper presents a valuable contribution to the field of object completion, offering a novel and effective approach that combines generation and segmentation in a synergistic manner. The results are compelling, but the paper also identifies areas for future research, such as improving the efficiency of the method and reducing its dependence on paired training data. The authors also suggest that the method could be used for other editing tasks such as object layer switching, which could be a promising direction for future work.\n\n\n## Soundness:\n\n3.0\n\n\n## Presentation:\n\n3.0\n\n\n## Contribution:\n\n2.5\n\n\n## Strengths:\n\nI find the core strength of this paper to be the innovative approach of combining image generation and segmentation in an iterative manner for object completion. The proposed Iterative Mask Denoising (IMD) process is a novel and effective way to progressively refine object masks, leading to superior completion results. The idea of using the generated image to improve the mask, and then using the improved mask to generate a better image, is a clever way to leverage the strengths of both generative and segmentation models. The paper clearly articulates this methodology, making it easy to understand the core contribution. The experimental results, which demonstrate the superiority of MaskComp over existing methods like ControlNet and Stable Diffusion, provide strong empirical support for the proposed approach. The authors have conducted sufficient experiments on two datasets, AHP and DYCE, to validate their claims. The paper is also well-written and easy to follow, which enhances its accessibility and impact. The authors have clearly explained the motivation behind their work, the details of their method, and the results of their experiments. The use of ControlNet as a base model, while also a weakness, is also a strength in that it provides a strong foundation for the proposed method. The authors have made fundamental modifications to ControlNet to enable object completion, which demonstrates their understanding of the underlying techniques. The use of Stable Diffusion as the generative model also contributes to the strength of the method, as it is a powerful and widely used model. The authors have also clearly stated the limitations of their method, which is a sign of good scientific practice. Overall, I believe the paper presents a significant contribution to the field of object completion, offering a novel and effective approach that combines generation and segmentation in a synergistic manner. The results are compelling, and the paper is well-written and easy to follow. The authors have clearly articulated their contribution and have provided strong empirical support for their claims.\n\n\n## Weaknesses:\n\nAfter a thorough examination of the paper, I have identified several key weaknesses that warrant careful consideration. First, the proposed method's efficiency is a significant concern. The iterative nature of the IMD process, which involves alternating between generation and segmentation stages, introduces a substantial computational overhead. As the paper states, \"The IMD process is repeated for a fixed number of steps (T), with each step involving the generation and segmentation stages.\" (Method -> Iterative Mask Denoising (IMD) Process). The experimental section further reveals that, during inference, the IMD process is conducted for 5 steps, with 5 images generated for each step. The inference time for each component per IMD step is 14.3 seconds for generation and 1.2 seconds for segmentation, resulting in a total of 15.5 seconds per step. This means that the total inference time for the entire process is 77.5 seconds per image. This computational cost is substantial and could limit the practical application of the method in real-time scenarios or when processing large datasets. This is a valid concern, and I have high confidence in this assessment, based on the explicit description of the iterative process and the provided inference times. Second, the method's reliance on paired data for training is a significant limitation. While the paper does not explicitly state that paired data is required, the problem formulation and the training process strongly suggest that paired or near-paired data is used. The paper states, \"We address the object completion task, wherein the objective is to predict the image of a complete object $I_c$ based on its visible (non-occluded) part $I_p$.\" (Method -> Problem Definition and Key Insight). This implies a mapping from partial to complete objects, which typically requires paired data. Furthermore, the training is conducted on the AHP dataset and a filtered subset of OpenImage, both of which likely contain paired or near-paired information. This reliance on paired data restricts the applicability of the method in real-world scenarios where such data is often unavailable. This is a partially valid concern, and I have medium confidence in this assessment, based on the problem formulation and the training datasets used. Third, the method's dependence on a pre-trained object completion method, specifically ControlNet, raises concerns about the fairness of comparisons with methods that do not use such priors. The paper states, \"We introduce \\textbf{\\textsl{CompNet}} as the generative model $\\mathcal{G}$ which aims to recover complete objects based on partial conditions. We build CompNet based on popular ControlNet \\cite{zhang2023adding} while making fundamental modifications to enable object completion.\" (Method -> Generation Stage). The performance of MaskComp is therefore influenced by the quality of the base model, Stable Diffusion, which is used in a frozen state. The comparison with methods that are not based on ControlNet, such as Kandinsky 2.1 and Stable Diffusion, may not be entirely fair, as MaskComp benefits from the pre-training of ControlNet. This is a valid concern, and I have high confidence in this assessment, based on the explicit statement about building CompNet on ControlNet. Fourth, the method's requirement for ground-truth text prompts is a significant limitation for real-world applications. The paper mentions using BLIP to generate text prompts for training ControlNet, stating, \"For a more generalized setting, we train the CompNet on a subset of the OpenImage \\citep{kuznetsova2020open} dataset for 36 epochs. We generate text prompts using BLIP \\citep{li2022blip} for all experiments (prompts are necessary to train ControlNet).\" (Experiments -> Implementation details). The fact that all baseline methods are given the same text prompts during experiments further supports the claim that the method relies on these prompts. While the method description itself doesn't explicitly mention text prompts as input to the generation stage, the reliance on ControlNet and Stable Diffusion, which are text-conditional, makes this a valid concern. Obtaining such prompts is not always feasible in real-world scenarios, limiting the method's applicability. This is a valid concern, and I have high confidence in this assessment, based on the mention of using BLIP for prompt generation and the statement about prompts being necessary for training ControlNet. Finally, the experimental evaluation is limited by the choice of baselines. The paper compares MaskComp with ControlNet, Kandinsky 2.1, and Stable Diffusion, which are not specifically designed for object completion. The lack of comparisons with recent object completion methods is a significant weakness, as it does not fully demonstrate the method's superiority over state-of-the-art techniques in this specific domain. This is a valid concern, and I have high confidence in this assessment, based on the list of baselines in the experimental section and the absence of comparisons with dedicated object completion methods. Additionally, the evaluation is primarily focused on human-related datasets and relatively simple objects, which does not fully demonstrate the method's robustness and scalability. Testing on more intricate objects with varying shapes, textures, and occlusion patterns is necessary to validate its broader applicability. This is a partially valid concern, and I have medium confidence in this assessment, based on the description of the datasets used and the visual examples provided in the paper.\n\n\n## Suggestions:\n\nBased on the identified weaknesses, I propose several concrete and actionable improvements. First, to address the efficiency issue, future work should explore techniques to reduce the number of iterations required for convergence. This could involve incorporating more sophisticated mask refinement strategies or using more efficient generative and segmentation models. For example, instead of a fixed number of iterations, the process could terminate when the change in mask quality between iterations falls below a certain threshold. Additionally, investigating the possibility of parallelizing the generation and segmentation steps could also lead to significant speedups. The current approach processes each stage sequentially, which may not be optimal. Exploring methods to perform these steps concurrently or with minimal dependencies could lead to significant speedups. Second, to mitigate the reliance on paired data, future research should explore methods to reduce or eliminate this requirement. This could involve exploring self-supervised learning methods or leveraging existing large-scale datasets with unpaired data. Techniques like cycle-consistency or adversarial training could be explored to learn the mapping between partial and complete objects without requiring explicit paired examples. Furthermore, investigating the use of synthetic data or data augmentation techniques could also help to mitigate the need for large amounts of real-world paired data. The authors should also consider exploring the use of pre-trained models or transfer learning to reduce the need for large datasets. Third, to address the dependence on ControlNet, future work should explore methods to reduce the reliance on this specific pre-trained model. This could involve exploring alternative base models or developing a method that is less dependent on a specific pre-trained architecture. A more rigorous evaluation would involve comparing MaskComp against methods that start from a similar baseline, or alternatively, providing a detailed ablation study that isolates the contribution of the IMD pipeline from the base model. Furthermore, the paper should explore the sensitivity of MaskComp to the choice of the base model. For example, how would MaskComp perform with different base models, and what are the trade-offs in terms of computational cost and performance? This would provide a more comprehensive understanding of the method's robustness and generalizability. Fourth, to address the requirement for ground-truth text prompts, the authors should acknowledge that obtaining such prompts is not always feasible and explore alternative approaches to address this issue. For instance, the authors could investigate the use of automatically generated text prompts or explore methods that do not rely on text prompts at all. The paper should also discuss the impact of inaccurate or misleading text prompts on the performance of MaskComp. How does the method behave when the provided text prompt does not accurately describe the object in the image? This analysis would provide a more realistic assessment of the method's applicability in practical scenarios. Additionally, the authors could consider incorporating techniques for prompt refinement or correction to mitigate the impact of noisy prompts. Finally, to improve the evaluation, the authors should include comparisons with more recent and specialized object completion methods. This would provide a more comprehensive evaluation of MaskComp's performance and demonstrate its advantages over state-of-the-art object completion techniques. The authors should also consider using a wider range of evaluation metrics that are specifically designed for object completion, such as metrics that measure the accuracy of the completed object's shape and appearance. This would provide a more detailed and nuanced assessment of the method's performance. The evaluation should also be expanded to include more complex objects and scenes. This could involve using datasets with a wider variety of object categories, including those with intricate shapes, textures, and occlusion patterns. The authors should also consider evaluating the method's performance on scenes with multiple objects and varying levels of occlusion. This would provide a more comprehensive assessment of the method's robustness and scalability. Additionally, the authors should analyze the failure cases of the method and discuss the limitations of the current approach. This would help in identifying areas for future improvement and provide a more balanced view of the method's capabilities.\n\n\n## Questions:\n\nBased on my analysis, I have several questions that I believe are crucial for a deeper understanding of the method. First, what is the efficiency of the proposed method in terms of processing time per image, and how does this compare to existing methods? While the paper provides the inference time per IMD step, a more direct comparison with other methods would be beneficial. Second, how does the proposed method perform on other datasets, such as the dataset used in ControlNet, or other datasets specifically designed for object completion? This would provide a more comprehensive evaluation of the method's generalizability. Third, how does the proposed method handle cases where the partial object is heavily occluded or has significant variations in appearance compared to the complete object? Are there any limitations in terms of the types of objects or scenes that the method can handle effectively? Fourth, what are the specific conditions under which the iterative refinement process fails to converge to a good solution? Are there specific types of objects or occlusions that are particularly challenging for the method? A more thorough analysis of these failure cases would provide valuable insights into the strengths and weaknesses of the proposed approach. Fifth, the paper mentions that the proposed method can be used for other editing tasks such as object layer switching. Can the authors provide more details on how this can be achieved and show some examples of such applications? This would demonstrate the versatility of the method and its potential for broader impact. Finally, how does the computational cost of the proposed method compare to existing methods? Are there any potential optimizations that could be made to improve its efficiency? This is a critical question, given the iterative nature of the method. These questions target key uncertainties and methodological choices, seeking clarification of critical assumptions and limitations.\n\n\n## Rating:\n\n5.5\n\n\n## Confidence:\n\n3.5\n\n\n## Decision:\n\nReject", "other": "\n## Summary:\n\nThis paper introduces MaskComp, a novel approach to object completion that iteratively refines both the object's mask and its visual representation. The core idea revolves around the observation that the quality of mask-conditioned generation is directly tied to the quality of the mask itself. To leverage this, MaskComp employs an Iterative Mask Denoising (IMD) process, which alternates between generating an image based on the current mask and then refining that mask using an off-the-shelf segmentation model. This iterative process allows the method to progressively improve both the mask and the generated object, even in cases of significant occlusion. The method is built upon ControlNet, a conditional image generation framework, and incorporates a time-variant gating operation to modulate the influence of the mask during the diffusion process. The authors demonstrate the effectiveness of MaskComp through extensive experiments on two datasets, AHP and DYCE, showing that it outperforms existing methods in terms of both quantitative metrics and qualitative visual results. The paper also includes a thorough ablation study, exploring the impact of various design choices, such as the number of iterations, the choice of segmentation model, and the use of different conditioning masks. The results indicate that the proposed iterative approach is robust to segmentation errors and can handle various occlusion scenarios. The authors also show that the method can be used for amodal segmentation by comparing it to a baseline that combines a state-of-the-art amodal segmentation method with ControlNet. Overall, the paper presents a well-motivated and effective approach to object completion, with a clear focus on the interplay between mask quality and generative performance. The iterative nature of the method, coupled with the use of off-the-shelf segmentation models, makes it a practical and versatile solution for a range of image editing tasks. The paper is well-written and easy to follow, making it a valuable contribution to the field of image generation and editing.\n\n\n## Soundness:\n\n3.25\n\n\n## Presentation:\n\n3.25\n\n\n## Contribution:\n\n3.0\n\n\n## Strengths:\n\nI find several aspects of this paper to be particularly strong. First, the core idea of iteratively refining both the mask and the generated object is both intuitive and effective. The authors clearly articulate the motivation behind this approach, highlighting the crucial relationship between mask quality and generative performance. This is a key insight that drives the entire method. Second, the experimental results are compelling. The authors demonstrate through extensive experiments on two datasets that MaskComp outperforms existing methods, including ControlNet and various versions of Stable Diffusion. The quantitative results, measured by FID scores, are significantly better than the baselines, and the qualitative results are also impressive, showing that MaskComp can generate realistic and complete objects even in cases of significant occlusion. The ablation study is also well-designed and thorough, exploring the impact of various design choices. This provides valuable insights into the inner workings of the method and demonstrates its robustness. The authors also show that the method is robust to segmentation errors, which is a crucial aspect for practical applications. The use of off-the-shelf segmentation models makes the method versatile and adaptable to different scenarios. Furthermore, the paper is well-written and easy to follow. The authors clearly explain the method and its components, making it accessible to a broad audience. The figures and tables are also well-presented and help to convey the key ideas and results. Finally, the authors have provided the code for their method, which is a valuable contribution to the community and will facilitate further research in this area. The fact that the method is built upon existing models like ControlNet and Stable Diffusion, and uses off-the-shelf segmentation models, makes it a practical and readily applicable solution for various image editing tasks. The authors have successfully demonstrated the effectiveness of their approach and have made a valuable contribution to the field.\n\n\n## Weaknesses:\n\nDespite the strengths of this paper, I have identified several weaknesses that warrant further discussion. First, while the paper demonstrates the effectiveness of MaskComp for object completion, it does not fully explore the potential of the learned model for other tasks. The method is trained specifically for the object completion task, and the experiments primarily focus on this application. While the authors show a comparison to an amodal segmentation baseline, they do not explore the possibility of using the trained model for amodal segmentation directly, or for other tasks such as image inpainting or outpainting. This limits the perceived versatility of the method. The paper states that the model is trained for 50 epochs on the AHP dataset and 36 epochs on a subset of OpenImage, which is a relatively small number of iterations, and I agree with the reviewer that more training iterations might improve the model's performance. Second, the paper lacks a thorough analysis of the failure cases of the method. While the authors present successful examples of object completion, they do not discuss the limitations of the approach or the scenarios where it might fail. This is a crucial aspect for understanding the practical applicability of the method. For example, the paper does not discuss how the method performs when the initial partial mask is inaccurate or when the object being completed is very complex. The paper also does not discuss the potential for the method to generate unrealistic or inconsistent results. This lack of discussion of limitations makes it difficult to fully assess the robustness of the method. Third, the paper does not provide a detailed analysis of the computational cost of the iterative mask denoising (IMD) process. While the authors provide the inference time for each component, they do not discuss the memory requirements or the impact of the number of iterations on the computational cost. This is an important consideration for practical applications, and the paper should provide more details on this aspect. The paper also does not compare the computational cost of MaskComp with other object completion methods. Fourth, the paper does not provide a clear explanation of the time-variant gating operation. While the authors mention that it is used to adjust the importance of conditions during the diffusion steps, they do not provide a detailed analysis of how this operation affects the generation process. The paper states that the gating operation improves the generation quality by 1.3 FID, but it does not provide a more detailed analysis of the impact of this operation. The paper also does not provide a clear explanation of the linear transform used in the gating operation. Fifth, the paper does not provide a clear explanation of the mask voting process. While the authors describe the mathematical formulation of the voting process, they do not provide a more detailed explanation of how this process works and how it affects the quality of the generated masks. The paper also does not discuss the potential limitations of the voting process, such as the possibility of introducing artifacts or biases. Sixth, the paper does not provide a clear explanation of the role of the object encoder and the auxiliary path for predicting the complete object mask. While the authors mention that the encoder captures shape information, they do not provide a more detailed analysis of how this is achieved. The paper also does not discuss the potential limitations of this approach, such as the possibility of the encoder failing to capture the complete shape of the object. Finally, the paper does not provide a clear explanation of the text prompts used in the experiments. While the authors mention that they use class labels as text prompts, they do not provide a more detailed explanation of how these prompts are generated and how they affect the generation process. The paper also does not discuss the potential limitations of using class labels as text prompts, such as the possibility of generating generic or unrealistic results. In summary, while the paper presents a novel and effective approach to object completion, it lacks a thorough analysis of the limitations, computational cost, and the impact of various design choices. These weaknesses limit the practical applicability of the method and make it difficult to fully assess its robustness.\n\n\n## Suggestions:\n\nBased on the identified weaknesses, I recommend several concrete improvements for this paper. First, the authors should explore the potential of the learned model for other tasks beyond object completion. Specifically, they should investigate the possibility of using the trained model for amodal segmentation, image inpainting, or outpainting. This would demonstrate the versatility of the method and make it more appealing to a broader audience. The authors could also explore fine-tuning the model on different datasets to assess its generalization capabilities. Second, the authors should conduct a more thorough analysis of the failure cases of the method. This should include a discussion of the scenarios where the method might fail, such as when the initial partial mask is inaccurate or when the object being completed is very complex. The authors should also discuss the potential for the method to generate unrealistic or inconsistent results. This analysis would provide a more balanced view of the method's capabilities and limitations. Third, the authors should provide a more detailed analysis of the computational cost of the iterative mask denoising (IMD) process. This should include a discussion of the memory requirements and the impact of the number of iterations on the computational cost. The authors should also compare the computational cost of MaskComp with other object completion methods. This analysis would help to assess the practical applicability of the method. Fourth, the authors should provide a more detailed analysis of the time-variant gating operation. This should include a discussion of how this operation affects the generation process and how it is related to the diffusion steps. The authors should also provide a more detailed explanation of the linear transform used in the gating operation. Fifth, the authors should provide a more detailed explanation of the mask voting process. This should include a discussion of how this process works and how it affects the quality of the generated masks. The authors should also discuss the potential limitations of the voting process, such as the possibility of introducing artifacts or biases. Sixth, the authors should provide a more detailed explanation of the role of the object encoder and the auxiliary path for predicting the complete object mask. This should include a discussion of how the encoder captures shape information and how the auxiliary path contributes to the overall performance of the method. The authors should also discuss the potential limitations of this approach, such as the possibility of the encoder failing to capture the complete shape of the object. Finally, the authors should provide a more detailed explanation of the text prompts used in the experiments. This should include a discussion of how these prompts are generated and how they affect the generation process. The authors should also discuss the potential limitations of using class labels as text prompts, such as the possibility of generating generic or unrealistic results. In addition to these specific suggestions, I also recommend that the authors conduct more training iterations, as this might improve the model's performance. Overall, these suggestions are aimed at addressing the identified weaknesses and making the paper more complete and robust.\n\n\n## Questions:\n\nI have several questions that arise from my analysis of this paper. First, I am curious about the specific details of the time-variant gating operation. While the paper mentions that it is used to adjust the importance of conditions during the diffusion steps, I would like to understand more about the exact mechanism of this operation. Specifically, how is the linear transform learned, and how does it relate to the diffusion steps? What is the impact of different gating functions on the final results? Second, I am interested in the details of the mask voting process. While the paper provides the mathematical formulation of this process, I would like to understand more about how it works in practice. How does the voting process handle cases where there are multiple distinct object parts in the generated images? What is the impact of different thresholds on the final mask quality? Third, I would like to understand more about the role of the object encoder and the auxiliary path for predicting the complete object mask. How does the encoder capture shape information from the partial object and mask? What is the impact of the auxiliary path on the overall performance of the method? How does the auxiliary path interact with the main diffusion process? Fourth, I am curious about the choice of text prompts used in the experiments. While the paper mentions that class labels are used as text prompts, I would like to understand more about how these prompts are generated and how they affect the generation process. What is the impact of using different types of text prompts, such as more descriptive or ambiguous prompts? Fifth, I would like to understand more about the limitations of the method. What are the scenarios where the method is likely to fail? How does the method perform when the initial partial mask is inaccurate or when the object being completed is very complex? What is the potential for the method to generate unrealistic or inconsistent results? Finally, I would like to understand more about the potential for extending the method to other tasks beyond object completion. How could the learned model be used for amodal segmentation, image inpainting, or outpainting? What are the challenges and opportunities for extending the method to these other tasks? These questions are aimed at clarifying some of the key uncertainties in the paper and at exploring the potential for future research directions.\n\n\n## Rating:\n\n6.5\n\n\n## Confidence:\n\n3.75\n\n\n## Decision:\n\nAccept", "other_type": "DeepReviewer_fast", "v.s.": "B", "result": "**Technical Accuracy - Evidence Analysis:**\n- From Assistant A:\n  - Claim 1: \"The core idea of iteratively refining both the mask and the generated object is both intuitive and effective.\"\n    - Evidence:  Assistant A refers to the paper's description of IMD and its motivation based on the relationship between mask quality and generation quality.\n    - Analysis: This claim accurately reflects the core contribution of the paper, which is the IMD process. The paper emphasizes the iterative refinement of masks and images.\n  - Claim 2: \"The experimental results are compelling... MaskComp outperforms existing methods, including ControlNet and various versions of Stable Diffusion.\"\n    - Evidence: Assistant A points to the quantitative results (FID scores) and qualitative results presented in the paper, and the ablation study.\n    - Analysis: This is consistent with the paper's \"Main Results\" section and Table 2, which shows lower FID scores for MaskComp compared to baselines. Figure 4 also visually supports this claim.\n  - Claim 3: \"The ablation study is also well-designed and thorough, exploring the impact of various design choices.\"\n    - Evidence: Assistant A refers to the ablation studies mentioned in the paper.\n    - Analysis: The paper includes several ablation studies in Section 5.3 and Tables 1, 3, and 4, examining design choices like mask conditions, occlusion rates, IMD steps, segmentation networks, etc.\n  - Claim 4: \"The use of off-the-shelf segmentation models makes the method versatile and adaptable to different scenarios.\"\n    - Evidence: Assistant A notes the use of SAM in the segmentation stage.\n    - Analysis: The paper explicitly mentions using SAM and ablates with other segmentation models like CLIPSeg and SEEM in Table 1a, supporting the claim of versatility.\n\n- From Assistant B:\n  - Claim 1: \"The proposed Iterative Mask Denoising (IMD) process is a novel and effective way to progressively refine object masks, leading to superior completion results.\"\n    - Evidence: Assistant B refers to the paper's description of IMD and its claimed benefits.\n    - Analysis: This accurately reflects the paper's central claim and methodology.\n  - Claim 2: \"The experimental results, which demonstrate the superiority of MaskComp over existing methods like ControlNet and Stable Diffusion, provide strong empirical support for the proposed approach.\"\n    - Evidence: Assistant B points to the experimental results showing MaskComp's outperformance.\n    - Analysis: Consistent with the paper's \"Main Results\" section and Table 2, similar to Assistant A's claim.\n  - Claim 3 (Weakness): \"First, the proposed method's efficiency is a significant concern... total inference time for the entire process is 77.5 seconds per image.\"\n    - Evidence: Assistant B quotes inference times from Table 1c (14.3s Gen + 1.2s Segm = 15.5s per step) and multiplies by 5 steps and 5 samples per step (implicitly, although N=5 samples is for segmentation stage, not generation within IMD step, and the 5 steps is correct).  The calculation is slightly off, 15.5s * 5 steps = 77.5s.\n    - Analysis:  Table 1c shows inference time per IMD step is 15.5s. The paper mentions 5 IMD steps.  Assistant B's calculation is based on 5 IMD steps, which is stated in the paper (Implementation Details). The calculation is correct based on the provided numbers, although the reasoning about 5 samples per step is slightly misinterpreting N=5.  The concern about efficiency is valid as the paper itself acknowledges reduced throughput due to multiple diffusion processes.\n  - Claim 4 (Weakness): \"Second, the method's reliance on paired data for training is a significant limitation... implies a mapping from partial to complete objects, which typically requires paired data.\"\n    - Evidence: Assistant B infers paired data requirement from the problem definition and training datasets (AHP, OpenImage).\n    - Analysis: The paper describes training CompNet to recover complete objects from partial objects and masks. While not explicitly stated as *paired* data requirement, the training setup implies the need for corresponding partial and complete object data.  Datasets like AHP and OpenImages, when used for object completion tasks, often involve creating partial views from complete objects, suggesting a paired or near-paired data setup.\n  - Claim 5 (Weakness): \"Third, the method's dependence on a pre-trained object completion method, specifically ControlNet, raises concerns about the fairness of comparisons...\"\n    - Evidence: Assistant B quotes the paper stating \"We build CompNet based on popular ControlNet\".\n    - Analysis: The paper explicitly states building upon ControlNet.  Assistant B's concern about fair comparison is valid, as MaskComp leverages ControlNet's pre-existing capabilities.\n  - Claim 6 (Weakness): \"Fourth, the method's requirement for ground-truth text prompts is a significant limitation for real-world applications... prompts are necessary to train ControlNet.\"\n    - Evidence: Assistant B quotes the paper mentioning text prompts generated by BLIP for training ControlNet and that baselines are given the same prompts.\n    - Analysis: The paper mentions using text prompts for training and evaluation. ControlNet and Stable Diffusion are text-conditional models, so text prompts are indeed relevant.  The concern about needing ground-truth prompts in real applications is valid, although the paper uses BLIP to generate them, suggesting a degree of automation.\n  - Claim 7 (Weakness): \"Finally, the experimental evaluation is limited by the choice of baselines... not specifically designed for object completion.\"\n    - Evidence: Assistant B points out that baselines (ControlNet, Kandinsky, Stable Diffusion) are not *specifically* object completion methods.\n    - Analysis: While ControlNet can be adapted for conditional generation tasks, and Stable Diffusion is a general image generation model, Assistant B is correct that these are not *dedicated* object completion methods in the same way MaskComp is designed to be.  This is a reasonable critique of the baseline selection.\n\n- Comparative Assessment:\n  - Both assistants accurately identify the core method and its claimed strengths.\n  - Assistant B delves deeper into potential weaknesses, accurately pointing out efficiency concerns, possible paired data reliance, dependence on ControlNet, and text prompt requirements. These are all valid points based on the paper's description.\n  - Assistant B's calculation of inference time is slightly off in reasoning (misinterpreting N), but the final number is correct based on the paper's data.\n  - Both assistants' claims are generally well-supported by evidence from the paper. Assistant B provides more critical technical assessments.\n\n**Technical Accuracy - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate good technical accuracy in understanding the paper's core contributions and experimental setup. However, Assistant B goes further by identifying and articulating potential technical limitations and concerns related to efficiency, data requirements, and baseline comparisons, all of which are grounded in the paper's details. Assistant B's analysis of weaknesses demonstrates a more critical and in-depth technical understanding.\n**Better Assistant:** B\n\n---\n**Constructive Value - Evidence Analysis:**\n- From Assistant A:\n  - Suggestions:\n    - \"explore the potential of the learned model for other tasks beyond object completion. Specifically, they should investigate the possibility of using the trained model for amodal segmentation, image inpainting, or outpainting.\"\n      - Analysis: This is a reasonable and actionable suggestion to broaden the impact and demonstrate versatility.\n    - \"conduct a more thorough analysis of the failure cases of the method.\"\n      - Analysis:  Standard constructive feedback to improve robustness understanding.\n    - \"provide a more detailed analysis of the computational cost of the iterative mask denoising (IMD) process... compare the computational cost of MaskComp with other object completion methods.\"\n      - Analysis:  Actionable suggestion to address a practical concern.\n    - \"provide a more detailed analysis of the time-variant gating operation... mask voting process... role of the object encoder and the auxiliary path... text prompts used in the experiments.\"\n      - Analysis: These are all requests for more in-depth analysis and explanation of specific components, which are constructive for improving clarity and understanding.\n    - \"conduct more training iterations, as this might improve the model's performance.\"\n      - Analysis:  A standard suggestion for potentially improving performance.\n\n- From Assistant B:\n  - Suggestions:\n    - \"explore techniques to reduce the number of iterations required for convergence... parallelizing the generation and segmentation steps.\"\n      - Analysis:  Specific and actionable suggestions to improve efficiency.\n    - \"explore methods to reduce or eliminate this requirement [paired data]... self-supervised learning methods or leveraging existing large-scale datasets with unpaired data... cycle-consistency or adversarial training... synthetic data or data augmentation... pre-trained models or transfer learning.\"\n      - Analysis:  Provides a range of concrete techniques to address the paired data concern.\n    - \"explore methods to reduce the reliance on this specific pre-trained model [ControlNet]... alternative base models... ablation study that isolates the contribution of the IMD pipeline from the base model... sensitivity of MaskComp to the choice of the base model.\"\n      - Analysis:  Actionable suggestions to address dependence on ControlNet and improve evaluation rigor.\n    - \"acknowledge that obtaining such prompts is not always feasible and explore alternative approaches... automatically generated text prompts or explore methods that do not rely on text prompts at all... impact of inaccurate or misleading text prompts... techniques for prompt refinement or correction.\"\n      - Analysis:  Provides concrete ways to address the text prompt dependency.\n    - \"include comparisons with more recent and specialized object completion methods... wider range of evaluation metrics... expanded to include more complex objects and scenes... analyze the failure cases of the method.\"\n      - Analysis:  Actionable suggestions to improve evaluation comprehensiveness and rigor.\n\n- Comparative Assessment:\n  - Both assistants provide constructive suggestions.\n  - Assistant B's suggestions are generally more specific and actionable. For example, instead of just saying \"analyze computational cost,\" Assistant B suggests \"parallelizing generation and segmentation steps.\" Instead of just \"reduce paired data reliance,\" Assistant B lists specific techniques like \"self-supervised learning,\" \"cycle-consistency,\" etc.\n  - Assistant B's suggestions are more directly tied to the weaknesses identified in their review, making them highly relevant and targeted.\n  - Assistant A's suggestions are also constructive but tend to be broader and less detailed in terms of concrete actions.\n\n**Constructive Value - Judgment:**\n**Evidence-Based Reason:** Both assistants offer valuable suggestions for improvement. However, Assistant B's feedback is more actionable and specific, providing concrete techniques and directions for the authors to address the identified weaknesses. Assistant B's suggestions are also more directly linked to the limitations they pointed out, making the feedback more targeted and impactful.\n**Better Assistant:** B\n\n---\n**Analytical Depth - Evidence Analysis:**\n- From Assistant A:\n  - Analysis focuses on:\n    - Core idea (iterative refinement)\n    - Experimental results (quantitative and qualitative)\n    - Ablation study\n    - Strengths (intuitive idea, compelling results, thorough ablation, versatility, well-written)\n    - Weaknesses (limited task exploration, failure case analysis, computational cost analysis, gating operation explanation, mask voting explanation, object encoder/auxiliary path explanation, text prompt explanation)\n  - Depth:  Provides a good overview of the paper's strengths and weaknesses, covering key aspects like methodology, experiments, and presentation.  Identifies several areas for deeper analysis in the paper.\n\n- From Assistant B:\n  - Analysis focuses on:\n    - Core idea (IMD process)\n    - Strengths (novelty of IMD, empirical support, clarity, ControlNet base)\n    - Weaknesses (efficiency, paired data reliance, ControlNet dependence, text prompt requirement, baseline limitations, evaluation scope)\n    - Questions (efficiency comparison, dataset generalizability, handling heavy occlusion, convergence failure, layer switching details, computational cost comparison)\n  - Depth:  Provides a more in-depth analysis of potential limitations, focusing on efficiency, data dependencies, model dependencies, and evaluation rigor.  Questions are targeted at probing deeper into these limitations and seeking further clarification.  The analysis of weaknesses is more structured and detailed than Assistant A's.\n\n- Comparative Assessment:\n  - Both assistants demonstrate analytical depth by examining key aspects of the paper.\n  - Assistant B's analysis is arguably deeper in its critical evaluation of potential limitations. Assistant B's weaknesses are more focused and structured, delving into specific concerns about the method's practical applicability and evaluation rigor.\n  - Assistant B's questions also reflect a deeper analytical engagement with the paper, probing into specific areas of concern and seeking more detailed explanations.\n  - While Assistant A provides a good overview, Assistant B's analysis is more focused on critical evaluation and identifying potential shortcomings.\n\n**Analytical Depth - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate analytical depth. However, Assistant B's review exhibits a greater analytical depth by focusing more critically on potential limitations and practical concerns. Assistant B's structured analysis of weaknesses and targeted questions demonstrate a more thorough and probing examination of the paper's methodology and evaluation.\n**Better Assistant:** B\n\n---\n**Communication Clarity - Evidence Analysis:**\n- From Assistant A:\n  - Clarity:  Well-written, clear summary and points. Uses bullet points for strengths and weaknesses, and numbered suggestions.  Language is easy to understand.\n  - Specificity:  Suggestions are reasonably specific, though sometimes broad (e.g., \"more detailed analysis\"). Strengths and weaknesses are clearly articulated.\n  - Reasoning: Reasoning is generally well-explained, connecting claims to evidence from the paper.\n  - Examples: Uses examples by referring to specific aspects of the paper (e.g., ablation study, ControlNet).\n\n- From Assistant B:\n  - Clarity:  Well-written, clear summary and points. Uses bullet points for strengths, weaknesses, suggestions, and questions. Language is easy to understand.\n  - Specificity:  Suggestions are highly specific and actionable, listing concrete techniques and directions. Weaknesses are clearly and specifically articulated with justifications. Questions are targeted and specific.\n  - Reasoning: Reasoning is clearly explained, with justifications for weaknesses and suggestions.  Connects claims to evidence from the paper and logical deductions.\n  - Examples: Uses examples by referring to specific parts of the method (IMD process, ControlNet) and experimental details (inference time).\n\n- Comparative Assessment:\n  - Both assistants communicate clearly and effectively.\n  - Assistant B's communication is slightly clearer in terms of structure and organization, using bullet points effectively for each section (strengths, weaknesses, suggestions, questions).\n  - Assistant B's language is also very precise and direct in articulating weaknesses and suggestions.\n  - Both use examples effectively, but Assistant B's examples are often more specific and directly tied to the identified weaknesses and suggested improvements.\n\n**Communication Clarity - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate excellent communication clarity. Both reviews are well-written, clearly structured, and easy to understand. However, Assistant B's review is slightly better in communication clarity due to its more structured organization, highly specific and actionable suggestions, and precise language in articulating weaknesses and recommendations.\n**Better Assistant:** B\n\n---\n**Comprehensive Analysis:**\nAssistant A provides a good, balanced review, summarizing the paper effectively, highlighting strengths and weaknesses, and offering constructive suggestions. Assistant A appreciates the novelty and effectiveness of the method and acknowledges the thoroughness of the experiments. However, the weaknesses and suggestions are somewhat general.\n\nAssistant B provides a more critical and in-depth review. While also acknowledging the strengths, Assistant B focuses more on potential limitations and practical concerns related to efficiency, data dependencies, model dependencies, and evaluation rigor. Assistant B's weaknesses are more specific and well-justified, and the suggestions are more actionable and concrete, directly addressing the identified limitations. Assistant B's questions are also more targeted and probing, seeking deeper clarification on key aspects.\n\nAssistant B's review demonstrates a more rigorous and critical analysis of the paper's technical aspects and practical implications. While both reviews are valuable, Assistant B's review offers a more insightful and actionable critique.\n\n**Overall Judgment:**\n**Evidence-Based Reason:** Assistant B's review is stronger overall because it demonstrates a deeper technical understanding, provides more specific and actionable feedback, and offers a more critical and insightful analysis of the paper's limitations and potential improvements. While both reviews are competent, Assistant B's review provides a more valuable and rigorous assessment that would be more helpful to the authors in improving their work. Assistant B consistently provides more detailed evidence and reasoning for their judgments across all aspects.\n**Better Assistant:** B"}
{"id": "sqRgz88TM3", "year": 2024, "paper_context": "\\title{VFLAIR: A Research Library and Benchmark for Vertical Federated Learning}\n\n\\begin{abstract}\n  Vertical Federated Learning (VFL) has emerged as a collaborative training paradigm that allows participants with different features of the same group of users to accomplish cooperative training without exposing their raw data or model parameters. VFL has gained significant attention for its research potential and real-world applications in recent years, but still faces substantial challenges, such as in defending various kinds of data inference and backdoor attacks. Moreover, most of existing VFL projects are industry-facing and not easily used for keeping track of the current research progress. We also benchmark $11$ attacks and $8$ defenses performance under different communication and model partition settings and draw concrete insights and recommendations on the choice of defense strategies for different practical VFL deployment scenarios. \n  \n\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\nThe concept of Federated Learning (FL) was first introduced by Google in 2016~\\citep{McMahan2016fl} describing a cross-device scenario where millions of mobile users collaboratively train a shared model using their local private data\nwithout centralizing these data. This scenario is regarded as Horizontal FL (HFL)~\\citep{yang2019federatedbook} as data are partitioned by sample. In another type of FL, regarded as Vertical FL (VFL)~\\citep{yang2019federatedbook}, data are partitioned by feature. VFL is often applied in industrial collaborative learning scenarios where each organization controls disjoint features of a common group of users. In VFL, local data and local model are kept private at each participant. Instead, local model outputs and their gradients are transmitted between parties.\n\nVFL has drawn increasing attention from both academic and industry in recent years with hundreds of research papers published every year and a number of open-sourced projects released, including FATE~\\citep{FATE,liu2021fate}, Fedlearner~\\citep{Fedlearner}, PaddleFL~\\citep{PaddleFL}, Pysyft~\\citep{ryffel2018generic,romanini2021pyvertical}, FedTree~\\citep{fedtree2022}, and FedML~\\citep{he2020fedml}. Real-world industrial cases are also emerged in the field of advertising~\\citep{cai2020Bytedance,tencent2021} and finance~\\citep{cheng2020federated,Cheng2022DigitalEra} etc. However, mainstream VFL projects such as FATE are industrial grade and not designed for keeping up with research advances.  %, to name a few.\n\nMeanwhile, research interests for VFL have been growing rapidly over the past years, focusing on improving various aspects of VFL protocols, such as communication efficiency~\\citep{fu2021vf2boost,liu2022fedbcd,castiglia2022compressed,fu2022towards}, robustness to attacks~\\citep{liu2021rvfr,cheng2021secureboost,li2022label,zou2022defending,zou2023mutual,sun2022label,yang2022differentially}, model utility~\\citep{li2022semi,Yitao2022multiview,feng2020multi,feng2022semisupervised}, and fair incentive designs~\\citep{liu2021achieving,qi2022fairvfl}. % are two main lines of reserach work considering VFL. \nFor communication efficiency, methods like decrease communication rounds using multiple local updates between each round~\\citep{liu2022fedbcd,fu2022towards} or compress information~\\citep{castiglia2022compressed} have been proposed. As for data security and privacy, various attacks injected by one or multiple parties aiming to either steal other parties' private label~\\citep{li2022label,fu2021label,zou2022defending}, private features~\\citep{jin2021cafe,luo2021feature,li2022ressfl,jiang2022comprehensive,ye2022feature}, sensitive attributes~\\citep{Song2020Overlearning} and sample relations~\\citep{qiu2022your}, or negatively impact the model behavior~\\citep{liu2021rvfr,zou2022defending} have been put forward. Multiple defending methods have also been proposed to tackle these threats, including adding noise~\\citep{dwork2006DP,zou2022defending,li2022label}, sparsifying gradients~\\citep{aji2017sparse,fu2021label,zou2022defending}, discreting gradients~\\citep{fu2021label}, label differential privacy~\\citep{ghazi2021deep,yang2022differentially}, adding distance correlation regularizor~\\citep{sun2022label,vepakomma2019reducing}, disguising labels~\\citep{zou2022defending}, adding mutual information regularizer~\\citep{zou2023mutual}, adversarial training~\\citep{sun2021defending,li2022ressfl} or performing robust feature recovery~\\citep{liu2021rvfr}. However, each of these defenses are evaluated under specific tasks and settings, lacking of key insights and metrics on evaluating these defense strategies to defend all possible attacks in practical deployment.\n\nTo facilitate future research for VFL, we introduce a lightweight and comprehensive VFL framework, namely \\verb|VFLAIR|, which includes not only basic VFL training and inference for a variety of models and settings but also efficiency enhancement techniques and multiple defense methods that mitigate potential threats. Moreover, we perform extensive experiments on combinations of the above settings using multiple datasets to provide different perspectives on VFL efficiency and safety. We believe \\verb|VFLAIR| and these benchmark results will provide researchers with useful tools and guidance for their future work. Our contributions are summarized in the following:\n\n\\textbf{(1).} We design \\verb+VFLAIR+, a lightweight and extensible VFL framework that aims to facilitate research development of VFL (see \\cref{fig:VFLAIR}). We design standardized pipelines for VFL training and validation, supporting $13$ datasets, $29$ different local model architectures including linear regression, tree and neural networks, $6$ different global models, $2$ model partition settings, $5$ communication protocols, $1$ encryption method, $11$ attacks and $8$ defense methods, each implemented as a distinct module and can be easily extended.\n\n\\textbf{(2).} We propose new evaluation metrics and modules, and perform extensive experiments to benchmark various perspectives of VFL, from which we draw key insights on VFL system design choice, in order to promote future development and practical deployment of VFL. % Performance of VFL models trained with standard VFL training, attack performance of $11$ attacks, defense capability of $8$ defense methods are evaluated under both aggVFL and splitVFL with communication protocol using both FedSGD and FedBCD.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.89\\linewidth]{figures/VFLAIR_7_5.pdf}\n    \\caption{{An overview of the Components of VFLAIR.}} % After specifying the settings of VFL system, attacks and defense methods can be evaluated on top of the system. Overall evaluation metrics can be further exploit to get recommendation for selecting a proper defense.\n\\label{fig:VFLAIR}\n\\end{figure}\n\n\n\n\\section{Related Work}\n\n\nA number of open-source FL projects have been developed supporting VFL. FATE~\\citep{FATE,liu2021fate} is an industry-grade FL project which supports a variety of model architectures and secure computation protocols; Fedlearner~\\citep{Fedlearner} is specialized in advertising scenarios; PaddleFL~\\citep{PaddleFL} supports 2-party and 3-party VFL with MPC protection; Pysyft~\\citep{ryffel2018generic,romanini2021pyvertical} introduces PyVertical, which focus on SplitNN-type of VFL settings; FedTree~\\citep{fedtree2022} focuses on tree-based VFL only; FedML~\\citep{he2020fedml} supports basic training of VFL with logistic regression models. Real-world industrial applications have been witnessed in domains such as advertising~\\citep{cai2020Bytedance,tencent2021} and finance~\\citep{cheng2020federated,Cheng2022DigitalEra}. These works demonstrate the widespread interest and the practical significance of VFL. \nHowever, these works are often relatively heavy-weight as they are designed for industrial deployment. \nOn the other hand, most existing benchmarks on FL focus on HFL scenario~\\citep{chai2020fedeval,lai2022fedscale,zhang2023fedaudio}. For VFL, ~\\citep{kang2022framework} evaluates several defense strategies for data reconstruction attacks; SLPerf~\\citep{zhou2023slperf} focuses on benchmarking and comparing various kinds of splitNN scenarios like splitVFL. No existing work provides a comprehensive evaluation covering a variety of key aspects of VFL settings, including model performance, communication efficiency and robustness to attacks. Due to space limitation, %{\\color{red}{we only include previous works on VFL Framework and Benchmarks here}} but \n} \\tianyuan{No, at least currently, no.}\n\n\n\n\\section{VFL Framework}\n\nIn a typical VFL setting with $K$ parties, each party owns their local private feature $\\{X_{k}\\}_{k=1}^K$  and local model $\\{G_{k}\\}_{k=1}^K$ with parameters $\\{\\theta_{k}\\}_{k=1}^K$ respectively. Only one party controls the private label information $Y$ and is referred to as \\textit{active} party while other parties are referred to as \\textit{passive} parties.\nThe active party also controls a global trainable model parameterized by $\\varphi$ (splitVFL) or global non-trainable function $F$ (aggVFL) to aggregate each party's local model output. Note in tree-based VFL the global function is an aggregation function that identifies the optimal feature split based on feature splitting information received from all parties. %The classification task is also regarded as the main task since it is the purpose for the collaboration between parties and reflects the utility of the VFL model. \nWithout loss of generality, we assume that the $K^{th}$ party is the active party while other $K-1$ parties are passive parties.\nIn the collaborative training process of NN-based VFL, each party computes its local feature embedding $H_k=G_k(X_k,\\theta_k), k=1,\\dots,K$. The active party collects $\\{H_k\\}_{k=1}^K$ and gets the final prediction $\\hat{Y}=F(H_1,\\dots,H_K, \\varphi)$. %Based on whether $F$ is a trainable model or a non-trainable function, we follow~\\citep{liu2022vertical} to categorize VFL systems into splitVFL and aggVFL separately. \nThe loss $\\mathcal{L}=\\ell(Y,\\hat{Y})$ is calculated at the active party. The gradient w.r.s. to $H_k$ as $g_k = \\frac{\\partial \\mathcal{L}}{\\partial H_k}, k=1,\\dots,K$ are then calculated and transmitted back to each party by the active party. Using these gradients, each party performs local model updates by SGD using $\\nabla_{\\theta_k}\\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial \\theta_k}=\\frac{\\partial \\mathcal{L}}{\\partial H_k}\\frac{\\partial H_k}{\\partial \\theta_k}, k=1,\\dots,K$. Also the active party performs model update with SGD on global model $F$ if it is trainable using $\\nabla_{\\varphi}\\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial \\varphi}$. In the inference procedure, the same is done but without the backward gradient descent to get the prediction of labels. If the exchange of $H_k$ and $g_k$ is performed each round, such VFL protocol is referred as FedSGD protocol. On the other hand, if communication is done every $Q>1$ steps of local updates, such protocol is referred to as FedBCD~\\citep{liu2022fedbcd}. %{\\color{red}{Also, communication protocols that quantifies $H_k$ (using only $2^{b}$ bits for each element), or sparsifies $H_k$ by preserving only Top-k useful elements (using only $0<r<1$ proportion of the total elements), before information transmission to reduce communication cost, are referred to as Quantize~\\citep{castiglia2022compressed} and Top-k~\\citep{castiglia2022compressed} respectively. Further, cached FedBCD communication protocol using different batches of data for local update steps are termed CELU-VFL~\\citep{fu2022towards}.}}\n\n\n\n\n\\section{Overview of VFLAIR}\n\n\n\\textbf{Implemented Components.} An overview of the components of \\verb|VFLAIR| is shown in \\cref{fig:VFLAIR}.\n\\verb|VFLAIR| incorporates not only basic VFL training and testing process for both NN-based and tree-based VFL of various settings, but also multiple existing efficiency enhancement techniques, data leakage and model utility impairing attacks as well as defending methods that aim to mitigate potential threats. \\verb|VFLAIR| provides support for both aggVFL and splitVFL with easily adjustable model architectures. %For tree-based VFL, \\verb|VFLAIR| accommodates XGBoost and Random Forest for aggregation. \nCurrently, \\verb|VFLAIR| supports $5$ communication protocols %including FedSGD (vanilla protocol), FedBCD~\\citep{liu2022fedbcd}, CELU-VFL~\\citep{fu2022towards}, Quantize~\\citep{castiglia2022compressed} and Top-k~\\citep{castiglia2022compressed} \nto improve communication efficiency. Also, $11$ existing attacks and $8$ \ndefenses are supported. Moreover, \\verb|VFLAIR| supports the comprehensive assessment of defense performance using carefully designed metrics (see \\cref{sec:metrics_definition}), based on which defense strategy recommendations can be provided. Paillier Encryption~\\citep{cheng2021secureboost} is also supported to further protect transmitted results.\nIn total, $13$ datasets from  a diverse range of industrial domains, including but not limited to medical, financial, and recommendation\nare supported.\n\n\\textbf{How to use and extend.} \\verb|VFLAIR| is a light-weight and comprehensive VFL framework that can be launched on a single GPU or CPU (see \\cref{tab:hardware} for its system requirement compared to FATE). \\verb|VFLAIR| facilitates the easy integration of different datasets for model training and inference through simple dataset loading and partitioning functions. New attacks and defenses can be quickly incorporated into the framework thanks to the modular structure.  % VFL model utility and its susceptibility to attacks under various defense strategies by using $5$ carefully designed evaluation metrics. Furthermore,\n\n\n\n\\section{VFL Benchmark}\n\n\n\\subsection{VFL Settings, Models and Datasets} \nUsing \\verb|VFLAIR|, We benchmark the VFL main task performance using $13$ datasets including MNIST~\\citep{MNISTdataset}, CIFAR10~\\citep{krizhevsky2009learning}, CIFAR100~\\citep{krizhevsky2009learning}, NUSWIDE~\\citep{NUSWIDEdataset}, Breast Cancer~\\citep{street1993nuclear}, Diabetes~\\citep{Diabetes1999dataset}, Adult Income~\\citep{AdultIncome1996dataset}, Criteo~\\citep{guo2017deepfm_Criteo}, Avazu~\\citep{qu2018product_Avazu}, Cora~\\citep{mccallum2000automating_Cora}, News20~\\citep{lang1995News20},Credit~\\citep{Dua:2019} and Nursery~\\citep{Dua:2019}.\n\nWe explore $2$ distinct architectures, namely aggVFL and splitVFL, and comprehensively benchmark their performance. For global model $F$, a global softmax function is applied under aggVFL setting while a 1-layer fully-connected model serves as the global model for splitVFL setting (except for Cora dataset, for which a 1-layer graph convolution layer is applied). \nAdditionally, we investigate the impact of different communication protocols by comparing FedBCD~\\citep{liu2022fedbcd} ($Q=5$) and CELU-VFL~\\citep{fu2022towards} ($Q=5,W=5$), as well as compression mechanisms Quantize ($b=16$)~\\citep{castiglia2022compressed} and Top-k ($r=0.9$)~\\citep{castiglia2022compressed} to the conventional FedSGD, as discussed in \\cref{sec:vfl_setting} and further provide insights into the communication cost reduction achieved by communication efficient protocols,\nas well as the impact of FedBCD when various attacks and defenses are deployed. % demonstrating its effectiveness in enhancing efficiency.\nWe also evaluate the impact of the number of participating parties as well as the type of local model (logistic regression, tree, NN) on the main task performance of VFL. For tee-based VFL, we further benchmark both Random Forrest and XGBoost algorithms. Moreover, for \ntree-based VFL, we employ Paillier Encryption~\\citep{cheng2021secureboost} to protect transmitted information and measure its impact on computation efficiency. %such as labels and gradients. {\\color{red}{We measure the execution time along side the model performance across various tree-based VFL aggregation algorithms with and without the application of Paillier Encryption, shedding light on the execution overhead incurred by encryption.}} \n\n\n\\subsection{Attacks and Defenses} \\label{subsec:attacks}\n \n\\begin{table}[!tb]\n\\caption{Summary of attacks for NN-based VFL}\n\\label{tab:attacks}\n\\resizebox{0.998\\linewidth}{!}{\n    \\begin{tabular}{c|c|c|c|c}\n    \\toprule\n    Attack Type & Attack & \\shortstack{Requirements\\\\/ Limitations} & \\shortstack{Attacker\\\\Party} & Attack Performance (AP) \\\\\n    \\midrule\n    \\multirow{7}{*}{\\shortstack{Label\\\\Inference\\\\(LI)}} & \\shortstack{Norm-based Scoring (NS)~\\citep{li2022label}} & \\multirow{2}{*}{\\shortstack{binary classification,\\\\sample-level}} & \\multirow{7}{*}{passive} & \\multirow{2}{*}{\\shortstack{AUC of\\\\inferred labels}}\\\\\n    \\cline{2-2}\n    \\\\[-1em]\n     & \\shortstack{Direction-based Scoring (DS)~\\citep{li2022label}} &  &  & \\\\\n    \\cline{2-3}\n    \\cline{5-5}\n    \\\\[-1em]\n     & \\shortstack{Direct Label Inference (DLI)~\\citep{li2022label,zou2022defending}} & sample-level &  & \\multirow{5}{*}{\\shortstack{ratio of correctly\\\\inferred labels\\\\ \\,}}\\\\\n    \\cline{2-3}\n    \\\\[-1em]\n    & Batch-level Label Inference (BLI)~\\citep{zou2022defending} & - &  & \\\\\n    \\cline{2-3}\n    \\\\[-1em]\n    & Passive Model Completion (PMC)~\\citep{fu2021label} & \\multirow{2}{*}{\\shortstack{auxiliary labeled\\\\data for each class}} &  & \\\\\n    \\cline{2-2}\n    \\\\[-1em]\n    & Active Model Completion (AMC)~\\citep{fu2021label} &  &  & \\\\\n    \\cline{1-5}\n    \\\\[-1em]\n    \n    \n    \\multirow{3}{*}{\\shortstack{Feature\\\\Reconstruction\\\\(FR)}} & \\shortstack{Generative Regression Network (GRN)~\\citep{luo2021feature}} & \\shortstack{black-box} & \\multirow{3}{*}{\\shortstack{active}} & \\multirow{3}{*}{\\shortstack{$1-\\text{MSE}(U_0,U_{rec})$ }}\\\\\n    \\cline{2-3}\n    \\\\[-1em]\n     & \\shortstack{Training-based Back Mapping by\\\\model inversion (TBM)~\\citep{li2022ressfl}} & \\shortstack{white-box,\\\\auxiliary i.i.d. data} &  & \\\\\n    \\cline{1-5}\n    \\\\[-1em]\n    \\shortstack{Targeted\\\\Backdoor (TB)\\\\ \\,} & \\shortstack{Label Replacement\\\\Backdoor (LRB)~\\citep{zou2022defending}} & \\shortstack{$\\geq1$ sample of\\\\target class} & \\shortstack{passive\\\\ \\,\\\\ \\,} & \\shortstack{ratio of triggered samples\\\\inferred as target class}\\\\\n    \\cline{1-5}\n    \\\\[-1em]\n    \\multirow{2}{*}{\\shortstack{Non-targeted\\\\Backdoor (NTB)}} & Noisy-sample Backdoor (NSB)~\\citep{zou2023mutual} & - & \\multirow{2}{*}{passive} & \\multirow{2}{*}{\\shortstack{MP difference between total\\\\and noisy/missing samples}}\\\\\n    \\cline{2-3}\n    \\\\[-1em]\n     & Missing Feature (MF)~\\citep{liu2021rvfr} & - &  & \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\nWe benchmark the performance of $11$ attacks with $8$ defenses on $3$ datasets including MNIST~\\citep{MNISTdataset}, CIFAR10~\\citep{krizhevsky2009learning} and NUSWIDE~\\citep{NUSWIDEdataset}.\nFor these evaluations, we mainly consider a VFL setting with $1$ active party and $1$ passive party following original works~\\citep{li2022label,luo2021feature,li2022ressfl}, denoted as party $a,p$ respectively, with each party owning their local feature $X_a, X_p$ and local model $G_a, G_p$.\nThe local model output of the active and passive party are denoted as $H_a, H_p$ respectively.\nWe summarized the evaluated attacks in \\cref{tab:attacks}. Note in \\cref{tab:attacks}, NS and DS attacks can only be applied to binary classification scenarios; \"sample-level\" indicates that the attack requires gradient information for each sample, whereas \"batch-level\" means only batch-level gradients information are available; \"black-box\" indicates that the model is kept private at the party under attack, but can be queried by the attacker and honestly return the output to the attacker, whereas \"white-box\" means the attacker has access to the model; $\\text{MSE}(U_0,U_{rec})=\\mathbb{E}[(u_0^{(f)}-u_{rec}^{(f)})^2]$ where $u_0^{(f)},u_{rec}^{(f)}$ are the $f^{th}$ feature of original input $U_0$ and recovered input $U_{rec}$ respectively. LI, FR and NTB attacks are inference time attacks that are launched separately from VFL training procedure while only TB attacks are training time attacks. \n Defense methods are summarized in \\cref{tab:defense_param} with respective hyper-parameters. \n \n\n\\begin{table}[!tb]\n\\caption{Summary of defense methods and tested hyper-parameter values for NN-based VFL.} % \\tianyuan{parameter double check.}\n\\label{tab:defense_param}\n\\resizebox{0.998\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c|c|c|c}\n    \\toprule\n        Defense & Methodology & Hyper-parameter & Hyper-parameter Values \\\\\n    \\midrule\n        G-DP~\\citep{dwork2006DP,fu2021label,zou2022defending} & add noise to gradients or local prediction & DP Strength & $0.0001,0.001,0.01,0.1$\\\\\n        L-DP~\\citep{dwork2006DP,fu2021label,zou2022defending} & add noise to gradients or local prediction & DP Strength & $0.0001,0.001,0.01,0.1$\\\\\n        GS~\\citep{aji2017sparse,fu2021label,zou2022defending} & drop gradient elements close to $0$ & Sparsification Rate & $95.0\\%,97.0\\%,99.0\\%,99.5\\%$\\\\ %\n        GPer~\\citep{yang2022differentially} & perturb gradient with that of other class & Perturbation Strength & $0.0001,0.001,0.01,0.1$\\\\\n        dCor~\\citep{sun2022label,vepakomma2019reducing} & distance correlation regularization & Regularizer Strength & $0.0001,0.01,0.1,0.3$\\\\\n        CAE~\\citep{zou2022defending} & disguise label & Confusion Strength $\\lambda$ & $0.0,0.1,0.5,1.0$\\\\\n        DCAE~\\citep{zou2022defending} & discrete gradient in addition to CAE & Confusion Strength $\\lambda$ & $0.0,0.1,0.5,1.0$\\\\\n    MID~\\citep{zou2023mutual} & mutual information (MI) regularization & Regularizer Strength $\\lambda$ & $0.0,1e^{-8},1e^{-6},1e^{-4},0.01,0.1,1.0,1e^{2},1e^{4}$ \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\n\\subsection{Evaluation Metrics}\\label{sec:metrics_definition}\n\\textbf{Main Task Performance (MP).}\nMP is defined as the final model prediction accuracy on the test dataset, which reveals the utility of the VFL system.\n\n\\textbf{Communication and Computation Efficiency.} Number of communication rounds (\\#Rounds) and the amount of data transferred for each round (Amount) are used for measuring communication efficiency. Execution Time (Exec.Time) is used to measure computation efficiency. \n\n\\textbf{Attack Performance (AP).} %As we also include various kinds of attacks and defense in our platform and benchmark experiments, we define AP so as to evaluate the success rate of a given attack which also reflects the vulnerability of a VFL system to that given attack. \nThe definition of AP varies with respect to the type of the attack and is summarized in \\cref{tab:attacks}. \n\n\\textbf{Defense Capability Score (DCS).} %To evaluate the defense methods, \nIntuitively, an ideal defense should not compromise the utility of the original main task and should thwart the attack completely. Therefore, considering that both AP and MP are key metrics to evaluate defenses. We further propose \\textbf{Defense Capability Score (DCS)}, to directly compare all the defenses under one unified metric. Let $df=(\\text{AP},\\text{MP})$ represents the performance of a defense on an AP-MP graph, then we define its defense capability score (DCS) based on the distance between $df$ to an ideal defense $df^{*}=(\\text{AP}^{*}, \\text{MP}^{*})$.  MP$^{*}$ is the MP of VFL without defense and AP$^{*}$ is set to $0.0$ representing the performance of a completely incapable attacker. Then, we formulate the definition of DCS as:\n\\begin{equation} \\label{eq:dcs}\n \\setlength\\abovedisplayskip{0.05cm}\n \\setlength\\belowdisplayskip{0.05cm}\n    \\text{DCS} = \\frac{1}{1+D(df,df^{*})} = \\frac{1}{1+\\sqrt{(1-\\beta)(\\text{AP}-\\text{AP}^{*})^2+\\beta(\\text{MP}-\\text{MP}^{*})^2}},\n\\end{equation}\n\n\\begin{wrapfigure}{l}{0.36\\textwidth}\n    \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/score/DCS_LI_v4.pdf}\n    \\caption{A visual illustration example of DCS. The numbers on the contour lines are DCSs calculated with $\\beta=0.5$.} % \\yang{make the points larger and font bigger}\n    \\label{fig:dcs}\n\\end{wrapfigure}\n\nwhere $D(\\cdot)$ is a user-defined distance function. Here we use Euclidean distance with an adjustable trade-off weighting parameter $\\beta$. A visualization of DCS on an AP-MP graph with $\\beta=0.5$ can be seen in \\cref{fig:dcs}. %\\yang{add a sketch figure to illustrate the distance}\\tianyuan{DONE, but this figure takes up too much space.}). \nA point closer to the bottom-right corner of an AP-MP graph has a higher DCS score indicating a better defense capability, consistent with intuition. $\\beta=0.5$ is used in our experiments. %Also, $\\hat{m}_{j}^{*}$ denotes the \\textit{ideal MP} defined as the MP of a vanilla VFL without defense, representing no accuracy loss on the main task and $\\hat{a}_{j}^{*}$ denotes the \\textit{ideal AP} defined as the worst AP for each attack (each attack type) as defined above in \\cref{subsubsec:ap_mp} and \\cref{tab:attack_performance_definition}.\n\n\\textbf{Type-level Defense Capability Score (T-DCS).} T-DCS is the DCS score averaged by attack type. %proposed to evaluate the capability of a defense method on a certain type of attack. %we considered in \\cref{subsec:attacks} rather than on a single attack. \nTreating all $I_j$ attacks of the same attack type $j$ as equally important, we average DCS for each attack $i$ to get T-DCS for attack type $j$:% $j\\in \\mathcal{A}$:\n\\begin{equation} \\label{eq:t-dcs}\n \\setlength\\abovedisplayskip{0.05cm}\n \\setlength\\belowdisplayskip{0.05cm}\n    \\text{T-DCS}_j = \\frac{1}{I_j} \\sum_{i=1}^{I_j} \\text{DCS}_i .\n\\end{equation}\n\n\\textbf{Comprehensive Defense Capability Score (C-DCS).} C-DCS is a comprehensive assessment of the capability of a defense strategy with respect to all kinds of attacks and is a weighted average of T-DCS as shown in \\cref{eq:c-dcs}:\n\\begin{equation} \\label{eq:c-dcs}\n \\setlength\\abovedisplayskip{0.05cm}\n \\setlength\\belowdisplayskip{0.05cm}\n    \\text{C-DCS} = \\sum_{j\\in \\mathcal{A}} w_j \\text{T-DCS}_j, \\,\\, \\text{with} \\sum_{j \\in \\mathcal{A}} w_j = 1.0 \\, .\n\\end{equation}\nWeights $\\{w_j\\}_{j \\in \\mathcal{A}}$ can be tailored to user preference. In our experiments, we simply use an unbiased weight $w_j=\\frac{1}{|\\mathcal{A}|}$ for each attack type $j \\in \\mathcal{A}=\\{\\text{LI}, \\text{FR}, \\text{TB}, \\text{NTB}\\}$.\n\n\n\n\n\n\n\n\\section{{Evaluation Results}\n\n\n\n\n\n\\subsection{VFL Main Task Performance} \\label{subsec:MP}\nWe first comprehensively evaluate the impact of various settings on the performance of VFL.\n\n\\textbf{Model Partition.}\nThe splitVFL setting yields a comparable or slightly higher MP  compared to aggVFL on most datasets, due to the additional trainable layer serving as global model, evidenced by results from \\cref{tab:NN_MP,tab:4party_MP,tab:real_world_dataset_MP}.\n\n\\textbf{Communication Protocols.} %We compare $5$ different kinds of communication protocols in VFL. \nAs shown in \\cref{tab:NN_MP} and \\cref{tab:communication_MP}, compared to FedSGD, FedBCD and CELU-VFL exhibit comparable MP across all datasets with fewer communication rounds, supporting their efficacy in reducing communication overhead. Quantize and Top-k compress the transmitted data and successfully reduce the communication cost per round, but may result in an increase in communication rounds. %CELU-VFL also effectively reduces the \\#Rounds compared to FedSGD, and is slightly less than that of FedBCD\n\n\\textbf{Encryption.} %{\\color{red}{We compare the MP and execution time with and without HE protection using Paillier Encryption in \nFor tree-based VFL, we consider two models with and without Paillier Encryption using 512-bit key size in \\cref{tab:tree_MP}. Note that XGBoost with Paillier Encryption is equivalent to SecureBoost~\\citep{cheng2021secureboost}. Although MP values are consistent regardless of encryption, the execution time experiences a notable increase of 3 to 20 times when encryption is applied due to the additional encryption and decryption process. \n\n\\textbf{Number of Participants.}\nImpact of number of participants are shown in \\cref{tab:4party_MP}. \nA slightly lower MP is achieved using fewer communication rounds as the number of participants increases, demonstrating the increasing challenges brought by multi-party collaboration.\n\n\\textbf{Model Architectures.}\n% {\\color{red}{tree-based VFL performs better on Credit dataset, while NN-based VFL performs better on Nursery dataset.\n\n\\textbf{Real world datasets.}\nAdditional results on Criteo~\\citep{guo2017deepfm_Criteo}, Avazu~\\citep{qu2018product_Avazu}, Cora~\\citep{mccallum2000automating_Cora} and News20~\\citep{lang1995News20} datasets using domain specific models (e.g. Wide\\&Deep Model~\\citep{cheng2016wide} for Criteo and Avazu, GNN for Cora) are provided in \\cref{tab:real_world_dataset_MP}\n, as they are considered for typical VFL applications, such as in recommendation problems. %click through rate prediction in advertising, node classification in citation network and classification of news data\n\\begin{table}[!tb]\n\\caption{MP under $4$ different settings of NN-based VFL. $Q=5$ when FedBCD is applied. In \"\\#Rounds\" column, the first and second numbers are the communication rounds needed to reach the specified MP for FedSGD and FedBCD respectively.}\n\\label{tab:NN_MP}\n\\resizebox{0.998\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c||c|c|c||c|c|c}\n    \\toprule\n    Dataset & aggVFL, FedSGD & aggVFL, FedBCD & \\#Rounds & splitVFL, FedSGD & splitVFL, FedBCD & \\#Rounds \\\\\n    \\midrule\n    MNIST & 0.972$\\pm$0.001 & 0.971 $\\pm$0.001 & 150 / 113 & 0.973$\\pm$0.001 & \\textbf{0.974$\\pm$0.001} & 180 / 143 \\\\\n    NUSWIDE & 0.887$\\pm$0.001 & 0.882$\\pm$0.001 & 60 / 26 & \\textbf{0.888$\\pm$0.001} & 0.884$\\pm$0.001 & 60 / 29 \\\\ \n    Breast Cancer & 0.914$\\pm$0.033 & 0.919$\\pm$0.029 & 5 / 3 & \\textbf{0.925$\\pm$0.028} & 0.907$\\pm$0.045 & 5 / 4 \\\\\n    Diabetes & 0.755$\\pm$0.043 & 0.736$\\pm$0.021 & 15 / 13 & \\textbf{0.766$\\pm$0.024} & 0.746$\\pm$0.039 & 15 / 11 \\\\\n    Adult Income & 0.839$\\pm$0.006 & 0.841$\\pm$0.005 & 17 / 15 & 0.842$\\pm$0.004 & \\textbf{0.842$\\pm$0.005} & 30 / 13 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\n\\begin{table}[!tb]\n\\caption{MP under 2-party VFL verses MP under 4-party VFL  under $4$ different settings of NN-based VFL using FedSGD communication protocol. \"\\#Rounds\" has the same meaning as in \\cref{tab:NN_MP}.}\n\\label{tab:4party_MP}\n\\resizebox{0.998\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c|c||c|c||c|c}\n    \\toprule\n    Dataset  & ~ & aggVFL, 2-party & aggVFL, 4-party & splitVFL, 2-party & splitVFL, 4-party\\\\\n    \\midrule\n    \\multirow{2}{*}{CIFAR10} & MP & 0.790$\\pm$0.003 & 0.747$\\pm$0.003 & \\textbf{0.798$\\pm$0.010} & 0.762$\\pm$0.003 \\\\\n    ~ & \\#Rounds & 244$\\pm$16 & 205$\\pm$12 & 238$\\pm$14 & 173$\\pm$3 \\\\\n    \\midrule\n    \\multirow{2}{*}{CIFAR100} & MP & \\textbf{0.454$\\pm$0.006} & 0.417$\\pm$0.008 & 0.423$\\pm$0.005 & 0.382$\\pm$0.004 \\\\\n    ~ & \\#Rounds & 130$\\pm$11 & 124$\\pm$2 & 125$\\pm$2 & 100$\\pm$1 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\\begin{table}[!tb]\n\\caption{MP and execution time under 2 different types of tree-based VFL.} %\\yang{too many digits, round to fewer, especially for time}\n\\label{tab:tree_MP}\n\\resizebox{0.99\\linewidth}{!}{\n\\centering\n\\begin{tabular}{@{}c|c|c|c|c|c@{}}\n\\toprule\nDataset &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Random Forest \\\\ w/o Encryption\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}XGBoost\\\\ w/o Encryption\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Random Forest \\\\ w/ Encryption\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}XGBoost\\\\ w/ Encryption\\\\ (a.k.a. SecureBoost)\\end{tabular} \\\\ \\midrule\n\\multirow{2}{*}{Credit} &\n  MP &\n  0.816$\\pm$0.005 &\n  0.816$\\pm$0.004 &\n  0.816$\\pm$0.005 &\n  0.816$\\pm$0.004 \\\\\n &\n  \\multicolumn{1}{c|}{Exec.Time [s]} &\n  \\multicolumn{1}{c|}{138$\\pm$4} &\n  \\multicolumn{1}{c|}{366$\\pm$16} &\n  \\multicolumn{1}{c|}{410$\\pm$10} &\n  \\multicolumn{1}{c}{881$\\pm$6} \\\\ \n\\hline\n\\\\[-1em]\n\\multirow{2}{*}{Nursery} &\n  MP &\n  0.884$\\pm$0.010 &\n  0.890$\\pm$0.011 &\n  0.884$\\pm$0.010 &\n  0.890$\\pm$0.011 \\\\\n &\n  \\multicolumn{1}{c|}{Exec.Time [s]} &\n  \\multicolumn{1}{c|}{29$\\pm$2} &\n  \\multicolumn{1}{c|}{69$\\pm$4} &\n  \\multicolumn{1}{c|}{243$\\pm$5} &\n  \\multicolumn{1}{c}{1194$\\pm$21} \\\\ \n\\bottomrule\n\\end{tabular}\n}\n\\end{table}\n\n\\begin{table}[!tb]\n\\caption{\nMP, communication rounds (\\#Rounds), amount of information exchanged per round (Amount) under different communication protocols of NN-based VFL under aggVFL setting. %$Q=5$ when FedBCD and CELU-VFL are used, otherwise $Q=1$. For Quantize, $b=16$ while for Top-k, top $90\\%$ of elements are kept in forward local model prediction.\n'Total' column is the total amount that equals to \\#Rounds$\\times$Amount.}\n\\label{tab:communication_MP}\n\\resizebox{0.99\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c|cccc|cccc}\n    \\toprule\n        ~ & \\multicolumn{4}{c|}{MNIST} & \\multicolumn{4}{c}{NUSWIDE} \\\\\n        \\cline{2-9}\n        \\\\[-1em]\n        ~ & MP & \\#Rounds & Amount (MB) & Total (MB) & MP & \\#Rounds & Amount (MB) & Total (MB)\\\\ \n    \\midrule\n        FedSGD & \\textbf{0.972$\\pm$0.001} & 150 & 0.156 & 23.438 & \\textbf{0.887$\\pm$0.001} & 60 & 0.039 & 2.344 \\\\\n        FedBCD & 0.971$\\pm$0.001 & 113 & 0.156 & 17.656 & 0.882$\\pm$0.001 & 26 & 0.039 & 1.016 \\\\\n        Quantize & 0.959$\\pm$0.006 & 161 & 0.117 & 18.867 & 0.881$\\pm$0.002 & 94 & 0.029 & 2.754 \\\\\n        Top-k & 0.968$\\pm$0.001 & 150 & 0.148 & 22.266 & 0.887$\\pm$0.001 & 60 & 0.037 & 2.227 \\\\\n        CELU-VFL & 0.971$\\pm$0.002 & 105 & 0.156 & 16.406 & 0.880$\\pm$0.001 & 25 & 0.039 & 0.977 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\n\\begin{table}[!tb]\n\\caption{Comparison of aggVFL and splitVFL on MP, \\#Rounds, Amount, total communication cost, Exec.Time for reaching specified MP with $4$ real-world datasets of NN-based VFL with FedSGD communication protocol.}\n\\label{tab:real_world_dataset_MP}\n\\resizebox{0.99\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c|ccccc|ccccc}\n    \\toprule\n        \\multirow{2}{*}{Dataset} & \\multicolumn{5}{c|}{aggVFL} & \\multicolumn{5}{c}{splitVFL} \\\\\n        \\cline{2-11}\n        \\\\[-1em]\n        ~ & MP & \\#Rounds & \\shortstack{Amount\\\\(MB)} & \\shortstack{Total\\\\(MB)} & Exec.Time [s] & MP & \\#Rounds & \\shortstack{Amount\\\\(MB)} & \\shortstack{Total\\\\(MB)} & Exec.Time [s]\\\\\n    \\midrule\n        Criteo & 0.715$\\pm$0.053& 2 & 0.125 & 0.250 & 0.190$\\pm$0.132 & 0.744$\\pm$0.001 & 3 & 0.125 & 0.375 & 0.234$\\pm$0.126\\\\\n        Avazu & 0.832$\\pm$0.001 & 5 & 0.125 & 0.625 & 0.517$\\pm$0.185 & 0.832$\\pm$0.001 & 9 & 0.125 & 1.125 & 1.203$\\pm$1.516\\\\ \n        Cora & 0.721$\\pm$0.004 & 11 & 0.145 & 1.591 & 0.205$\\pm$0.085 & 0.724$\\pm$0.012 & 13 & 0.145 & 1.880 & 0.270$\\pm$0.082 \\\\\n        News20-S5 & 0.882$\\pm$0.014 & 57 & 0.005 & 0.278 & 0.430$\\pm$0.076 & 0.893$\\pm$0.013 & 61 & 0.005 & 0.298 & 0.613$\\pm$0.269 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\n\\subsection{Attack and Defense Performance} \\label{subsec:attack_defense_performance}\n\n\\begin{figure}[!htb]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/cifar10/ALL_agg.pdf}\n  \\caption{MPs and APs for different attacks under defenses [CIFAR10 dataset, aggVFL, FedSGD]}\n  \\label{fig:cifar10_MPAP}\n\\end{figure}\n\nWe demonstrate attack and defense results of VFL on the AP-MP graph for each attack on MNIST, CIFAR10 and NUSWIDE datasets under aggVFL setting using FedSGD protocol in \\cref{fig:mnist_MPAP,fig:cifar10_MPAP,fig:nuswide_MPAP}.\nEach point in the figure represents a $(\\text{MP},\\text{AP})$ pair with the size of markers representing the relative magnitude of the corresponding defense hyper-parameter listed in \\cref{tab:defense_param}.\nNote that although we try to provide comprehensive evaluation for various defenses, we do not force defense onto attacks, meaning that if %we evaluate defense methods only on the specific attacks they were designed for. In other words, if \na defense mechanism is designed for mitigating label inference attacks only, we do not assess its effectiveness against FR attacks or backdoor attacks. \n\nWe further rank all the defenses of different hyper-parameters based on their C-DCS. Due to space limitation, we show representative results for NUSWIDE dataset in \\cref{tab:tiny_nuswide_dcs_ranking}.%Consequently, our experimental evaluations do not include assessments of GPer, CAE, and DCAE against FR attacks and NTB attacks, while GS is excluded from evaluations involving FR attacks.}}\n\n\n\\textbf{Attacks pose great threat to VFL.} Comparing the black squares illustrating the MP and AP of the attack against a VFL system without any defense in the sub-figures, we can observe that DS, DLI, BLI and TBM attacks are strong attacks with AP higher than $0.97$, while MF attacks are quite weak with AP below $0.1$.\n\n\\textbf{Defenses exhibit trade-offs between MP and AP.} For most of the attacks and defenses, we can observe an apparent trade-off between MP and AP, i.e. a lower AP is often gained with increasing harm of MP as defense strength grows, which can be controlled by adjusting defense hyper-parameters. \nAn increase of noise level in DP-G and DP-L, sparsification rate in GS, regularization hyper-parameter $\\alpha_d$ in dCor, confusional strength $\\lambda_2$ in CAE and DCAE, regularization hyper-parameter $\\lambda$ in MID or a decrease of DP budget $\\epsilon$ in GPer will lead to lower MP and AP.\n\n\\textbf{DCS rankings are consistent across various datasets and settings.} As shown in \\cref{tab:mnist_dcs_ranking,tab:nuswide_dcs_ranking,tab:cifar10_dcs_ranking}, the results of the C-DCS rankings are generally consistent across all $3$ datasets. %% namely MNIST, CIAFR10 and NUSWIDE, under the same aggVFL setting with FedSGD communication. Also, \nAs summarised in \\cref{fig:dcs_ranking_mean_std}, these results demonstrate the robustness of the proposed DCS metrics, as well as the stableness of relative performance of different defense methods.Note that, T-DCS$_{\\text{FR}}$ values are much lower than the T-DCS of other types, indicating that FR attacks are harder to defend than other attacks, which are consistent with human observation (see \\cref{fig:visualization_mnist_tbm} in \\cref{subsec:visualization_tbm}). %and more effective defense methods against FR attacks are in need.\n\n\\textbf{MID, L-DP and G-DP are effective on a wide spectrum of attacks.} MID demonstrates its capability of achieving a relatively lower AP while maintaining a higher MP compared to most other defenses as shown in \\cref{fig:mnist_MPAP,fig:nuswide_MPAP,fig:cifar10_MPAP} and  \\cref{tab:mnist_dcs_ranking,tab:nuswide_dcs_ranking,tab:cifar10_dcs_ranking}; L-DP and G-DP are also generally effective under most attacks with above average T-DCS and C-DCS; DCAE is effective in defending against LI attacks; %\\textbf{CAE is effective only against LI attacks that utilize merely the information of the current sample}%\\textbf{CAE consistently performs well across when defending against LI attacks that utilize merely the information of the current sample} by disguising label directly; \nGS demonstrates strong defense ability for most of the LI attacks but performs less than satisfactory on LRB attacks; GPer performs similar to DP-G and DP-L in defending against label related attacks; %as it targets at defending LI attacks and guarantees label-DP \\yang{what about other attac}ks?} \\tianyuan{Not evaluated... Do we need to include the explanation?}\ndCor is less effective in limiting AP under NTB attacks but is largely effective against PMC and AMC attacks as shown in \\cref{fig:mnist_MPAP,fig:nuswide_MPAP,fig:cifar10_MPAP}. \n\n\n\\begin{wrapfigure}{l}{0.55\\textwidth}\n  \\centering\n  \\includegraphics[width=0.99\\linewidth]{figures/mnist/mnist_rank_beta.pdf}\n  \\caption{Change of C-DCS ranking with the change of $\\beta$. [MNIST dataset, aggVFL, FedSGD]}\n  \\label{fig:change_beta_dcs_mnist}\n\\end{wrapfigure}\n\\textbf{Change in $\\beta$ does not significantly impact the C-DCS ranking.} $\\beta$ in \\cref{eq:dcs} represents users' trade-off preference on AP and MP when evaluating defenses, and can be adjusted. Here we use $\\beta =0.5$ for our main results. %If the user wants to place more attention on maintaining a high MP, hyper-parameter $\\beta$ in \\cref{eq:dcs} can be adjusted to a value larger than $0.5$. \n\\cref{fig:change_beta_dcs_mnist,fig:change_beta_dcs_cifar10,fig:change_beta_dcs_nuswide} show the change of the ranking results with the change of $\\beta$. Overall the relative rankings are not significantly impacted by $\\beta$, demonstrating the stableness of the comparison results among various defenses. As $\\beta$ grows to large values, e.g. $\\geq 0.9$, the metric places overly strong weight on MP, resulting in more variations on the rankings. Specifically, dCor ranks higher with the increase of $\\beta$ thanks to its better MP preservation at the cost of a weaker AP limitation.\n\\begin{table}[!tb]\n\\caption{T-DCS and C-DCS for All Defenses [NUSWIDE dataset, aggVFL, FedSGD]}\n\\label{tab:tiny_nuswide_dcs_ranking}\n\\resizebox{0.998\\linewidth}{!}{\n  \\centering\n   \\begin{tabular}{cc|cccccc|c}\n    \\toprule\n    \\textbf{\\makecell{Defense\\\\Name}} & \\textbf{\\makecell{Defense\\\\Parameter}} & \\bm{$T\\text{-}DCS_{LI_{2}}$} &\n    \\bm{$T\\text{-}DCS_{LI_{5}}$} & \\bm{$T\\text{-}DCS_{LI}$} & \\bm{$T\\text{-}DCS_{FR}$} & \\bm{$T\\text{-}DCS_{TB}$} & \\bm{$T\\text{-}DCS_{NTB}$} & \\bm{$C\\text{-}DCS$} \\\\ \n    \\midrule \n    MID  & 10000  & 0.7358 & 0.8559 & \\textbf{0.8159} & 0.5833 & \\textbf{0.7333} & 0.8707 & 0.7508  \\\\\n    MID  & 1.0    & 0.7476 & 0.8472 & 0.8140 & 0.5833 & 0.7331 & 0.8700 & 0.7501  \\\\\n    MID  & 100    & 0.7320 & 0.8536 & 0.8130 & 0.5833 & 0.7326 & \\textbf{0.8711} & 0.7500  \\\\\n    G-DP & 0.1    & 0.7375 & 0.8262 & 0.7966 & 0.5863 & 0.7282 & 0.8675 & 0.7447  \\\\\n    L-DP & 0.1    & 0.7389 & 0.8177 & 0.7915 & 0.5863 & 0.7258 & 0.8603 & 0.7410  \\\\\n    MID  & 0.1    & 0.7516 & 0.8259 & 0.8011 & 0.5833 & 0.7172 & 0.8563 & 0.7395  \\\\\n    MID  & 0.01   & 0.7280 & 0.8092 & 0.7822 & 0.5844 & 0.7151 & 0.8627 & 0.7361  \\\\\n    dCor & 0.3    & \\textbf{0.7641} & 0.8411 & 0.8155 & 0.5834 & 0.7289 & 0.8051 & 0.7332  \\\\\n    dCor & 0.0001 & 0.6496 & 0.6340 & 0.6392 & \\textbf{0.5864} & 0.6307 & 0.8287 & 0.6712 \\\\\n    \\hline\n    \\\\[-1em]\n    GS   & 99.0   & 0.7404 & 0.8060 & 0.7841 & -      & 0.6415 & 0.8408 & -      \\\\\n    CAE  & 1.0    & 0.6863 & 0.7822 & 0.7502 & -      & 0.6830  & -           \\\\\n    DCAE & 0.0    & 0.6669 & \\textbf{0.8660} & 0.7996 & -      & 0.6816 & -      & -       \\\\\n    GPer & 0.01   & 0.7386 & 0.8412 & 0.8070 & -      & 0.7193 & -      & -       \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\\textbf{splitVFL is less vulnerable to attacks than aggVFL.}\nUsing DCS metrics, we directly compare all the aforementioned attacks and defenses under aggVFL and splitVFL settings to understand the impact of changing the model partition strategy on VFL's vulnerability against attacks. We mainly use the DCS gap, defined as $\\text{DCS}^{\\text{splitVFL}}-\\text{DCS}^{\\text{aggVFL}}$ for each attack-defense point. \\cref{fig:mnist_splitVFL_DCS_gap,fig:nuswide_split_DCS_gap} \\begin{wrapfigure}{l}{0.55\\textwidth}\n  \\centering\n  \\includegraphics[width=0.99\\linewidth]{figures/split_agg_compare/splitagg_hist_total.pdf}\n  \\caption{DCS gap Distribution, y-axis represents density [MNIST dataset, splitVFL/aggVFL, FedSGD]}   \n  \\label{fig:mnist_splitVFL_DCS_hist}           \n\\end{wrapfigure}\n \n\\cref{fig:mnist_splitVFL_DCS_hist,fig:nuswide_split_hist} displays the distribution of the DCS gaps depicted in \\cref{fig:mnist_splitVFL_DCS_gap,fig:nuswide_split_DCS_gap} respectively.\nAs all the black square points in \\cref{fig:mnist_splitVFL_DCS_gap} appear above or close to the red horizontal line at a value of $0.0$ (see also the blue histograms that appear mostly at the right of the vertical line at a value of $0.0$ in \\cref{fig:mnist_splitVFL_DCS_hist}), we can conclude that splitVFL is less vulnerable to attacks than aggVFL when no defense is applied. In addition, splitVFL has an overall positive effect on boosting defense performance against attacks as well, as most of the DCS gap is positive in the last subplot of \\cref{fig:mnist_splitVFL_DCS_gap} when no attack is applied. \n\n\\begin{wrapfigure}{l}{0.55\\textwidth}\n      \\centering\n      \\includegraphics[width=0.99\\linewidth]{figures/bcd_compare/BCD_hist_total.pdf}\n    \\caption{DCS gap Distribution, y-axis represents density [MNIST dataset, aggVFL, FedBCD/FedSGD]}\n  \\label{fig:mnist_FedBCD_DCS_hist}           \n\\end{wrapfigure}\n\n\\textbf{FedBCD is less vulnerable to attacks than FedSGD.} In addition, we compare DCS gap under FedSGD setting and FedBCD with $Q=5$ to assess the impact of different communication protocols on model's vulnerability to attacks. DCS gap is defined as $\\text{DCS}^{\\text{FedBCD}}-\\text{DCS}^{\\text{FedSGD}}$ for each attack-defense point. \n\nAs shown in \\cref{fig:mnist_FedBCD_DCS_hist}, the blue histograms generally appear on the right of the vertical line of value $0.0$, indicating that a system with FedBCD protocol is less vulnerable to attacks when no defense method is applied. In addition, a system with FedBCD also has an overall positive effect on boosting defense performance against FR and NTB attacks. This is evidenced by the fact that that the majority of DCS gaps are positive for FR and NTB attacks as shown in \\cref{fig:mnist_FedBCD_DCS_hist}.\nSimilar conclusions can be drawn from \\cref{fig:nuswide_FedBCD_hist,fig:nuswide_FedBCD_DCS_gap}.\n\n\n\\section{Conclusions and Limitations}\n\n\nIn this work, we introduce a light-weight VFL framework \\verb|VFLAIR| that implements basic VFL training and evaluation flow under multiple model partition, model architectures,communication protocols and attacks and defenses algorithms using datasets of different modality. We also introduce unified evaluation metrics and benchmark model utility, communication and computation efficiency, and defense performance under various VFL settings, which sheds lights on choosing partition, communication and defense techniques in practical deployment. \nCurrently, the library has limited implementations on cryptographic techniques. Combination of non-cryptograhic and cryptographic techniques would be an interesting next step and we plan to add more advanced privacy-preserving and communication-efficient methods to our library. %encryption implementation, since is not implemented for NN-based VFL, we will add this part in the near future.}}\n\n\n\n\\section{Reproducibility Statement}\n\n Our code is also available at\n\n\n\n\n", "DeepReviewer": "\n## Summary:\n\nThis paper introduces VFLAIR, a lightweight and extensible framework for Vertical Federated Learning (VFL), designed to facilitate research and practical deployment. The core contribution of this work lies in the creation of a comprehensive benchmark for VFL, encompassing a wide range of models, datasets, communication protocols, and, crucially, various attack and defense strategies. The authors implement 29 local model architectures, 6 global models, and 5 communication protocols, alongside 11 different attack methods and 8 defense mechanisms. The framework supports 13 datasets, including image, text, and tabular data, allowing for a diverse set of experiments. The authors benchmark the performance of these components under different communication and model partition settings, aiming to provide insights and recommendations on the choice of defense strategies for different practical VFL deployment scenarios. The empirical findings include evaluations of model utility, defense performance, and the computational overhead of different configurations. The paper introduces new evaluation metrics, such as Defense Capability Score (DCS), Type-level DCS (T-DCS), and Comprehensive DCS (C-DCS), to assess the effectiveness of defense methods. The authors also explore the trade-offs between model utility and defense effectiveness, providing concrete recommendations for practitioners. The paper's significance lies in its attempt to provide a unified platform for VFL research, addressing the need for a comprehensive benchmark that includes a wide range of methods and scenarios. However, the paper's primary focus is on implementation and empirical evaluation, with limited theoretical contributions. The authors acknowledge the limitations of the current implementation, particularly in terms of cryptographic techniques, and suggest future directions for improvement. Overall, the paper presents a valuable contribution to the VFL community by providing a practical framework and benchmark, but it also highlights areas where further research is needed.\n\n\n## Soundness:\n\n3.0\n\n\n## Presentation:\n\n3.0\n\n\n## Contribution:\n\n3.0\n\n\n## Strengths:\n\nI find several strengths in this paper, primarily centered around its contribution to the practical application and benchmarking of Vertical Federated Learning (VFL). The most significant strength is the development of VFLAIR, a lightweight and extensible VFL framework. This framework, as the authors describe, includes a wide array of VFL components, such as 29 local model architectures, 6 global models, and 5 communication protocols. This comprehensive collection of components, coupled with the support for 13 diverse datasets, makes VFLAIR a valuable resource for researchers and practitioners alike. The inclusion of 11 attack methods and 8 defense mechanisms further enhances the framework's utility as a benchmarking tool. The authors have clearly put considerable effort into implementing state-of-the-art VFL methods, which is a non-trivial undertaking. The empirical evaluation is another strong point. The authors conduct extensive experiments, benchmarking the performance of various VFL settings, including model partitioning, communication protocols, and defense strategies. The introduction of new evaluation metrics, such as DCS, T-DCS, and C-DCS, is a positive step towards a more nuanced understanding of defense effectiveness. These metrics allow for a more comprehensive assessment of the trade-offs between model utility and defense performance. The paper also provides concrete recommendations on the choice of defense strategies for different practical VFL deployment scenarios, which is particularly helpful for practitioners. The authors have clearly identified a need for a comprehensive VFL benchmark and have made a commendable effort to address this gap. The framework's extensibility is also a notable strength, as it allows for future additions and modifications, ensuring its continued relevance to the VFL community. The paper's focus on practical deployment scenarios is also a positive aspect, as it highlights the real-world applicability of the research. The authors have successfully created a valuable tool that can facilitate further research and development in the field of VFL.\n\n\n## Weaknesses:\n\nDespite the strengths of this paper, I have identified several weaknesses that warrant careful consideration. First, the paper's novelty is limited. While the implementation of a comprehensive VFL framework is a valuable contribution, the paper does not introduce any novel defense techniques. As I've verified, the authors explicitly state that they implement and benchmark existing methods, rather than proposing new ones. This focus on implementation and empirical evaluation, while useful, positions the paper more as a system paper than a theoretical contribution. The lack of detailed implementation specifics further reinforces this perception. Second, the paper lacks a significant theoretical contribution. The focus is primarily on empirical evaluation and benchmarking, with no in-depth theoretical analysis or proofs. While the introduction of new evaluation metrics is a contribution, it is still within the realm of empirical analysis. The paper does not delve into the theoretical underpinnings of the observed empirical results, such as the relationship between the properties of the defense mechanisms and their ability to mitigate specific types of attacks. Third, the paper does not provide a detailed analysis of the computational overhead of the proposed defense methods. While the authors include 'Execution Time' as a metric in some experiments, this is primarily for comparing the overall training time with and without encryption for tree-based models. There is no detailed breakdown of the computational overhead introduced by each individual defense method. This is a significant omission, as the computational cost of defense methods is a crucial factor to consider in practical deployments, especially in resource-constrained environments. Fourth, the paper does not explore the potential vulnerabilities of the proposed defense methods to more advanced attacks. The evaluation is limited to a set of 11 specific attacks, which are established attacks but not necessarily adaptive attacks designed to circumvent these specific defenses. The paper does not consider the robustness of these defenses against sophisticated adversaries who might develop new attack strategies to target the weaknesses of the evaluated defenses. Fifth, the evaluation of the framework is limited to a relatively small number of datasets and model architectures. While the paper includes 13 datasets and 29 local model architectures, a wider range of datasets and models would further demonstrate the framework's generalizability. The current selection, while diverse, could be expanded to include more complex data modalities and tasks. Finally, the paper lacks a detailed discussion of the limitations of the proposed framework and potential future research directions. While the 'Conclusions and Limitations' section briefly mentions some limitations, it does not provide a comprehensive analysis of the challenges of deploying VFL in real-world scenarios, such as the need for robust security mechanisms and the difficulty of handling heterogeneous data distributions. The paper also does not thoroughly outline potential future research directions, such as the development of more efficient and robust defense methods or the exploration of new VFL architectures. These limitations, taken together, suggest that while the paper makes a valuable contribution in terms of a practical framework, it falls short in terms of theoretical depth, computational analysis, and exploration of advanced attack scenarios. My confidence in these identified weaknesses is high, as they are directly supported by the paper's content and lack of specific details in the areas mentioned.\n\n\n## Suggestions:\n\nBased on the identified weaknesses, I recommend several concrete improvements to enhance the paper's contribution. First, the authors should conduct a more in-depth analysis of the computational overhead associated with the proposed VFL framework, particularly the defense methods. This analysis should include a breakdown of the time spent on different stages of the training process, such as forward and backward propagation, communication, and encryption/decryption. This analysis should be conducted for various model architectures and datasets to understand how these factors influence the overall training time. Furthermore, the impact of different defense methods on the computational cost should be quantified. For example, the authors could measure the increase in training time and communication costs when using techniques like gradient perturbation or homomorphic encryption. This detailed analysis would allow practitioners to make informed decisions about the trade-offs between security and efficiency when deploying the framework in real-world scenarios. It would also be beneficial to explore optimization techniques to mitigate the computational overhead of the defense methods. Second, the authors should expand the evaluation of the framework to include a wider range of datasets and model architectures. While the current evaluation includes 13 datasets and 29 local model architectures, it would be beneficial to include datasets with varying characteristics, such as different data modalities (e.g., images, text, time series) and different data sizes. This would help to assess the generalizability of the framework and identify potential limitations. Similarly, the evaluation should include a wider range of model architectures, such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers, to demonstrate the framework's versatility. Furthermore, the authors should consider evaluating the framework's performance on more complex tasks, such as multi-label classification or regression problems. This would provide a more comprehensive understanding of the framework's capabilities and limitations. Third, the authors should explore the potential vulnerabilities of the proposed defense methods to more advanced attacks. This could involve considering adaptive attacks that are specifically designed to circumvent the defenses. The authors should also consider exploring the use of adversarial training techniques to improve the robustness of the defenses against such advanced attacks. This could involve training the models on adversarial examples generated by more sophisticated attack methods, or using techniques such as robust optimization to minimize the impact of adversarial perturbations. Furthermore, the authors should investigate the potential for attacks that exploit the communication protocols used in VFL, such as man-in-the-middle attacks or replay attacks. Fourth, the authors should include a more detailed discussion of the limitations of the proposed framework and potential future research directions. The authors should acknowledge the challenges of deploying VFL in real-world scenarios, such as the need for robust security mechanisms and the difficulty of handling heterogeneous data distributions. They should also discuss the limitations of the current evaluation, such as the limited number of datasets and model architectures. Furthermore, the authors should outline potential future research directions, such as the development of more efficient and robust defense methods, the exploration of new VFL architectures, and the application of VFL to new domains. This discussion would provide valuable insights for researchers in the field and help to guide future research efforts. Finally, the authors should consider adding more implementation details to the paper, such as code snippets or architectural diagrams of the framework itself. This would make the paper more useful for researchers who want to use the framework and would also help to clarify the technical contributions of the work. These suggestions, if implemented, would significantly strengthen the paper and make it a more valuable contribution to the VFL community.\n\n\n## Questions:\n\nBased on my analysis, I have several questions that I believe are crucial for a deeper understanding of the paper's contributions and limitations. First, how do the proposed defense methods perform against more advanced attacks that are specifically designed to target these defenses? The current evaluation focuses on a set of existing attacks, but it does not consider the potential for adaptive adversaries to circumvent these defenses. Understanding the robustness of the defenses against such advanced attacks is crucial for assessing their practical utility. Second, what is the computational overhead of the proposed defense methods, and how does it impact the scalability of VFL systems? The paper does not provide a detailed analysis of the computational cost of each defense method, which is a critical factor to consider in practical deployments, especially in resource-constrained environments. A detailed breakdown of the time spent on different stages of the training process, such as forward and backward propagation, communication, and encryption/decryption, would be beneficial. Third, how do the proposed defense methods affect the convergence rate and stability of VFL training? The paper does not explore the impact of the defense methods on the training dynamics, which is an important aspect to consider. Understanding how these methods affect the convergence rate and stability of the training process is crucial for ensuring the reliability of the framework. Fourth, how do the proposed defense methods perform in real-world VFL deployment scenarios, and what are the practical challenges of implementing these defenses in such settings? The paper focuses on benchmarking in a controlled environment, but it does not address the practical challenges of deploying these defenses in real-world scenarios, such as handling heterogeneous data distributions and dealing with communication constraints. Fifth, what are the potential limitations of the proposed framework, and what are the future research directions that the authors envision? The paper briefly mentions some limitations, but a more detailed discussion of the challenges of deploying VFL in real-world scenarios and the potential future research directions would be valuable. Finally, how does the proposed framework compare to other existing VFL frameworks in terms of performance, scalability, and security? A comparative analysis with other existing frameworks would provide a better understanding of the strengths and weaknesses of VFLAIR. These questions, I believe, are essential for a comprehensive understanding of the paper's contributions and limitations and would help to guide future research in this area.\n\n\n## Rating:\n\n6.0\n\n\n## Confidence:\n\n3.5\n\n\n## Decision:\n\nAccept", "other": "\n## Summary:\n\nThis paper introduces VFLAIR, a novel framework designed for research in Vertical Federated Learning (VFL). The authors aim to address the gap between academic research and industrial applications by providing a lightweight, modular, and extensible platform. VFLAIR supports a wide range of VFL algorithms, including various model architectures (linear regression, tree-based models, and neural networks), communication protocols (FedSGD, FedBCD, CELU-VFL, Top-k, and Quantize), and defense methods against data inference and backdoor attacks. The framework is designed to facilitate research by providing a standardized pipeline for VFL training and validation. A key contribution of this work is the extensive benchmarking of 11 different attacks and 8 defense strategies under diverse settings, which the authors use to derive insights and recommendations for practical VFL deployment. The empirical evaluation includes experiments on 13 datasets, assessing the impact of different model partition strategies, communication protocols, and defense mechanisms on model performance, communication efficiency, and robustness to attacks. The authors also introduce new evaluation metrics, such as Attack Performance (AP), Main Task Performance (MP), and Defense Capability Score (DCS), to quantify the effectiveness of different VFL configurations. The paper's findings highlight the trade-offs between model utility and security in VFL systems and provide practical guidance for selecting appropriate defense strategies. Overall, the paper presents a significant contribution to the VFL research community by providing a comprehensive framework and benchmark for evaluating the performance and security of VFL systems. However, as I will discuss in the weaknesses section, there are some areas where the paper could be improved to further enhance its impact and utility.\n\n\n## Soundness:\n\n2.75\n\n\n## Presentation:\n\n3.0\n\n\n## Contribution:\n\n2.75\n\n\n## Strengths:\n\nI find several aspects of this paper to be particularly strong. First, the development of VFLAIR as a lightweight, modular, and extensible framework is a significant contribution. The authors have clearly identified a need for a research-focused VFL platform that bridges the gap between academic research and industrial applications. By providing a standardized pipeline for VFL training and validation, VFLAIR has the potential to accelerate research in this area. The framework's support for a wide range of VFL algorithms, including various model architectures, communication protocols, and defense methods, makes it a versatile tool for researchers. Second, the extensive benchmarking of 11 attacks and 8 defenses under diverse settings is a valuable contribution. The authors have conducted a thorough evaluation of the performance and robustness of different VFL configurations, providing practical insights into the trade-offs between model utility and security. The use of multiple datasets and the exploration of different model partition strategies further enhance the generalizability of the findings. The introduction of new evaluation metrics, such as AP, MP, and DCS, is also a positive aspect, as these metrics provide a standardized way to quantify the effectiveness of different VFL configurations. Finally, the paper is well-written and easy to follow, with clear explanations of the methods and experimental results. The authors have made a commendable effort to make the paper accessible to a broad audience, which will undoubtedly increase its impact on the VFL research community. The inclusion of a comprehensive appendix further enhances the paper's value, providing additional details and context for the readers.\n\n\n## Weaknesses:\n\nDespite the strengths of this paper, I have identified several weaknesses that warrant further discussion. First, while the paper introduces VFLAIR as a lightweight and modular framework, it lacks a direct comparison with existing VFL frameworks, such as FATE. Although the introduction mentions that mainstream VFL projects like FATE are industrial-grade and not designed for keeping up with research advances, a quantitative comparison of system characteristics, such as system requirements, execution time, and resource usage, would be beneficial. This comparison would help potential users understand the trade-offs between VFLAIR and other frameworks and would further justify the need for a new framework. The absence of such a comparison makes it difficult to assess the practical advantages of VFLAIR over existing solutions. Second, the paper's evaluation of defense methods lacks a comprehensive analysis of their applicability to different attack types. While the paper benchmarks 11 attacks and 8 defenses, it does not explicitly state which defenses are applicable to which attacks in the main text, although this information is available in Table 1. This lack of clarity makes it difficult to understand the effectiveness of each defense method against specific attacks. For example, it is not immediately clear whether a defense designed for label inference attacks would be effective against feature reconstruction attacks. This is a crucial point for researchers who want to use VFLAIR to evaluate specific defense strategies. Third, the paper does not provide a detailed analysis of the computational overhead associated with the implemented defense methods. While the paper reports the execution time for tree-based models with and without encryption, it does not provide a similar analysis for the other defense methods. This is a significant limitation, as the computational cost of defense methods is a critical factor in practical VFL deployments. Researchers need to know the computational overhead of each defense method to make informed decisions about which methods to use in their specific contexts. Fourth, the paper's experimental evaluation is limited to a specific set of datasets and model architectures. While the paper includes experiments on 13 datasets and various model architectures, it does not explicitly test the framework's performance with larger models, such as ResNet or Inception, which are commonly used in computer vision tasks. This is a limitation, as the performance and scalability of VFL frameworks can vary significantly depending on the model architecture. The paper also does not include experiments on datasets with highly imbalanced classes or high-dimensional sparse features, which are common in real-world applications. This limits the generalizability of the findings and raises questions about the framework's robustness in diverse scenarios. Fifth, the paper does not address the issue of fairness in VFL. While the paper focuses on privacy and security, fairness is another critical aspect of VFL that needs to be considered. The paper does not include any discussion of fairness-related metrics or defense methods, which is a significant limitation, especially given the increasing awareness of fairness issues in machine learning. Finally, the paper does not provide a detailed analysis of the scalability of VFLAIR with an increasing number of participants. While the paper includes experiments with 2 and 4 parties, it does not explicitly test the framework's performance with a significantly larger number of participants. This is a limitation, as scalability is a crucial factor in practical VFL deployments. The paper also does not discuss the potential challenges of deploying VFLAIR in real-world scenarios, such as network latency, data heterogeneity, and the need for secure multi-party computation. These issues need to be addressed to make VFLAIR a more practical and useful tool for researchers and practitioners.\n\n\n## Suggestions:\n\nBased on the identified weaknesses, I recommend several improvements to enhance the paper's impact and utility. First, I suggest that the authors include a direct comparison of VFLAIR with existing VFL frameworks, such as FATE. This comparison should include quantitative metrics, such as system requirements, execution time, and resource usage, to provide a clear understanding of the trade-offs between VFLAIR and other frameworks. This would help potential users make informed decisions about which framework to use for their specific needs. Second, I recommend that the authors provide a more detailed analysis of the applicability of each defense method to different attack types. This analysis should be included in the main text, not just in the appendix or tables, and should clearly state which defenses are effective against which attacks. This would help researchers quickly identify the most appropriate defense methods for their specific concerns. Third, I suggest that the authors include a detailed analysis of the computational overhead associated with each defense method. This analysis should include metrics such as training time, communication cost, and memory usage. This would help researchers understand the practical implications of using different defense methods and make informed decisions about which methods to use in their specific contexts. Fourth, I recommend that the authors expand the experimental evaluation to include a wider range of datasets and model architectures. This should include experiments with larger models, such as ResNet or Inception, as well as datasets with highly imbalanced classes or high-dimensional sparse features. This would enhance the generalizability of the findings and demonstrate the robustness of VFLAIR in diverse scenarios. Fifth, I suggest that the authors incorporate fairness-related metrics and defense methods into VFLAIR. This would make the framework more comprehensive and address an important aspect of VFL that is currently missing. This could include metrics such as demographic parity, equal opportunity, and predictive parity, as well as defense methods that aim to mitigate bias in VFL models. Sixth, I recommend that the authors conduct more extensive experiments to evaluate the scalability of VFLAIR with an increasing number of participants. This should include experiments with a significantly larger number of parties to assess the framework's performance in more realistic scenarios. Finally, I suggest that the authors discuss the potential challenges of deploying VFLAIR in real-world scenarios, such as network latency, data heterogeneity, and the need for secure multi-party computation. This would make the paper more relevant to practitioners and would help bridge the gap between academic research and industrial applications.\n\n\n## Questions:\n\nBased on my analysis, I have several questions that I believe would be beneficial for the authors to address. First, given the focus on label inference attacks, I am curious about the specific scenarios where the other attack types (feature reconstruction, backdoor, and non-targeted attacks) are most relevant. What are the practical situations in which these attacks would be a primary concern, and how do the defense methods perform in those specific contexts? Second, regarding the computational overhead of the defense methods, what are the specific bottlenecks that contribute to the increased execution time? Are there any optimization techniques that could be used to reduce the computational cost of these methods? Third, concerning the scalability of VFLAIR, what are the main challenges in deploying the framework with a large number of participants? Are there any specific architectural limitations that need to be addressed to improve scalability? Fourth, given the lack of explicit fairness considerations, what are the authors' plans for incorporating fairness-related metrics and defense methods into VFLAIR? Are there any specific fairness challenges in VFL that the authors are particularly interested in addressing? Fifth, regarding the choice of datasets, what are the authors' criteria for selecting the datasets used in the experiments? Are there any specific characteristics of the datasets that make them representative of real-world VFL scenarios? Finally, given the focus on benchmarking, what are the authors' plans for maintaining and updating the benchmark results? How will they ensure that the benchmark remains relevant and up-to-date with the latest advancements in VFL research?\n\n\n## Rating:\n\n6.0\n\n\n## Confidence:\n\n3.75\n\n\n## Decision:\n\nAccept", "other_type": "DeepReviewer_fast", "v.s.": "B", "result": "```\n**Technical Accuracy - Evidence Analysis:**\n- From Assistant A:\n  [Direct quotes and specific examples]\n  - \"This paper introduces VFLAIR, a novel framework designed for research in Vertical Federated Learning (VFL).\" - Accurate summary of the paper's main contribution.\n  - \"VFLAIR supports a wide range of VFL algorithms, including various model architectures (linear regression, tree-based models, and neural networks), communication protocols (FedSGD, FedBCD, CELU-VFL, Top-k, and Quantize), and defense methods against data inference and backdoor attacks.\" - Accurately lists key features of VFLAIR as described in the paper (Introduction and Section 4).\n  - \"A key contribution of this work is the extensive benchmarking of 11 different attacks and 8 defense strategies under diverse settings...\" - Correctly identifies the benchmarking as a major contribution (Abstract and Section 5).\n  - \"The empirical evaluation includes experiments on 13 datasets...\" - Accurate number of datasets used in experiments (Section 5.1).\n  - \"The introduction of new evaluation metrics, such as AP, MP, and DCS, is also a positive aspect...\" - Correctly points out the new metrics (Section 5.3).\n- From Assistant B:\n  [Direct quotes and specific examples]\n  - \"This paper introduces VFLAIR, a lightweight and extensible framework for Vertical Federated Learning (VFL), designed to facilitate research and practical deployment.\" - Accurate summary of the paper's purpose.\n  - \"The core contribution of this work lies in the creation of a comprehensive benchmark for VFL, encompassing a wide range of models, datasets, communication protocols, and, crucially, various attack and defense strategies.\" - Correctly identifies the benchmark as the core contribution.\n  - \"The authors implement 29 local model architectures, 6 global models, and 5 communication protocols, alongside 11 different attack methods and 8 defense mechanisms.\" - Accurately lists the components of VFLAIR (Section 4 and Section 5.1, 5.2).\n  - \"The framework supports 13 datasets, including image, text, and tabular data, allowing for a diverse set of experiments.\" - Correctly states the number of datasets and their types (Section 5.1).\n  - \"The paper introduces new evaluation metrics, such as Defense Capability Score (DCS), Type-level DCS (T-DCS), and Comprehensive DCS (C-DCS), to assess the effectiveness of defense methods.\" - Correctly lists the new evaluation metrics (Section 5.3).\n- Comparative Assessment:\n  Both assistants demonstrate excellent technical accuracy. They both correctly identify the key contributions, features, and experimental setup of the paper.  Both accurately list the number of datasets, attacks, defenses, and other components. There are no factual errors in either review regarding the paper's content.\n\n**Technical Accuracy - Judgment:**\n**Evidence-Based Reason:** Both Assistant A and Assistant B accurately and comprehensively summarize the paper's technical details, including the framework's components, experimental setup, and key contributions. They both demonstrate a strong understanding of the paper's content and present factually correct information.\n**Better Assistant:** Tie\n\n---\n**Constructive Value - Evidence Analysis:**\n- From Assistant A:\n  [Direct quotes and specific examples]\n  - \"First, while the paper introduces VFLAIR as a lightweight and modular framework, it lacks a direct comparison with existing VFL frameworks, such as FATE... This comparison would help potential users understand the trade-offs between VFLAIR and other frameworks and would further justify the need for a new framework.\" - Suggestion for improvement is actionable and specific. Authors could add a table comparing VFLAIR to FATE on metrics like system requirements, ease of use, features, etc.\n  - \"Second, the paper's evaluation of defense methods lacks a comprehensive analysis of their applicability to different attack types... This lack of clarity makes it difficult to understand the effectiveness of each defense method against specific attacks.\" - Actionable suggestion. Authors could add a table or section explicitly mapping defenses to attack types and discussing their effectiveness.\n  - \"Third, the paper does not provide a detailed analysis of the computational overhead associated with the implemented defense methods... Researchers need to know the computational overhead of each defense method to make informed decisions...\" - Actionable suggestion. Authors could add experiments and analysis focusing on the computational cost of defenses.\n  - \"Fourth, the paper's experimental evaluation is limited to a specific set of datasets and model architectures... This limits the generalizability of the findings...\" - Actionable suggestion. Authors could expand experiments to include larger models, more diverse datasets.\n  - \"Fifth, the paper does not address the issue of fairness in VFL... The paper does not include any discussion of fairness-related metrics or defense methods...\" - Actionable suggestion. Authors could add a section discussing fairness and potentially incorporate fairness metrics into VFLAIR.\n  - \"Finally, the paper does not provide a detailed analysis of the scalability of VFLAIR with an increasing number of participants... This is a limitation, as scalability is a crucial factor in practical VFL deployments.\" - Actionable suggestion. Authors could add scalability experiments with more parties.\n- From Assistant B:\n  [Direct quotes and specific examples]\n  - \"First, the paper's novelty is limited... the paper does not introduce any novel defense techniques... This focus on implementation and empirical evaluation, while useful, positions the paper more as a system paper than a theoretical contribution.\" - Less actionable as it's a general comment on novelty rather than a specific improvement.\n  - \"Second, the paper lacks a significant theoretical contribution... The paper does not delve into the theoretical underpinnings of the observed empirical results...\" - Less actionable, similar to the first point, more of a characterization.\n  - \"Third, the paper does not provide a detailed analysis of the computational overhead of the proposed defense methods... This is a significant omission, as the computational cost of defense methods is a crucial factor to consider in practical deployments...\" - Similar to Assistant A's third point, actionable. Authors could add computational overhead analysis.\n  - \"Fourth, the paper does not explore the potential vulnerabilities of the proposed defense methods to more advanced attacks... The paper does not consider the robustness of these defenses against sophisticated adversaries...\" - Actionable suggestion. Authors could explore robustness against adaptive attacks.\n  - \"Fifth, the evaluation of the framework is limited to a relatively small number of datasets and model architectures... a wider range of datasets and models would further demonstrate the framework's generalizability.\" - Similar to Assistant A's fourth point, actionable. Authors could expand experiments.\n  - \"Finally, the paper lacks a detailed discussion of the limitations of the proposed framework and potential future research directions... The paper also does not thoroughly outline potential future research directions...\" - Actionable suggestion. Authors could expand the conclusion and future work section.\n- Comparative Assessment:\n  Both assistants provide constructive feedback. Assistant A's suggestions are generally more specific and actionable. For example, suggesting a direct comparison with FATE, or explicitly mapping defenses to attack types. Assistant B's suggestions are also valuable, but sometimes more general, like \"the paper lacks a significant theoretical contribution\" or \"novelty is limited\". While valid points, they are less directly actionable than Assistant A's detailed suggestions for experiments and analyses. Both identify the lack of computational overhead analysis and limited dataset/model range as weaknesses, which are important and constructive.\n\n**Constructive Value - Judgment:**\n**Evidence-Based Reason:** Both assistants offer constructive criticism and suggestions for improvement. However, Assistant A's feedback is more specific and directly actionable. Suggestions like comparing VFLAIR to FATE quantitatively, explicitly mapping defenses to attacks, and expanding experiments to larger models and diverse datasets provide clearer directions for the authors to improve their paper. While Assistant B also provides valuable points, they are sometimes more general and less immediately translatable into concrete actions for revision.\n**Better Assistant:** Assistant A\n\n---\n**Analytical Depth - Evidence Analysis:**\n- From Assistant A:\n  [Direct quotes and specific examples]\n  - Analyzes the paper's contribution in the context of existing frameworks (FATE).\n  - Identifies the lack of clarity in defense applicability as a weakness.\n  - Points out the missing computational overhead analysis as a significant limitation for practical deployment.\n  - Critiques the limited scope of experimental evaluation regarding model size and dataset diversity.\n  - Raises the issue of fairness, a relevant and important aspect in federated learning.\n  - Discusses the scalability issue with increasing participants.\n  - Questions the practical challenges of real-world deployment.\n  - Questions about attack scenarios, computational bottlenecks, scalability challenges, fairness considerations, dataset selection criteria, and benchmark maintenance. These questions demonstrate a deep engagement with the paper's topic and identify areas needing further clarification or research.\n- From Assistant B:\n  [Direct quotes and specific examples]\n  - Analyzes the paper's novelty and theoretical contribution, categorizing it as more of a system paper.\n  - Identifies the lack of computational overhead analysis.\n  - Points out the limited exploration of advanced attacks.\n  - Critiques the limited range of datasets and models in evaluation.\n  - Discusses the lack of detailed limitation discussion and future directions.\n  - Questions about advanced attacks against defenses, computational overhead impact on scalability, defense impact on convergence, performance in real-world scenarios, limitations and future directions, and comparison to other frameworks. These questions also show a good level of analytical depth and identify key areas for further consideration.\n- Comparative Assessment:\n  Both assistants demonstrate good analytical depth. They both identify key weaknesses and limitations of the paper, such as the lack of computational overhead analysis, limited experimental scope, and missing discussion of real-world deployment challenges. Assistant A's analysis is slightly more detailed in some areas, for example, in breaking down the suggestions into specific points and in the range of questions asked, which cover a slightly broader spectrum of concerns (fairness, benchmark maintenance). Assistant B focuses more on the theoretical and novelty aspects, which is also a valid analytical perspective. Both reviews are comprehensive in their assessment of the paper's strengths and weaknesses.\n\n**Analytical Depth - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate a strong analytical depth by thoroughly examining various aspects of the paper, including its contributions, limitations, experimental design, and practical implications. They both identify crucial weaknesses and ask insightful questions that probe deeper into the paper's methodology and findings. Assistant A's analysis is slightly more granular and covers a broader range of relevant aspects, including fairness and benchmark maintenance, giving it a marginal edge in depth. However, Assistant B's focus on theoretical contributions and advanced attacks also demonstrates a strong analytical understanding.\n**Better Assistant:** Assistant A\n\n---\n**Communication Clarity - Evidence Analysis:**\n- From Assistant A:\n  [Direct quotes and specific examples]\n  - Uses clear and concise language.\n  - Organizes feedback into strengths, weaknesses, and suggestions with bullet points, making it easy to read and understand.\n  - Provides specific examples and justifications for each point.\n  - Questions are clearly formulated and directly relevant to the paper's content.\n- From Assistant B:\n  [Direct quotes and specific examples]\n  - Uses clear and concise language.\n  - Organizes feedback into strengths, weaknesses, suggestions, and questions, similar to Assistant A, enhancing readability.\n  - Provides specific examples and justifications for each point.\n  - Questions are clearly formulated and relevant.\n- Comparative Assessment:\n  Both assistants communicate their feedback with excellent clarity. Both reviews are well-structured, using headings and bullet points to organize their points effectively. The language used is clear, concise, and professional. Both assistants provide specific examples and reasoning to support their claims, making their feedback easy to understand and follow. There is no significant difference in communication clarity between the two reviews.\n\n**Communication Clarity - Judgment:**\n**Evidence-Based Reason:** Both Assistant A and Assistant B demonstrate excellent communication clarity. Their reviews are well-organized, use clear and concise language, and provide specific examples and justifications. Both reviews are easy to read and understand, effectively conveying their feedback to the authors.\n**Better Assistant:** Tie\n\n---\n**Comprehensive Analysis:**\nAssistant A and Assistant B both provide strong and insightful reviews of the paper. Both assistants accurately summarize the paper's contributions, identify key strengths, and point out important weaknesses. They both offer constructive suggestions for improvement and ask relevant questions.\n\nAssistant A's review is slightly stronger in terms of **Constructive Value** and **Analytical Depth**. Assistant A's suggestions are more specific and actionable, providing clearer directions for the authors to revise their paper.  Assistant A also covers a slightly broader range of analytical points, including fairness and benchmark maintenance, and asks questions that delve into a wider spectrum of concerns.\n\nAssistant B's review is also excellent, particularly in its identification of the limitations regarding theoretical contributions and advanced attack scenarios. Assistant B's feedback is also highly constructive and clearly articulated.\n\nIn **Technical Accuracy** and **Communication Clarity**, both assistants are equally strong.\n\n**Overall Judgment:**\n**Evidence-Based Reason:** Both assistants provide high-quality reviews that are technically accurate, constructive, analytically deep, and clearly communicated. While both reviews are commendable, Assistant A's review is marginally better due to its slightly more specific and actionable suggestions and slightly broader analytical scope. This makes Assistant A's feedback potentially more immediately useful for the authors in revising their paper. However, the difference is not substantial, and both reviews are of high caliber.\n**Better Assistant:** Assistant A\n"}
{"id": "syoLhUJmth", "year": 2024, "paper_context": "\\title{From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models}\n\n\\begin{abstract}\n  Multi-modal Large Language Models (MLLMs) have made significant strides in expanding the capabilities of Large Language Models (LLMs) through the incorporation of visual perception interfaces. Despite the emergence of exciting applications and the availability of diverse instruction tuning data, existing approaches often rely on CLIP or its variants as the visual branch, and merely extract features from the deep layers. However, these methods lack a comprehensive analysis of the visual encoders in MLLMs. In this paper, we conduct an extensive investigation into the effectiveness of different vision encoders within MLLMs. Our findings reveal that the shallow layer features of CLIP offer particular advantages for fine-grained tasks such as grounding and region understanding. Surprisingly, the vision-only model DINO, which is not pretrained with text-image alignment, demonstrates promising performance as a visual branch within MLLMs. By simply equipping it with an MLP layer for alignment, DINO surpasses CLIP in fine-grained related perception tasks. Building upon these observations, we propose a simple yet effective feature merging strategy, named \\textbf{COMM}, that integrates \\textbf{C}LIP and DIN\\textbf{O} with \\textbf{M}ulti-level features \\textbf{M}erging, to enhance the visual capabilities of MLLMs. We evaluate \\textbf{COMM} through comprehensive experiments on a wide range of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination. Experimental results demonstrate the superior performance of \\textbf{COMM} compared to existing methods, showcasing its enhanced visual capabilities within MLLMs.\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\n\n\\label{sec:intro}\nLarge Language Models (LLMs)~\\cite{chatgpt, openai2023gpt4, touvron2023llama, llama2, taori2023stanford, chiangvicuna} have made significant strides in the domains of language understanding and generation, achieving remarkable progress recently. Through instruction tuning~\\cite{wei2021finetuned,wang2022self}, existing LLMs demonstrate their versatility as general-purpose models capable of handling a wide range of tasks. This capability unlocks their potential zero-shot learning ability, enabling seamless task switching guided by instructions. Building upon the promising performance of LLMs, researchers are now motivated to enhance their capabilities by incorporating visual signals as inputs. This extension allows the generation of textual outputs that are closely related to visual content, opening up exciting possibilities in the realm of vision-language understanding.\n\nTo this end, Flamingo~\\cite{alayrac2022flamingo} and BLIP2~\\cite{li2023blip2} align the powerful LLMs with a frozen visual encoder to understand visual inputs and perform various vision-language tasks. A series of following works, LLaVA~\\cite{liu2023llava}, InstructBLIP~\\cite{dai2023instructblip}, MiniGPT-4~\\cite{zhu2023minigpt} and mPLUG-OWL~\\cite{ye2023mplug} further improve the ability to follow human instructions by constructing multi-modal instruction-following datasets for training. However, these methods are built on image-level alignments, which suffer from the limited fine-grained understanding (such as region description~\\cite{liu2017referring} and reasoning~\\cite{zellers2019recognition}) and severe object hallucination problem~\\cite{li2023obj_Hallucination}. To this end, GPT4ROI~\\cite{zhang2023gpt4roi} proposes instruction tuning on region-of-interest and unlocks the region-level multimodal capacities. Kosmos-2~\\cite{peng2023kosmos} and Shikra~\\cite{chen2023shikra} further integrate the grounding abilities into LLMs and unlock the referential ability in dialogue, \\emph{i.e.}, enable the user to point to the object or region as input and the model responds with spatial coordinates of bounding boxes. Such grounding capacity can fulfill numerous vision-language tasks, which is a great progress in MLLMs.  \n\nDespite a wide variety of exciting methods and applications, most of existing multi-modal LLMs employ CLIP~\\cite{radford2021learning} or its variants ~\\cite{sun2023eva} as the visual branch, where the features output from the deep layers (\\emph{e.g.}, the penultimate layer) are usually employed as inputs to the language decoders. However, it still lacks analysis that: \\emph{Whether using the Vanilla CLIP features as visual encoder is the best way for MLLMs?} Though the visual encoder of CLIP is apparently well aligned with the word embedding space by image-text contrastive learning, it fails to learn more detailed pixel-level information such as color and positioning due to the global supervision of image captions, which might hinder the fine-grained perception ability in MLLMs. Besides, existing MLLMs have quite unbalanced visual and language encoders (\\emph{e.g.}, ViT-Large-300M vs. Vicuna-7B/13B). Since the language models have succeeded in scaling up the model size with progressively powerful language abilities, the short plate of the Buckets Effect for MLLMs lies in the visual models, which fails to demonstrate emerging capabilities, and suffer from domain gap and limited zero-shot ability. Consequently, it is critical to enhance the visual capabilities for boosting MLLMs. \n\nThis paper presents an extensive investigation into different visual encoders for MLLMs. Four typical visual foundation models are considered, \\emph{i.e.}, image-text contrastive learning CLIP, image-only contrastive learning DINOv2~\\cite{oquab2023dinov2}, masked image modeling MAE~\\cite{he2022masked} and supervised learning DeiT~\\cite{touvron2021training}. We evaluate the performance on commonly-used vision-language tasks including visual grounding, object hallucination, visual question answering, image captioning and MME benchmark. \nOur analysis reveals that different layers of features exhibit varying biases towards local and global patterns. Shallow layer features containing low-level detailed information prove beneficial for fine-grained perception tasks such as grounding and positioning ability, while deep layer features are superior at global understanding. To enhance representation, we propose a multi-level feature merging strategy that incorporates both low-level and high-level features. Surprisingly, when equipped with an MLP layer for alignment, the vision-only model DINOv2 shows promise as a visual branch for MLLMs. We attribute this to the fine-grained localization information captured by DINOv2. Conversely, MAE and DeiT perform inferiorly as visual branches for MLLMs. MAE learns limited semantic information, while DeiT's strong supervised training makes the alignment with the textual space challenging. Based on the above observations, we propose a fusion strategy that integrates \\textbf{C}LIP and DIN\\textbf{O} with \\textbf{M}ulti-level features \\textbf{M}erging), dubbed as \\textbf{COMM}, for boosting the visual branches of MLLMs.\nExperimental results demonstrate clear advantages of the proposed model over existing approaches and highlight the enhanced visual capabilities brought by \\textbf{COMM}. In a nutshell, the contributions of this paper are summarized as follows:\n\\begin{itemize}\n    \\item We are the first to extensively investigate the effectiveness of different visual encoders for MLLMs. Based on the analysis that shallow layer features contain low-level detailed information which is helpful for fine-grained tasks, we propose a multi-level feature fusion strategy to incorporate low-level and high-level features for improving representation.\n    \\item Our analysis indicates that vision-only DINOv2 achieves promising results in MLLMs with only an MLP layer for alignment. Considering fine-grained pixel information in DINOv2 and global semantic information in CLIP, we propose \\textbf{COMM} to fuse the visual embeddings of these two models to enhance visual capabilities for boosting MLLMs.\n    \\item Extensive experiments on a wide range of tasks including visual grounding, referring expression generation, object hallucination, visual question answering and image captioning demonstrate the superiority of \\textbf{COMM} over existing works.\n\\end{itemize}\n\n\n\\section{Related Work}\n\n\\textbf{Multi-modal Large Language Model.} LLMs~\\cite{dai2019transformer,brown2020language} have garnered significant attention in both academia and industry due to their remarkable understanding and generative abilities. The success of LLMs has motivated researchers to explore the integration of vision into these models, leading to the development of powerful multi-modal LLMs (MLLMs). Flamingo~\\cite{alayrac2022flamingo} employs a cross-attention module to extract visual contexts, which are concatenated with text token as input for LLMs. LLaVA~\\cite{liu2023visual} and FROMAGe~\\cite{koh2023grounding} leverage the vision encoder of CLIP to extract visual features, which is aligned to text features using a single linear layer and then input to LLMs. Models such as BLIP-2~\\cite{li2023blip}, mPLUG-OWL~\\cite{ye2023mplug}, MiniGPT-4~\\cite{zhu2023minigpt} and InstructBLIP~\\cite{dai2023instructblip} employ Q-former to extract text-aligned visual features for LLMs. Recently, some interesting works extend LLMs to image retrieval~\\cite{koh2023grounding}, video understanding~\\cite{zhang2023video}, audio~\\cite{su2023pandagpt}, biomedical analysis~\\cite{li2023llava}, control systems~\\cite{driess2023palme}. \n\nIn recent studies, there has been a growing interest in extending MLLMs to improve their fine-grained understanding abilities through region-level image-text alignment. Kosmos-2~\\cite{peng2023kosmos} addresses this by constructing a large-scale dataset of grounded region-text pairs, enabling the integration of grounding abilities into LLMs. GPT4RoI~\\cite{zhang2023gpt4roi} reformulates the bounding box as a spatial instruction format and extracts visual features based on region-of-interest, facilitating region-level multi-modal understanding. Shikra~\\cite{chen2023shikra} proposes a unified model that handles spatial coordinates to possess referential abilities in dialogue contexts. Ferret~\\cite{you2023ferret} and ViP-LLaVA~\\cite{cai2023making} further extends with a broader range of free-form shapes for referring, including points, boxes, sketches and scribbles.\nAdditionally, Qwen~\\cite{Qwen-VL} presents a set of MLLMs that demonstrate remarkable performance across various tasks. However, previous works have predominantly focused on extracting visual features solely from the last few layers of the CLIP model, resulting in an emphasis on global image properties. In this study, we draw attention to the fact that features extracted from shallower layers exhibit a stronger focus on localized properties, which we argue can be more potent in comprehending object locations and image details. Additionally, while CLIP primarily learns globally aligned features, advanced vision-alone models such as DINOv2 excel in capturing more fine-grained vision features. We posit that leveraging these fine-grained vision features can effectively enhance the capabilities of MLLMs, as demonstrated in our analysis. To further advance this line of research, we introduce a novel fusion module that expands and enhances the visual branches, thereby aiming to significantly improve the performance of MLLMs.\n\n\\textbf{Large Vision Foundation Model.} Recent progresses in training vision foundation models with large-scale image data focus on contrastive learning, masked image modeling and supervised training. For one thing, contrastive learning can be conducted in an image-only or image-text manner. DINOv2~\\cite{oquab2023dinov2} pretrains the image encoder on large curated image data, which shows a superior understanding of object parts and scene geometry across image domains. Image-text contrastive learning as CLIP~\\cite{radford2021learning} and EVA-CLIP~\\cite{sun2023eva} employs the natural language as weak supervision to guide the learning of visual features. \nFor another, BEiT~\\cite{HangboBao2021BEiT} predicts discrete tokens based on a pre-trained image tokenizer while iBOT~\\cite{zhou2021ibot} proposes an online image tokenizer. MAE~\\cite{he2022masked} proposes a masked autoencoder for reconstructing image pixels. Besides, DeiT III~\\cite{Touvron2022DeiTIR} proposes a training recipe to achieve promising performance.\nRecent MLLMs employ the vision encoder of CLIP/EVA-CLIP without considering the properties of specific visual models. \nIn this paper, we are the first to re-examine the effectiveness of existing visual models in MLLMs and propose a simple yet effective fusion strategy for boosting visual capabilities.\n\n\n\\section{Analysis of the Visual Branch in MLLMs}\n\nPrevious MLLMs~\\cite{liu2023llava,liu2023improvedllava,zhu2023minigpt,dai2023instructblip,ye2023mplug,peng2023kosmos,chen2023shikra,Qwen-VL,you2023ferret} usually utilize the vision encoder of CLIP as their visual branch. Typically, these models extract features from the last few layers, such as the penultimate layer, which are then fed into an alignment network. Subsequently, the aligned features are concatenated with text tokens to serve as input for the LLMs. While the image-text pretraining of CLIP aligns well with the language model, it primarily learns image-level features but overlooks the richer pixel-level features due to the constraint of limited fine-grained information in the guided captions. Moreover, the deep-layer features primarily focus on global image properties and inadequately explore the intricate details of local object parts. As depicted in Fig.~\\ref{fig:feature}, the visual features extracted from the shallow layers of CLIP and the deep visual features obtained from the visual-only model DINOv2 contain more detailed information regarding local objects, such as shape or texture. Leveraging these detailed features may enhance the MLLMs' fine-grained perception abilities. \n\n\\textbf{Evaluation Settings.} For further analysis, we conduct a series of quantitative experiments using different kinds of visual models, \\emph{i.e.}, image-text contrastive learning CLIP, image-only contrastive learning DINOv2, masked image modeling MAE and supervised learning DeiT. In specific, the visual features extracted from different layers of visual models (based on ViT-Large) are aligned using a linear projection layer and then concatenated with text tokens as the input for LLMs (here we use Vicuna-7B~\\cite{chiangvicuna}). The overall architecture and training process follow Shikra~\\cite{chen2023shikra} but with fewer iterations (9400 iterations, batch size 16 on 4 A800) to save the computation cost. Then, we measure the capability of the trained MLLMs on referring expression comprehension (REC)~\\cite{chen2023shikra}, referring expression generation (REG)~\\cite{peng2023kosmos} and object hallucination benchmark (POPE)~\\cite{li2023obj_Hallucination}. \nDetailed descriptions of these tasks can be referred to Sec.~\\ref{sec:experiments}.\n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=1\\textwidth]{feature.png}\n\\caption{Feature correspondence visualization by computing the cosine similarity of different visual tokens extracted from the shallow and deep layers of CLIP and DINOv2. }\\label{fig:feature}\n\\end{figure}\n\n\\textbf{CLIP as the Visual Branch of MLLMs.} As depicted in Fig.~\\ref{fig:rec}, we observe that different layers of features exhibit varying biases towards grounding and understanding abilities. For instance, the shallow features demonstrate relatively higher accuracy in terms of REC and reach their optimal value at layer 12. Conversely, the deep features achieve higher accuracy in terms of POPE, indicating superior understanding ability. Notably, the relatively deep features (layer 16) display the best REG CIDEr score, showcasing promising region understanding capabilities. Consequently, instead of solely relying on deep features as done in previous works, we argue that integrating both shallow and deep features is crucial for MLLMs with improved overall performance.\n\nWe further explore various merging modes of low-level and high-level features. Denoting the output features from each transformer layer of ViT with a depth of $N$ as $\\mathbf{z}=[ z_1,..,z_i,...,z_N ]$, we discuss several multi-level feature merging (MFM) strategies for combining shallow and deep features, namely: \n\n$\\bullet$ \\emph{Mean(half)}: averaging output patch token features in the second half of the backbone as $z=(z_{N/2}+\\cdots+z_{N})/(N/2)$. \n\n$\\bullet$ \\emph{Mean(all)}: averaging features output by all layers as $z=(z_{1}+\\cdots+z_{N})/N$. \n\n$\\bullet$ \\emph{Layerscale(all)}: learning a scale parameter as the weight to sum features output by all layers as $z=w_1z_{1}+\\cdots+w_Nz_{N}$, where $w_i$ refers to the weight assigned to the $i$-th layer feature and all these weights are dynamically updated and summed up to 1. \n\n$\\bullet$ \\emph{LLN-Layerscale(all)}: using a linear-layernorm module to align the feature space between different layers’ features and then summed by \\emph{Layerscale} as $z=w_1\\mathrm{LLN}(z_{1})+\\cdots+w_N\\mathrm{LLN}(z_{N})$.\n\n$\\bullet$ \\emph{Conv-Layerscale(all)}: using a convolution and bn module to align the feature space between different layers’ features and then summed by \\emph{Layerscale} as $z=w_1\\mathrm{Conv}(z_{1})+\\cdots+w_N\\mathrm{Conv}(z_{N})$. \n\nFig.~\\ref{fig:clipanddino} (a) and (b) shows that simply averaging all shallow and deep features of CLIP can \\emph{de facto} achieve a satisfactory accuracy and \\emph{LLN-Layerscale} strategy further improves performance. With \\emph{LLN-Layerscale} as MFM module, the performance of CLIP can be evidently improved on commonly-used vision-language tasks as shown in Table~\\ref{tab:fusion}. \n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n \\subfloat[Average REC accuracy.]{\n\\includegraphics[width=0.33\\columnwidth]{analysis_rec2.png}}\n\\subfloat[Average POPE accuracy.]{\\includegraphics[width=0.33\\columnwidth]{analysis_pope2.png}}\n\\subfloat[Average REG CIDEr.]{\\includegraphics[width=0.33\\columnwidth]{analysis_reg2.png}}\n\\caption{Average REC, POPE accuracy and REG CIDEr for using different layers of features extracted from various vision models (CLIP, DINOv2 and MAE), as input to MLLMs. Shikra uses the 23rd layer features of CLIP and we reproduce its results with fewer iterations (denoted as Shikra$^*$).}\n\\label{fig:rec}\n\\end{figure}\n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n \\subfloat[ REC acc for CLIP.]{\n\\includegraphics[width=0.26\\columnwidth]{recclip.png}}\n\\subfloat[ POPE acc for CLIP.]{\n\\includegraphics[width=0.248\\columnwidth]{popeclip.png}}\n\\subfloat[REC acc for DINO.]{\\includegraphics[width=0.247\\columnwidth,height=0.222\\columnwidth]{recdino.png}}\n\\subfloat[POPE acc for DINO.]{\\includegraphics[width=0.248\\columnwidth,height=0.222\\columnwidth]{popedino.png}}\n\\caption{Average REC and POPE accuracy for merging different layers of features with mutli-feature merging (MFM) strategies as input to MLLMs for visual backbones of CLIP and DINOv2. }\n\\label{fig:clipanddino}\n\\end{figure}\n\n\\textbf{DINOv2 as the Visual Branch of MLLMs.} \nTo leverage the rich fine-grained visual information present in DINOv2, but not inherently aligned with text, we employ a non-linear Multi-Layer Perceptron (MLP) module to align the image features with the word embedding space. Fig.~\\ref{fig:rec}  demonstrates that the deep-layer features of DINOv2 exhibit superior grounding abilities, as evidenced by higher REC accuracy, and display satisfactory understanding abilities, as indicated by favorable POPE and REG results. Additionally, we explore the efficacy of multi-level feature merging to enhance performance. In contrast to CLIP, the merging of shallow features from DINOv2 leads to a significant performance degradation. Specifically, in Fig.~\\ref{fig:clipanddino}(c) and (d), it is evident that \\emph{Mean(all)} performs notably worse than \\emph{Mean(19-24)} in terms of both REC and POPE accuracy, indicating that the shallow representations lack sufficient semantic information. Building upon the \\emph{LLN-Layerscale} approach, the incorporation of the MLP module for a more potent connection between the visual and text spaces demonstrates a clear improvement in performance. Table~\\ref{tab:fusion} showcases the substantial performance gains achieved by employing \\emph{LLN-Layerscale-MLP} as Multi-Level Feature Merging (MFM) module across various vision language tasks. Further detailed ablation studies on the MLP module are in Section~\\ref{sec:mae}. \n\\begin{table}[!t]\n\\centering\n\\setlength{\\tabcolsep}{2.8pt}\n\\caption{Comparison of the visual model using CLIP, DINOv2 with Multi-level Feature Merging (MFM) and \\textbf{COMM} to incorporate visual embeddings of both models on VL tasks. CLIP baseline use the 23rd layer features, which follows Shikra but with fewer training iterations. DINOv2 baseline is w/o MLP module.\nMME CS and PS indicate cognition and perception score, respectively.\n}\n\\label{tab:fusion}\n\\scalebox{0.82}{\n\\begin{tabular}{l|cccccccc}\n\\toprule\nVisual Model  &Avg REC& Avg POPE &COCO & Flickr30k &MME CS&MME PS   & VQAv2&OK-VQA  \\\\\n\\cmidrule(lr){1-9}\nCLIP&47.3&82.3&125.0&80.7&209.6&1107.8&68.8&44.2 \\\\\nDINOv2&54.8&78.3&118.0&68.9&261.8&930.5&63.1&41.9\\\\\n\\cmidrule(lr){1-9}\nCLIP w/ MFM&  70.0    & 83.4 & 125.8 & 81.0&296.6&1164.4 &69.5  & 44.7 \\\\\nDINOv2 w/ MFM\n& \\textbf{72.8}&83.3&123.4&76.3 &  252.9 &1086.8&68.0 & 42.1 \\\\\n \\cmidrule(lr){1-9}\n\\textbf{COMM}\n& \\textbf{72.8} & \\textbf{83.6}&\\textbf{127.3}&\\textbf{81.9}  & \\textbf{360.4}&  \\textbf{1234.9}&  \\textbf{70.1}& \t\\textbf{45.0}\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{table}\n\n\\textbf{MAE and DeiT as the Visual Branch of MLLMs.} Fig.~\\ref{fig:rec} shows that MAE features achieve acceptable REC accuracy, but suffers large performance drop on POPE and REG evaluation. This is because MAE features lack sufficient semantic information for global or regional understanding. Therefore, MAE is not suitable as the visual branch for MLLMs. DeiT performs even worse than MAE (details in Section~\\ref{sec:mae}). We speculate that this is because supervised training is too strong, which learns a specialized visual space that is difficult to align with the word embedding space.\n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=1\\columnwidth]{overall2.pdf}\n\\caption{Overview of our \\textbf{COMM}. The image is input to the vision encoder of CLIP and DINOv2, and the features from shallow and deep layers are incorporated by multi-level feature merging. The features of DINOv2 are aligned with an MLP and concatenated with features of CLIP, which are input to a linear layer. Then the fused features are concatenated with text tokens as input to LLMs.}\n\\label{fig:overall}\n\\end{figure}\n\n\n\n\\section{\\textbf{COMM}\n\n\\textbf{Architecture Overview.} In this section, we introduce the proposed \\textbf{COMM}, that integrates CLIP and DINO with Multi-level features Merging to enhance the visual capabilities of MLLMs. The overall framework is illustrated in Fig.~\\ref{fig:overall}, \\textbf{COMM} is incorporated into a vision-language instruction following model built upon the\nrecent advanced language and vision-language foundation models. Following the input instructions, our model takes vision and language as inputs to generate text responses following the input instructions.\nSpecifically, we adopt the visual encoder of CLIP and DINOv2 (based on ViT-Large) with our proposed fusion strategy as the visual branch, and Vicuna~\\cite{chiangvicuna} (7B/13B) as language decoder. The visual encoder is downsampled with rate 14, meaning that an image with resolution $H\\times W$ will be represented by a sequence of $\\frac{H}{14}\\times \\frac{W}{14}$ tokens.\nThe fused token features are projected using a linear layer and then concatenated with the instruction tokens as inputs to the language decoder, which is a generic interface to unify various vision-language tasks as text generation task.\n\nSpecifically, denote the visual encoder of CLIP and DINOv2 (ViT Large used) as $f_1$ and $f_2$, respectively. Given an input image $x$, we extract the patch token features output by all layers of CLIP as $f_1(x)=[v_1^{1},...,v_1^{i},...,v_1^{24}]$, where $v_1^i \\in \\mathbf{R}^{N\\times D}$, $N$ is the number of patch tokens and $D$ is the embedding dimension. The features output by the deep layers of DINOv2 are $f_2(x)=[v_2^{19},...,v_2^{i},...,v_2^{24}]$. Then we concatenate the features output by these two models as $\\mathbf{v}=[v_1^{1},...,v_1^{24},v_2^{19},...,v_2^{24}]$. A linear-layernorm module is employed to align the feature space between different layers’ features and layerscale is used to merge multiple layer features as\n\\begin{equation}\n    \\overline{v}_1 = \\sum_{i=1}^{24} \\alpha_i \\cdot \\mathrm{Linear}(\\mathrm{LN}(v_1^{i})), \\qquad \\overline{v}_2 = \\sum_{j=19}^{24} \\beta_j \\cdot \\mathrm{Linear}(\\mathrm{LN}(v_2^{j}))\n\\end{equation}\nwhere $\\alpha$ and $\\beta$ are the learnable scaling parameter. Then, we employ an MLP layer to project the features of DINOv2 and concatenate the output features with that of CLIP as $\\mathbf{\\overline{v}}=[\\overline{v}_1,\\mathrm{MLP}(\\overline{v}_2)]$. Then, a linear layer is employed to match the dimension of visual features to that of text features as $\\mathbf{\\hat{v}}=\\mathrm{Linear}(\\mathbf{\\overline{v}})$. Finally, fused visual features $\\mathbf{\\hat{v}}$ are concatenated with text tokens as input to LLMs.\n\n\n\\section{Experiments}\n\n\\label{sec:experiments}\nIn this section, we conduct extensive evaluation on four kinds of vision-language tasks to comprehensively evaluate the visual understanding ability of our model, namely, Referring Expression Comprehension, Referring Expression Generation, Object Hallucination Benchmark, and Visual Question Answering and Image Captioning.\n\n\\textbf{Training Details.} Similar to previous MLLM methods, \\textbf{COMM} is trained in two stages. In the first pretraining stage, we train the model on the reorganized vision-language dataset as~\\cite{chen2023shikra}, including public VQA, Image Captioning datset and several datasets containing positional annotation RefCOCO, visual gemone~\\cite{krishna2017visualgenome} and Visual-7W~\\cite{mani2020pointqa}. The first pretraining stage is conducted for 100K steps. In the second instruction tuning stage, we set the sampling ratio to 50\\% on LLaVA-Instruct-150K~\\cite{liu2023llava} and Shikra-RD~\\cite{chen2023shikra}. Instead of 224 × 224 resolution currently used by existing MLLMs, we use 336 × 336 resolution to reduce the information loss caused by image down-sampling and promote the fine-grained perception ability. In both stages, we freeze the visual encoder and tune all parameters in LLMs, alignment layer and multi-level feature fusion module. We adopt AdamW~\\cite{DBLP:conf/iclr/LoshchilovH19adamw} as the optimizer and cosine annealing\nscheduler~\\cite{DBLP:conf/iclr/LoshchilovH17cos} as learning rate scheduler with an initial learning rate of\n2e-5 and global batch size of 64. All training runs on 8 NVIDIA A800 GPUs. It takes around 100h for stage one training and 20h for stage two.\n\n\\begin{table*}[!t]\n\\centering\n\\caption{Results on standard referring expression comprehension (REC) task. Generalist VL models can perform various vision-language tasks.\nSpecialist models are designed specifically for localization tasks or generalist pretraining models that undergone finetuning. The results of Shikra, Qwen, Ferret and Griffon are from their papers.\n}\n\\label{tab:rec}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{l|l|cccccccc}\n\\toprule\n\\multirow{2}{*}{Model type}\n& \\multirow{2}{*}{Model}  & \\multicolumn{3}{c}{RefCOCO} & \\multicolumn{3}{c}{RefCOCO+} & \\multicolumn{2}{c}{RefCOCOg} \\\\\n &  & val & test-A & test-B & val & test-A & test-B & val-u & test-u \\\\\n\\cmidrule(lr){1-2}\\cmidrule(lr){3-10}\n\\multirow{9}{*}{\\tabincell{l}{Generalist VL SOTAs \\\\ (w/o finetuning)}}\n& OFA-L*      & 79.96 & 83.67 & 76.39 & 68.29 & 76.00 & 61.75 & 67.57 & 67.58  \\\\\n& VisionLLM-H   & - & 86.70     & - & - & - & - & - & - \\\\\n& Shikra-7B      & 87.01 & 90.61 & 80.24 & 81.60 & 87.36 & 72.12 & 82.27 & 82.19 \\\\\n& Shikra-13B     & 87.83 & 91.11 & 81.81 & 82.89 & 87.79 & 74.41 & 82.64 & 83.16  \\\\\n&Ferret-7B&87.49&91.35&82.45&80.78&87.38&73.14&83.93&84.76\\\\\n&Ferret-13B&89.48&92.41&84.36&82.81&88.14&75.17&85.83&86.34\\\\\n&Griffon-13B&88.00&92.10&81.90&81.50&88.20&73.30&82.90&84.30\\\\\n&Qwen-VL-7B&89.36& 92.26 &85.34 &83.12& 88.25& 77.21& 85.58& 85.48\\\\\n&Qwen-VL-7B-Chat& 88.55& 92.27& 84.51 &82.82& 88.59 &76.79& 85.96 &86.32\\\\\n& \\textbf{COMM-7B (Ours)}     &\\textbf{91.73}& \\textbf{94.06}&\\textbf{88.85}&\\textbf{87.21}&\\textbf{91.74}&\\textbf{81.39}&\\textbf{87.32}&\\textbf{88.33}\\\\\n\\cmidrule(lr){1-2}\\cmidrule(lr){3-10}\n\\multirow{3}{*}{\\tabincell{l}{Specialist SOTAs \\\\ (Specialist/Finetuned)}}\n& G-DINO-L     & 90.56 & 93.19 & 88.24 & 82.75 & 88.95 & 75.92 & 86.13 & 87.02 \\\\\n& UNINEXT-H    & 92.64 & 94.33 & 91.46 & 85.24 & 89.63 & 79.79 & 88.73 & 89.37 \\\\\n& ONE-PEACE    & 92.58 & 94.18 & 89.26 & 88.77 & 92.21 & 83.23 & 89.22 & 89.27 \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table*}\n\\begin{table*}[!t]\n\\centering\n\\setlength{\\tabcolsep}{4.5pt}\n\\caption{Results on standard referring expression generation (REG) task in CIDEr score. We reproduce the results of Shikra-7B using its officially released checkpoint. SLR is a finetuned listener-speaker model with an added reward-based module (SLR).\n}\n\\label{tab:reg}\n\\scalebox{0.95}{\n\\begin{tabular}{l|cccccccc}\n\\toprule\n \\multirow{2}{*}{Model}  & \\multicolumn{3}{c}{RefCOCO} & \\multicolumn{3}{c}{RefCOCO+} & \\multicolumn{2}{c}{RefCOCOg} \\\\\n  & val & test-A & test-B & val & test-A & test-B & val-u & test-u \\\\\n\\cmidrule(lr){1-9}\nSLR~\\cite{yu2017joint}&-&69.7&132.3&-&49.4&70.9&59.2&-\\\\\nSLR+Rerank~\\cite{yu2017joint}&-&77.5&132.0&-&52.0&73.5&66.2&-\\\\\n\\cmidrule(lr){1-9}\nShikra     &75.61&44.26&104.83&56.42& 40.98&68.25&62.71&65.58\\\\\nKosmos-2    & - & - & - & - & - & - & 62.3 & - \\\\\n\\textbf{COMM} (Ours)   &\\textbf{93.35}&\\textbf{54.95}&\\textbf{131.13}&\\textbf{70.00}&\\textbf{52.27}&\\textbf{79.05}&\\textbf{79.22}&\\textbf{77.96}\\\\ \n\\bottomrule\n\\end{tabular}}\n\\end{table*}\n\\subsection{Referring Expression Comprehension}\nTo evaluate the fine-grained understanding and positioning capability of our model, we investigate the referring expression comprehension task on benchmarks as RefCOCO~\\cite{kazemzadeh2014referitgame}, RefCOCO+~\\cite{mao2016refcocog} and RefCOCOg~\\cite{mao2016refcocog}, where models are asked to localize the object described with an expression. As shown in Table~\\ref{tab:rec}, compared with generalist VL models and previous SOTA MLLMs, \\textbf{COMM} achieves significant performance gain on all benchmarks, \\emph{i.e.}, \\textbf{COMM}-7B outperforms Shikra-13B and Qwen-VL-7B-Chat by 4.87\\% and 3.10\\% accuracy on average, respectively. With more powerful visual capabilities of our proposed fusion model, we can evidently surpass recent SOTA MLLMs in a more efficient way, \\emph{e.g.}, using a smaller LLM than Shikra (7B vs. 13B) and less training data than Qwen (3.6M vs. 1.4B). Besides, our generalist model even achieves comparable results with specialist SOTA methods, showing the superior grounding ability of our MLLMs.\n\n\\subsection{Referring Expression Generation}\nMoreover, we evaluate the ability to understand image regions or objects referred via inputting bounding boxes. Instead of referring image regions or objects via detailed text descriptions, directly referring to image regions via its bounding boxes is more effective and can reduce the ambiguity.\nThe experiments are conducted on the referring expression generation task with RefCOCO, RefCOCO+ and RefCOCOg, aiming to generate text descriptions of specific regions in the bounding box. Table~\\ref{tab:reg} shows that our model outperforms Shikra and Kosmos-2 by a considerable margin of 16.51 CIDEr and 16.92 CIDEr gain on RefCOCOg, demonstrating the effectiveness of our model for fine-grained understanding. Besides, \\textbf{COMM} even outperforms finetuned SLR on RefCOCO+ and RefCOCOg.\n\n\\begin{table}[!t]\n\\centering\n\\setlength{\\tabcolsep}{1.5pt}\n\\caption{Object hallucination benchmark using POPE evaluation pipeline \\cite{li2023obj_Hallucination}. The results of Shikra-7B are taken from its paper. Except for Shikra-7B, the other results are obtained from \\cite{li2023obj_Hallucination}.\n}\n\\label{tab:pope_results}\n\\scalebox{0.95}{\n\\begin{tabular}{l|ccccccc}\n\\toprule\nDatasets  &\\textbf{COMM}& Shikra & InstructBLIP  & MiniGPT-4 & LLaVA&MM-GPT & mPLUG-Owl \\\\\n\\cmidrule(lr){1-8}\nRandom\n&   87.29   & 86.90 & \\textbf{88.57} & 79.67 &50.37 &50.10 & 53.97 \\\\\nPopular\n&   \\textbf{86.50}    & 83.97  & 82.77 &69.73  &49.87 &50.00 &50.90  \\\\\nAdversarial\n&   \\textbf{84.50}    & 83.10  & 72.10  &65.17  &  49.70 &50.00& 50.67\\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\\begin{table*}[!t]\n\\centering\n\\renewcommand{\\tabcolsep}{0.9mm}\n\\caption{Results on visual question answering (VQA) and image captioning.\nFor VQA, we evaluate SOTA generalist models and our \\textbf{COMM} onVQAv2 and OK-VQA following the normalization rules. Shikra and LLaVA-1.5~\\cite{liu2023improvedllava} is based on the 13B variant.\nFor image captioning, we evaluate them on COCO and Flickr30k in CIDEr score.\nWe call Flamingo as FM for short.\n}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{l|l|cccccccc}\n\\toprule\n\\multicolumn{2}{c|}{Datasets} &\\textbf{COMM}&LLaVA-1.5&Qwen& Shikra  & FM-80B& BLIP-2 & Unified-IO  & VPGTrans\\\\\n\\cmidrule(lr){1-2}\\cmidrule(lr){3-10}\n\\multirow{4}{*}{VQA}\n& VQAv2$^\\text{val}$     &\\textbf{79.05}&-&-& 75.33  &   -   & 65.2 & -  & 65.2 \\\\\n& VQAv2$^\\text{dev}$     &\\textbf{81.04}&80.0&79.5& 77.36  & 56.3 & 65.0 & 77.9  & - \\\\\n& VQAv2$^\\text{std}$     &\\textbf{81.17}&-&-& 77.51  &  -  &  -  & -  & - \\\\\n& OK-VQA                 &\\textbf{59.18}&-&58.6& 47.16  & 50.6 & 45.9 & 54.0  & 45.0 \\\\\n\\cmidrule(lr){1-2}\\cmidrule(lr){3-10}\n\\multirow{2}{*}{Caption}\n& Flickr30k      &\\textbf{88.2}&-&85.8 & 73.9 &  67.2   &   -  &   -   &   -   \\\\\n& COCO           &\\textbf{132.7}&-&- & 117.5 & 84.3  &   -  & 122.3 &   -  \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\label{tab:vl}\n\\end{table*}\n\\subsection{Object Hallucination Benchmark}\nWe compare our model against the baseline models\non the hallucination evaluation dataset recently introduced by POPE~\\cite{li2023obj_Hallucination}, which randomly selects 500 images from COCO~\\cite{caesar2018coco}. Table~\\ref{tab:pope_results} shows that \\textbf{COMM} surpasses recent popular MLLMs with 1.44\\% and 4.95\\% higher accuracy on average than Shikra and InstrutBLIP, respectively. By enhancing the fine-grained visual capabilities, \\textbf{COMM} can effectively alleviate the object hallucination problem.\n\n\\subsection{Visual Question Answering and Image Captioning}\nWe evaluate \\textbf{COMM} on conventional VL tasks of VQA and Image Captioning. Specifically, image captioning requires the model to generate description for the given image and VQA asks the model to generate answer for the given image-question pair. For image captioning, we choose COCO~\\cite{chen2015cococap} and Flickr30K~\\cite{plummer2015flickr30ke} as benchmarks and report the CIDEr score. For VQA task, we experiment on VQAv2~\\cite{antol2015vqav2} and OK-VQA~\\cite{marino2019ok}. \nAs shown in Table~\\ref{tab:vl}, \\textbf{COMM} achieves state-of-the-art performance on image captioning task, \\emph{i.e.}, 88.2 CIDEr score on Flickr30K and 132.7 CIDEr score on COCO, even outperforms previous SOTA models with much more parameters (\\emph{e.g.}, Shikra-13B with 13B parameters) or much more training data (\\emph{e.g.}, Qwen with 1.4B data). For VQA task, our model also shows significant advantages compared to other MLLMs. On VQAv2 val, dev and std, our model achieves 79.05, 81.04 and 81.17 accuracy respectively, which surpasses recent proposed Shikra with the same training data and procedure by a large margin, demonstrating the effectiveness of merging visual embeddings of DINOv2 and CLIP for enhancing visual capabilities. Besides, our \\textbf{COMM} model outperforms Qwen with 1.54 and 0.58 accuracy gain on VQAv2 dev and OK-VQA respectively with less VQA training data, \\emph{i.e.}, we use 0.6M and Qwen with 3.6M. Training with more VQA data might further improve performance and we leave it as future work.\n\\subsection{Ablation Study}\n\\label{sec:mae}\n\\textbf{Ablation on the MLP of DINOv2.} We conduct ablation study on the design of the MLP module in DINOv2 for aligning visual and text embedding space. \nWe ablate on the number and the expanding ratio of MLP module.\nTable~\\ref{tab:mlp} shows that increasing the number of MLP to 2 can evidently improve performance, demonstrate the effectiveness of using a more powerful network to align the vision only model DINOv2 to the word embedding space. However, increasing the number beyond 2 suffers the degraded performance. For the expanding ratio, increasing to 8 can improve performance, while increasing to 16 does not achieve significant performance gain. Moreover, we experiment with one linear layer, which suffers severe performance degradation. Thus, non-linear MLP is necessary for aligning the features of vision-only DINOv2 to the word embedding space.\n\n\\textbf{Ablation on the visual model of MAE and DeiT.} As shown in Table~\\ref{tab:deit}, MAE and DeiT suffers from evident performance degradation. For one thing, the visual features of MAE lack sufficient semantic information for global or regional understanding. For another, the supervised training of DeiT is so strong that it learns specialized visual space, making it difficult to align with the word embedding space. \n\\begin{table}[!t]\n\\centering\n\\setlength{\\tabcolsep}{2pt}\n\\caption{Ablation study on the number and expanding ratio of MLP module. Experiments are conducted on referring expression comprehension and object hallucination benchmark on Random (R), Adversarial (A), and Popular (P).\n}\n\\label{tab:mlp}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|ccccccccc}\n\\toprule\n\\multirow{2}{*}{Visual Model}  & \\multicolumn{3}{c}{RefCOCO+} & \\multicolumn{2}{c}{RefCOCOg}& \\multicolumn{3}{c}{RefCOCO}  &POPE\\\\\n& test-A & test-B &val& test-u&  val-u  &  test-A & test-B &val  &A/P/R\\\\\n \\cmidrule(lr){1-10}\nDINOv2 w/ MLP Ratio 4\n& 75.3 &59.3&67.0&73.0&71.8&84.4&74.1&79.6&80.3/84.2/85.5  \\\\\n\\cmidrule(lr){1-10}\nDINOv2 w/ 2MLP Ratio 4\n& \\textbf{77.5}& \t\\textbf{60.3} &69.2&\t\\textbf{74.6} &\\textbf{74.7} &\\textbf{86.5} &75.3 &81.4 &\\textbf{82.4}/84.5/86.2 \\\\\nDINOv2 w/ 4MLP Ratio 4\n& 53.7& \t34.4 &45.3&\t49.0 &48.8&\t65.4 &48.0 &57.9 &79.2/82.9/84.6 \\\\\nDINOv2 w/ 8MLP Ratio 4\n& 8.2 & \t6.5 & \t7.4 & \t6.8 & 6.7 & 14.8 & 12.9 & 14.9& 56.0/55.3/59.0\\\\ \n\\cmidrule(lr){1-10}\nDINOv2 w/ MLP Ratio 8\n&77.4 &59.9 &\\textbf{69.7} &73.7 &73.3 &85.7&74.1 &80.9 &81.5/\\textbf{85.8}/\\textbf{86.7}\\\\\nDINOv2 w/ MLP Ratio 16\n&76.2 &60.2 &\\textbf{69.7} &74.5&74.6&85.7 &\\textbf{75.5} &\\textbf{81.5} &80.4/83.7/85.7\\\\ \n\\cmidrule(lr){1-10}\nDINOv2 w/ Linear\n&61.8& \t48.8 &\t55.1 &\t64.1 &\t62.9 &\t76.5 &\t67.0 &\t71.9& \t75.6/79.3/83.7 \\\\\n\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\\begin{table}[!t]\n\\centering\n\\setlength{\\tabcolsep}{5.3pt}\n\\caption{Comparison of the visual model using CLIP, DINOv2 with our multi-level feature merging (MFM), MAE and DeiT. MAE-20 denotes using the features output by the 20-th layer of MAE. DeiT-20 denotes using the features output by 20-th layer.\n}\n\\label{tab:deit}\n\\scalebox{1}{\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{l|ccccccccc}\n\\toprule\n\\multirow{2}{*}{Visual Model}  & \\multicolumn{3}{c}{RefCOCO+} & \\multicolumn{2}{c}{RefCOCOg}& \\multicolumn{3}{c}{RefCOCO}  &POPE\\\\\n& test-A & test-B &val& test-u&  val-u  &  test-A & test-B &val  &A/P/R\\\\\n\\cmidrule(lr){1-10}\nCLIP w/ MFM&  73.7&53.8 &64.3 &69.1 &70.3&83.8 &68.4&76.4&\\textbf{80.7}/\\textbf{84.2}/\\textbf{85.8} \n \\\\\n \\cmidrule(lr){1-10}\nDINOv2 w/ MFM\n& \\textbf{75.3} &\\textbf{59.3}&\\textbf{67.0}&\\textbf{73.0}&\\textbf{71.8}&\\textbf{84.4}&\\textbf{74.1}&\\textbf{79.6}&80.3/\\textbf{84.2}/85.5  \\\\\n\\cmidrule(lr){1-10}\nMAE-20&64.7&\t49.4&\t56.8\t&63.7&\t62.8&\t77.9&\t68.6\t&73.6\t&66.8/71.1/76.7\\\\\nMAE-22&65.9& \t50.0 &\t58.5 &\t64.2 &\t63.2& \t79.3& \t69.8& \t74.9 &\t68.0/71.2/77.5\\\\ \n\\cmidrule(lr){1-10}\nDeiT-20&18.4&\t13.0&\t15.9&\t17.0\t&16.2\t&29.0&\t21.6&\t25.7&\t66.2/69.6/77.9\\\\\nDeiT-22&25.3&\t15.4&\t19.4&\t22.6&\t21.8\t&36.9&\t25.3&\t32.0&\t67.9/71.6/78.7\\\\\n\\bottomrule\n\\end{tabular}%\n}}\n\\end{table}\n\\subsection{Demonstrations}\nAs shown in Fig.~\\ref{fig:case}, our \\textbf{COMM} model exhibits a multitude of promising capabilities including visual grounding, fine-grained region understanding and robustness to object hallucination. The first example showcases our strong fine-grained perception ability, which identifies implicit strawberries in a blender. The second example exhibits our strong visual grounding ability to successfully locates the object of sugar. The third case demonstrates our robustness to object hallucination. In contrast, Shikra fails on these challenging cases, showing the superior capabilities of our model. We provide additional demonstrations of our \\textbf{COMM} model in this section to demonstrate a multitude of promising capabilities including visual grounding, fine-grained region understanding and robustness to object hallucination. For instance, we showcase Referring Expression Comprehension in Fig.~\\ref{fig:caserec} and Object Hallucination in Fig.~\\ref{fig:caseobj}.\n\\begin{figure}[h!]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=1\\columnwidth]{rec_case.png}\n\\caption{Referring Expression Comprehension (REC) using our \\textbf{COMM}-7B. The task intends to localize a target object in an image described by a referring expression.}\n\\label{fig:caserec}\n\\end{figure}\n\\begin{figure}[!h]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=1\\columnwidth]{object.png}\n\\caption{Object hallucination using our \\textbf{COMM}-7B. This task aims to evaluate the robustness to object hallucination, \\emph{i.e.}, answer yes or no for the existence of questioned object.}\n\\label{fig:caseobj}\n\\end{figure}\n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=0.9\\columnwidth]{case2.png}\n\\caption{Qualitative comparison between Shikra with its official checkpoint and our \\textbf{COMM}.}\n\\label{fig:case}\n\\end{figure}\n\n\n\\section{Conclusion}\n\nThis paper presented an extensive investigation into the efficacy of different visual models when employed as the visual branch in MLLMs. Through a systematic analysis, we highlight the significance of shallow layer features, which capture low-level details that prove beneficial for grounding and positioning tasks. Furthermore, we recognize the potential of the vision-only model DINOv2, which leverages its inherent fine-grained pixel-level information for enhanced fine-grained perception in MLLMs when combined with an MLP layer for alignment purposes. Motivated by our analysis, we introduce a fusion approach to combine the visual features obtained from CLIP and DINOv2, thereby further augmenting the visual capabilities and performance of MLLMs. Through qualitative analysis and extensive quantitative experiments, we demonstrate the effectiveness of our proposed method, surpassing the performance of existing MLLM models across diverse benchmark datasets. Looking ahead, we encourage future research to explore the integration of more powerful vision models to enhance the capabilities of visual branches in MLLMs. We believe that this avenue of investigation holds the key to unlocking the potential of the next generation of MLLMs.\n\n\n\n\n", "DeepReviewer": "\n## Summary:\n\nThis paper introduces COMM, a novel approach to enhance the visual capabilities of Multimodal Large Language Models (MLLMs) by integrating the strengths of CLIP and DINOv2 through a multi-level feature merging strategy. The authors' core contribution lies in their investigation of different visual encoders within MLLMs, demonstrating that combining the global semantic understanding of CLIP with the fine-grained localization information from DINOv2 leads to improved performance across a range of vision-language tasks. The COMM method employs a multi-level feature merging (MFM) strategy, which aligns and combines features from different layers of both CLIP and DINOv2 using a linear layer, layer normalization, and learnable scaling parameters. For DINOv2, which lacks inherent text alignment, an MLP layer is used to project its features into the word embedding space. The authors evaluate their method on a variety of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination, demonstrating superior performance compared to existing methods. The empirical findings suggest that the proposed feature merging strategy effectively leverages the complementary strengths of CLIP and DINOv2, resulting in enhanced visual understanding within MLLMs. The paper's overall significance lies in its exploration of visual encoder effectiveness and the introduction of a practical method for improving visual capabilities in MLLMs. However, the paper also presents several limitations, particularly in terms of the practical applicability of the method due to the large feature vector size, the lack of a strong theoretical foundation, and the limited generalizability of the results. Despite these limitations, the paper provides valuable insights into the potential of combining different visual encoders for MLLMs and offers a promising direction for future research.\n\n\n## Soundness:\n\n2.75\n\n\n## Presentation:\n\n2.75\n\n\n## Contribution:\n\n2.75\n\n\n## Strengths:\n\nI find several aspects of this paper to be commendable. Firstly, the paper's core idea of combining CLIP and DINOv2 to leverage their complementary strengths is both intuitive and effective. The authors' analysis of the different biases of CLIP and DINOv2, with CLIP excelling in global understanding and DINOv2 capturing fine-grained details, provides a solid rationale for their approach. The empirical results, demonstrating superior performance on a range of vision-language tasks, validate the effectiveness of the proposed COMM method. Specifically, the performance gains on tasks like Referring Expression Comprehension (REC), Object Hallucination Benchmark (POPE), and Visual Question Answering (VQA) are significant and showcase the enhanced visual capabilities of MLLMs when using COMM. The authors' exploration of different feature merging strategies, including the use of linear-layernorm and learnable scaling parameters, is a valuable contribution. The use of an MLP layer to align DINOv2's features with the word embedding space is also a practical and effective solution. Furthermore, the paper is well-written and easy to follow, making the methodology and results accessible to a broad audience. The authors' focus on practical application and empirical validation is a strength, as it demonstrates the real-world potential of their approach. The paper's exploration of visual encoder effectiveness within MLLMs is a significant contribution, as it highlights the importance of carefully selecting and combining visual encoders to achieve optimal performance. Finally, the paper's demonstration of the COMM method's ability to enhance visual capabilities in MLLMs is a promising direction for future research in this field.\n\n\n## Weaknesses:\n\nDespite the strengths of this paper, I have identified several significant weaknesses that warrant careful consideration. The most pressing limitation is the practical applicability of the COMM method due to the large visual feature length. The authors explicitly state that they concatenate features from both CLIP and DINOv2, which are both based on ViT-Large. Given the image resolution of 336x336 and the downsampling rate of 14, each encoder outputs 576 tokens per layer. Concatenating features from multiple layers of both encoders results in a feature vector that can easily exceed the context window of standard LLMs like LLaMA2, which has a maximum input length of 4096 tokens. This is a critical issue, as it makes it difficult to apply the COMM method to most existing MLLMs without significant modifications or feature compression techniques. This limitation is explicitly stated in the paper, where the authors mention the concatenation of features and the resulting token sequence (lines 296-298). My analysis confirms that this concatenation, even with multi-level merging, leads to a feature vector that could easily exceed the typical context window of transformer-based LLMs, creating a significant bottleneck for practical deployment. This is a high-confidence concern, as it is directly supported by the paper's methodology and the known limitations of standard LLMs. \n\nFurthermore, the reliance on two distinct visual encoders introduces a computational overhead that may not be feasible for resource-constrained environments or real-time applications. The paper confirms the use of both CLIP and DINOv2 as visual encoders (lines 291-292). Processing two large visual encoders and handling the resulting large feature vector will undoubtedly increase the computational cost, making the method less practical for deployment in resource-constrained environments. This is another high-confidence concern, as it is a direct consequence of the method's design. The paper lacks a strong theoretical foundation to explain why the COMM method enhances the visual capabilities of MLLMs. While the authors provide some intuition about the complementary strengths of CLIP and DINOv2, there is no formal analysis of the feature spaces or a theoretical framework explaining the benefits of the specific merging strategy. The paper mentions that shallow layers capture local details and deep layers capture global semantics (lines 49-52), and that DINOv2 captures fine-grained localization (lines 53-55). However, there is no mathematical analysis of the feature spaces of CLIP and DINOv2, nor is there a formal justification for the chosen merging strategy. This lack of theoretical grounding makes it difficult to understand the underlying mechanisms of the method and limits the generalizability of the findings. This is a high-confidence concern, as it is evident from the absence of a formal theoretical framework in the paper. The COMM method's generalizability is also limited, as it is primarily evaluated within the Shikra framework. The paper explicitly mentions using Vicuna as the language decoder and following the training setup of Shikra (lines 291-294, 311-313). While the architecture description is general, the experimental setup heavily relies on the Shikra framework's training data and procedure. This makes it difficult to assess the robustness of the proposed method across different MLLM designs. This is a high-confidence concern, as it is directly supported by the paper's experimental setup. The COMM method also lacks significant innovation in terms of feature merging techniques. The paper describes the MFM strategy as involving linear-layernorm for alignment and learnable scaling parameters for merging (lines 59-61). For DINOv2, an MLP is used for alignment (lines 61-63). While effective, these techniques are relatively standard and do not represent a significant architectural innovation. This is a high-confidence concern, as it is evident from the description of the MFM strategy in the paper. The paper's evaluation is limited to a few general visual tasks, such as REC, REG, POPE, VQA, and Image Captioning (lines 320-322). It does not include more specialized domains like medical imaging or video understanding. This lack of evaluation on specialized tasks limits the understanding of COMM's performance in more complex and domain-specific scenarios. This is a high-confidence concern, as it is evident from the list of evaluated tasks in the paper. Finally, the paper lacks sufficient interpretability, as it does not provide a detailed analysis of how the different feature levels contribute to the overall performance. While the paper provides some high-level analysis of feature layer contributions (lines 137-138, 142-144) and shows performance improvements, there is no detailed analysis of how the learnable scaling parameters in the MFM strategy affect the contribution of different feature levels, or a deeper dive into the interaction between CLIP and DINOv2 features. This lack of interpretability makes it harder to understand the specific mechanisms behind the performance gains and limits the ability to further refine the method. This is a high-confidence concern, as it is evident from the lack of detailed analysis in the paper. The paper also fails to convincingly demonstrate the unique advantages of the proposed method over existing approaches, particularly given the increased computational cost and complexity. While the paper shows performance improvements, the simplicity of the method and lack of deep interpretability, coupled with potential computational costs, might not fully justify the claimed advantages. This is a medium-confidence concern, as it is a more subjective assessment of the paper's overall contribution. In summary, while the paper presents a promising approach, these limitations significantly impact its practical applicability and generalizability.\n\n\n## Suggestions:\n\nTo address the identified weaknesses, I recommend several concrete improvements. Firstly, the authors should explore techniques to reduce the dimensionality of the combined feature space. Given the concatenation of features from both CLIP and DINOv2, the resulting feature vector can be quite large, potentially exceeding the context window of standard LLMs. To mitigate this, the authors could investigate methods such as Principal Component Analysis (PCA) or autoencoders to compress the feature representation while preserving the most informative components. This would make the method more applicable to a wider range of MLLMs and deployment scenarios. Furthermore, the authors should investigate the possibility of using more efficient feature merging strategies that do not rely on the concatenation of all feature levels. For example, they could explore attention-based mechanisms to selectively combine the most relevant features from CLIP and DINOv2. This would allow the model to focus on the most informative features, reducing the dimensionality of the combined feature space and potentially improving performance. Another approach could be to investigate knowledge distillation techniques to transfer the knowledge of the combined model into a more compact representation. This would allow the authors to leverage the performance of the combined model while reducing the computational cost of deployment. To enhance the theoretical grounding of the COMM method, the authors should provide a more detailed analysis of the feature spaces of CLIP and DINOv2. This could involve visualizing the feature distributions, analyzing the correlation between different feature levels, and investigating the impact of different merging strategies on the resulting feature representation. A formal analysis of the information content of the different feature levels would also be beneficial. The authors should also explore the possibility of using techniques such as information bottleneck to identify the most informative features for the downstream tasks. This would provide a more solid theoretical foundation for the proposed method and help to understand why it works. To address the limited generalizability of the method, the authors should significantly broaden the evaluation of the COMM method to include a more diverse set of MLLM architectures and visual tasks. This should include evaluating the method on different LLM backbones, such as LLaMA3, and on more specialized tasks such as medical image analysis or video understanding. The authors should also provide a more detailed analysis of the computational cost of the proposed method, including the memory footprint and inference time. This would help to assess the practical feasibility of the method for real-world applications. Furthermore, the authors should provide a more detailed analysis of the interpretability of the method, including visualizing the contribution of different feature levels to the final prediction. This would help to understand the underlying mechanisms of the proposed method and provide insights for further improvements. Finally, the authors should explore more sophisticated feature fusion techniques to enhance the innovation of the method. This could involve exploring cross-attention mechanisms that allow for more fine-grained interaction between the features of CLIP and DINOv2, or investigating adaptive layer selection mechanisms, perhaps guided by a task-specific objective. These improvements would not only address the identified weaknesses but also enhance the overall impact and significance of the paper.\n\n\n## Questions:\n\nI have several questions regarding the methodology and results presented in this paper. Firstly, given the significant increase in visual feature length due to the concatenation of CLIP and DINOv2 features, what specific strategies do the authors propose to mitigate the context window limitations of standard LLMs? Have they explored any feature compression techniques or alternative merging strategies that could reduce the dimensionality of the combined feature space without sacrificing performance? Secondly, considering the lack of a formal theoretical framework, what specific analyses have the authors conducted to understand the interaction between the feature spaces of CLIP and DINOv2? Have they visualized the feature distributions or analyzed the correlation between different feature levels? What is the rationale behind the specific merging strategy, and how does it impact the information content of the resulting feature representation? Thirdly, given the limited evaluation of the COMM method on the Shikra framework, what are the authors' plans to evaluate the method on a more diverse set of MLLM architectures? What specific challenges do they anticipate when applying the method to different LLM backbones, and how do they plan to address these challenges? Fourthly, considering the relatively straightforward nature of the feature merging strategy, what are the authors' plans to explore more sophisticated feature fusion techniques? Have they considered attention-based mechanisms or adaptive layer selection mechanisms? What are the potential benefits and drawbacks of these more complex approaches? Fifthly, given the limited evaluation on general visual tasks, what are the authors' plans to evaluate the method on more specialized domains, such as medical image analysis or video understanding? What specific challenges do they anticipate when applying the method to these more complex and domain-specific scenarios? Finally, considering the lack of interpretability of the method, what specific analyses have the authors conducted to understand the contribution of different feature levels to the final prediction? Have they visualized the feature maps or used techniques like t-SNE to project the high-dimensional features into a lower-dimensional space? What are the specific mechanisms behind the performance gains observed with the COMM method?\n\n\n## Rating:\n\n4.5\n\n\n## Confidence:\n\n5.0\n\n\n## Decision:\n\nReject", "other": "\n## Summary:\n\nThis paper introduces COMM, a novel multi-modal large language model (MLLM) that leverages the strengths of both CLIP and DINOv2 visual encoders. The core idea is to combine CLIP's image-text alignment capabilities with DINOv2's fine-grained visual representations. The authors propose a multi-level feature merging (MFM) strategy to integrate features from different layers of both encoders, aiming to enhance the model's performance on various vision-language tasks. Specifically, COMM employs a linear projection layer with layer normalization (LLN) and layer scaling to align and merge features from CLIP and DINOv2. For DINOv2, which lacks inherent text alignment, a non-linear multi-layer perceptron (MLP) module is used to align its features with the text embedding space. The authors conduct extensive experiments on a range of tasks, including visual question answering (VQA), image captioning, visual grounding, and object hallucination. The empirical results demonstrate that COMM generally outperforms existing MLLMs, particularly in tasks requiring fine-grained visual understanding. The paper also includes ablation studies to analyze the impact of different feature merging strategies and the contribution of each visual encoder. The authors emphasize the importance of combining both low-level and high-level features for improved representation learning. Overall, the paper presents a compelling approach to integrating diverse visual encoders into MLLMs, showcasing the potential of combining contrastive and self-supervised learning for enhanced vision-language understanding. The findings suggest that leveraging the complementary strengths of different visual models can lead to significant performance gains in multimodal tasks. However, the paper also has some limitations, particularly in terms of the novelty of the approach and the scope of the experimental evaluation, which I will discuss in detail in the following sections.\n\n\n## Soundness:\n\n2.75\n\n\n## Presentation:\n\n2.75\n\n\n## Contribution:\n\n2.0\n\n\n## Strengths:\n\nI found several aspects of this paper to be commendable. First, the core idea of combining CLIP and DINOv2 visual encoders is well-motivated. The authors correctly identify the strengths of each model: CLIP's strong image-text alignment and DINOv2's fine-grained visual representations. The proposed multi-level feature merging (MFM) strategy, while not entirely novel in its individual components, is a reasonable approach to integrate features from different layers of both encoders. The use of a non-linear MLP to align DINOv2's features with the text embedding space is a necessary step, given DINOv2's lack of inherent text alignment. The empirical results presented in the paper are generally strong, demonstrating that COMM outperforms existing MLLMs on a variety of tasks, including VQA, image captioning, visual grounding, and object hallucination. The ablation studies provide valuable insights into the impact of different feature merging strategies and the contribution of each visual encoder. The authors also highlight the importance of combining both low-level and high-level features for improved representation learning, which is a well-established concept in computer vision. The paper is generally well-written and easy to follow, with clear explanations of the proposed method and experimental setup. The authors also provide a detailed analysis of the performance of COMM on different tasks, which helps to understand the strengths and weaknesses of the proposed approach. The inclusion of experiments on visual grounding and object hallucination is particularly valuable, as these tasks are often challenging for MLLMs. The paper also includes a detailed analysis of the performance of COMM on different tasks, which helps to understand the strengths and weaknesses of the proposed approach. The inclusion of experiments on visual grounding and object hallucination is particularly valuable, as these tasks are often challenging for MLLMs. Finally, the paper's focus on fine-grained visual understanding is timely and relevant, given the increasing importance of this capability in many real-world applications.\n\n\n## Weaknesses:\n\nDespite the strengths of this paper, I have identified several weaknesses that warrant careful consideration. First, the novelty of the proposed method is somewhat limited. While the authors combine CLIP and DINOv2, the core idea of merging multiple features from different encoders is not entirely new, as similar approaches have been explored in models like Qwen-VL. The paper acknowledges Qwen-VL but doesn't provide a direct comparison of the feature merging strategies, which would have been beneficial. The use of a simple MLP for aligning DINOv2 features, while effective, is also not a groundbreaking contribution. The paper's claim of being the 'first to extensively investigate the effectiveness of different visual encoders' is also debatable, as other works like Qwen-VL have also explored this area. This lack of clear novelty is a significant concern, as it diminishes the overall contribution of the paper. My confidence in this assessment is high, as the paper itself acknowledges the existence of similar approaches, and the lack of a direct comparison makes it difficult to ascertain the unique contribution of COMM's feature merging strategy. Second, the paper lacks a comprehensive comparison with other state-of-the-art MLLMs. While the authors compare COMM with several models, the selection is not always consistent across different tasks, and some recent models, such as MiniCPM-V, are missing. This makes it difficult to assess the true performance of COMM relative to the current state-of-the-art. Furthermore, the paper does not include comparisons with models that use the same ViT-Large backbone, which would have provided a more controlled comparison. This is a significant limitation, as it makes it difficult to isolate the impact of the proposed feature merging strategy from the impact of the backbone architecture. My confidence in this assessment is high, as the absence of key models and consistent comparisons is evident from the paper's tables. Third, the paper lacks a detailed analysis of the computational cost of COMM. The authors do not provide information on the number of parameters, FLOPs, or inference time for COMM or the compared models. This is a critical omission, as the computational cost is a crucial factor in the practical applicability of any model. Without this information, it is difficult to assess the trade-off between performance and computational cost for COMM. My confidence in this assessment is high, as the absence of computational cost information is clear from the paper. Fourth, the paper does not adequately address the potential for redundancy when combining two powerful visual encoders. While the authors highlight the complementary strengths of CLIP and DINOv2, they do not provide a detailed analysis of the feature space to demonstrate that the combined features are indeed capturing distinct aspects of the visual input. This is a concern, as simply concatenating features from two models can lead to redundancy and increased computational cost without a corresponding increase in performance. My confidence in this assessment is medium, as the paper does provide some justification for the combination, but lacks a detailed analysis of feature redundancy. Fifth, the paper's claim that DINOv2 is not inherently aligned with text is not fully supported by the evidence. While DINOv2 is not trained with text-image pairs, it has been shown to possess emergent localization capabilities that could be leveraged for alignment. The paper does not explore the possibility of fine-tuning the MLP alignment layer with text-image pairs to leverage DINOv2's localization capabilities for better text alignment. This is a missed opportunity, as it could have led to improved performance. My confidence in this assessment is high, as the paper itself acknowledges DINOv2's localization capabilities, but does not explore their potential for text alignment. Finally, the paper does not explicitly discuss the limitations of the COMM method. This is a significant omission, as it is important to understand the potential drawbacks and challenges of any proposed approach. My confidence in this assessment is high, as the absence of a dedicated limitations section is clear from the paper.\n\n\n## Suggestions:\n\nBased on the identified weaknesses, I recommend several concrete improvements for this paper. First, the authors should conduct a more thorough comparison with state-of-the-art MLLMs, including models like MiniCPM-V. The comparison should be consistent across all tasks, and the authors should try to control for the backbone architecture when comparing different feature merging strategies. This would provide a more accurate assessment of the performance of COMM relative to the current state-of-the-art. Second, the authors should provide a detailed analysis of the computational cost of COMM, including the number of parameters, FLOPs, and inference time. This information is crucial for assessing the practical applicability of the model. The authors should also compare the computational cost of COMM with other MLLMs. Third, the authors should conduct a more detailed analysis of the feature space to demonstrate that the combined features from CLIP and DINOv2 are indeed capturing distinct aspects of the visual input. This could involve visualizing the feature maps or using other techniques to analyze the feature representations. This would help to address the concern about potential redundancy. Fourth, the authors should explore the possibility of fine-tuning the MLP alignment layer with text-image pairs to leverage DINOv2's emergent localization capabilities for better text alignment. This could potentially lead to improved performance. The authors should also compare the performance of COMM with a version that uses a CLIP backbone with an expanded MLP module. Fifth, the authors should include a dedicated section discussing the limitations of the COMM method. This would provide a more balanced and realistic assessment of the proposed approach. Sixth, the authors should consider exploring more sophisticated feature fusion techniques beyond simple concatenation and linear projection. This could involve exploring attention-based fusion mechanisms or other non-linear methods. This could potentially lead to further performance improvements. Seventh, the authors should provide a more detailed explanation of the differences between COMM and other multi-branch MLLMs, such as Qwen-VL. This would help to clarify the unique contribution of COMM. Finally, the authors should consider conducting experiments on additional tasks, such as long-text captioning and multi-modal in-context learning, to further demonstrate the capabilities of COMM. These improvements would address the identified weaknesses and strengthen the overall contribution of the paper.\n\n\n## Questions:\n\nI have several questions that arise from my analysis of this paper. First, what is the specific rationale for choosing the particular layers of CLIP and DINOv2 for feature merging? The paper mentions that shallow layers of CLIP are beneficial for grounding and positioning tasks, but a more detailed explanation of the layer selection process would be helpful. Second, how sensitive is the performance of COMM to the choice of the MLP architecture used for aligning DINOv2 features? The paper includes some ablation studies on the number and expanding ratio of the MLP layers, but a more thorough analysis of the MLP architecture would be beneficial. Third, what is the impact of the proposed feature merging strategy on the interpretability of the model? Does the combination of features from different encoders make it more difficult to understand how the model is making its predictions? Fourth, how does the performance of COMM scale with the size of the training data? Does the model benefit from larger training datasets, and how does this compare to other MLLMs? Fifth, what is the impact of the image resolution on the performance of COMM? Does the model benefit from higher resolution images, and how does this affect the computational cost? Sixth, how does the performance of COMM compare to other methods on more challenging visual reasoning tasks? Does the model exhibit any limitations in its ability to perform complex visual reasoning? Finally, what are the potential ethical implications of using a model like COMM, which combines multiple powerful visual encoders? Are there any potential biases or risks associated with the use of such a model? These questions target key uncertainties in the paper's methodology and seek clarification of critical assumptions, which would help to further understand the proposed approach.\n\n\n## Rating:\n\n5.25\n\n\n## Confidence:\n\n4.25\n\n\n## Decision:\n\nReject", "other_type": "DeepReviewer_fast", "v.s.": "A", "result": "```\n**Technical Accuracy - Evidence Analysis:**\n- From Assistant A:\n  - Claim: \"practical applicability of the COMM method due to the large visual feature length.\"\n  - Evidence: \"authors explicitly state that they concatenate features from both CLIP and DINOv2, which are both based on ViT-Large... Concatenating features from multiple layers of both encoders results in a feature vector that can easily exceed the context window of standard LLMs like LLaMA2...\" (Review)\n  - Analysis: Assistant A accurately points out a potential technical issue regarding the feature vector size. The paper describes concatenating features from CLIP and DINOv2 (Fig. 2, Section 4), both ViT-Large based, which could lead to a large input sequence for the LLM, potentially exceeding context limits. This is a valid technical concern based on the paper's description of COMM.\n- From Assistant B:\n  - Claim: \"novelty of the proposed method is somewhat limited... core idea of merging multiple features from different encoders is not entirely new, as similar approaches have been explored in models like Qwen-VL.\"\n  - Evidence: \"paper acknowledges Qwen-VL but doesn't provide a direct comparison of the feature merging strategies...\" (Review)\n  - Analysis: Assistant B correctly identifies a potential lack of novelty. The paper mentions Qwen-VL in Related Work (Section 2), and feature fusion is a common technique.  The review accurately notes the absence of a direct comparison of feature merging strategies with Qwen-VL, which is relevant to assessing the novelty.\n  - Claim: \"paper's claim that DINOv2 is not inherently aligned with text is not fully supported by the evidence... DINOv2... has been shown to possess emergent localization capabilities that could be leveraged for alignment.\"\n  - Evidence: \"paper does not explore the possibility of fine-tuning the MLP alignment layer with text-image pairs to leverage DINOv2's localization capabilities for better text alignment.\" (Review)\n  - Analysis: Assistant B questions the paper's framing of DINOv2's text alignment. While DINOv2 isn't trained with text, its emergent properties could be relevant for alignment. The review correctly points out that the paper doesn't explore fine-tuning the MLP with text-image data to further leverage DINOv2's capabilities, which is a valid technical critique.\n- Comparative Assessment:\n  - Both assistants raise technically accurate points. Assistant A focuses on a practical limitation related to context window size, directly stemming from the described architecture. Assistant B questions the novelty and a specific framing in the paper regarding DINOv2's alignment, also technically valid critiques. Both are grounded in the paper's content and general knowledge of MLLMs.\n\n**Technical Accuracy - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate strong technical accuracy by identifying valid concerns and critiques directly related to the paper's content and methodology. Assistant A highlights a practical limitation regarding context window size, while Assistant B questions novelty and the framing of DINOv2's alignment. Both critiques are well-founded and technically sound.\n**Better Assistant:** Tie\n- Tie: Both assistants demonstrate equally strong technical accuracy by raising valid and distinct technical concerns directly related to the paper's content.\n\n```\n**Technical Accuracy - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate strong technical accuracy by identifying valid concerns and critiques directly related to the paper's content and methodology. Assistant A highlights a practical limitation regarding context window size, while Assistant B questions novelty and the framing of DINOv2's alignment. Both critiques are well-founded and technically sound.\n**Better Assistant:** Tie\n- Tie: Both assistants demonstrate equally strong technical accuracy by raising valid and distinct technical concerns directly related to the paper's content.\n```\n\n```\n**Constructive Value - Evidence Analysis:**\n- From Assistant A:\n  - Suggestion: \"explore techniques to reduce the dimensionality of the combined feature space... PCA or autoencoders to compress the feature representation...\" (Review)\n  - Analysis: This is a highly constructive suggestion directly addressing the identified weakness of large feature vectors. PCA and autoencoders are feasible and relevant techniques for dimensionality reduction in this context.\n  - Suggestion: \"investigate the possibility of using more efficient feature merging strategies that do not rely on the concatenation of all feature levels... attention-based mechanisms...\" (Review)\n  - Analysis: This is also a constructive suggestion, offering alternative feature merging approaches that could be more efficient and potentially improve performance. Attention mechanisms are a relevant and advanced technique in this field.\n- From Assistant B:\n  - Suggestion: \"conduct a more thorough comparison with state-of-the-art MLLMs, including models like MiniCPM-V... control for the backbone architecture...\" (Review)\n  - Analysis: This is a constructive suggestion for improving the evaluation. Comparing with more SOTA models and controlling for backbone architecture would strengthen the paper's empirical validation and comparative analysis.\n  - Suggestion: \"provide a detailed analysis of the computational cost of COMM, including the number of parameters, FLOPs, and inference time.\" (Review)\n  - Analysis: This is a highly constructive and practical suggestion. Computational cost is crucial for practical applicability, and this suggestion directly addresses a missing element in the paper.\n  - Suggestion: \"explore the possibility of fine-tuning the MLP alignment layer with text-image pairs to leverage DINOv2's emergent localization capabilities for better text alignment.\" (Review)\n  - Analysis: This is a constructive suggestion building upon the critique about DINOv2's alignment. Fine-tuning the MLP with text-image data is a feasible and potentially beneficial experiment.\n- Comparative Assessment:\n  - Both assistants provide highly constructive feedback. Assistant A focuses on architectural improvements to address the feature vector size issue and suggests dimensionality reduction and alternative merging strategies. Assistant B focuses on improving the evaluation and analysis, suggesting broader comparisons, computational cost analysis, feature space analysis, and exploring fine-tuning. Both offer actionable and relevant suggestions that would significantly improve the paper.\n\n**Constructive Value - Judgment:**\n**Evidence-Based Reason:** Both assistants offer highly constructive and actionable feedback. Assistant A provides concrete suggestions for architectural improvements like dimensionality reduction and attention-based merging. Assistant B focuses on strengthening the evaluation and analysis with suggestions for broader comparisons, computational cost analysis, feature space analysis, and exploring fine-tuning. Both sets of suggestions are valuable and directly address identified weaknesses, offering clear paths for improvement.\n**Better Assistant:** Tie\n- Tie: Both assistants provide equally high constructive value by offering distinct yet highly relevant and actionable suggestions that would significantly improve the paper.\n\n```\n**Constructive Value - Judgment:**\n**Evidence-Based Reason:** Both assistants offer highly constructive and actionable feedback. Assistant A provides concrete suggestions for architectural improvements like dimensionality reduction and attention-based merging. Assistant B focuses on strengthening the evaluation and analysis with suggestions for broader comparisons, computational cost analysis, feature space analysis, and exploring fine-tuning. Both sets of suggestions are valuable and directly address identified weaknesses, offering clear paths for improvement.\n**Better Assistant:** Tie\n- Tie: Both assistants provide equally high constructive value by offering distinct yet highly relevant and actionable suggestions that would significantly improve the paper.\n```\n\n```\n**Analytical Depth - Evidence Analysis:**\n- From Assistant A:\n  - Analysis: \"practical applicability of the COMM method due to the large visual feature length... This limitation is explicitly stated in the paper, where the authors mention the concatenation of features and the resulting token sequence (lines 296-298). My analysis confirms that this concatenation, even with multi-level merging, leads to a feature vector that could easily exceed the typical context window of transformer-based LLMs...\" (Review)\n  - Analysis Depth: Assistant A demonstrates analytical depth by not just stating a weakness but also tracing it back to specific details in the paper (concatenation, token sequence, line numbers) and connecting it to a broader understanding of LLM context window limitations.\n  - Analysis: \"lack of a strong theoretical foundation to explain why the COMM method enhances the visual capabilities of MLLMs... no formal analysis of the feature spaces or a theoretical framework explaining the benefits of the specific merging strategy.\" (Review)\n  - Analysis Depth: Assistant A goes beyond surface-level critique and delves into the deeper issue of theoretical grounding, pointing out the absence of formal analysis of feature spaces and theoretical justification for the method.\n- From Assistant B:\n  - Analysis: \"novelty of the proposed method is somewhat limited... paper acknowledges Qwen-VL but doesn't provide a direct comparison of the feature merging strategies, which would have been beneficial.\" (Review)\n  - Analysis Depth: Assistant B shows analytical depth by connecting the paper to related work (Qwen-VL) and identifying a missing comparative element (feature merging strategies comparison) crucial for assessing novelty.\n  - Analysis: \"paper does not adequately address the potential for redundancy when combining two powerful visual encoders... do not provide a detailed analysis of the feature space to demonstrate that the combined features are indeed capturing distinct aspects of the visual input.\" (Review)\n  - Analysis Depth: Assistant B demonstrates analytical depth by raising the nuanced issue of feature redundancy, going beyond simply noting the use of two encoders and asking for deeper analysis of the feature space to justify their combination.\n- Comparative Assessment:\n  - Both assistants exhibit strong analytical depth. Assistant A deeply analyzes the practical implications of feature concatenation and the lack of theoretical grounding. Assistant B deeply analyzes the novelty aspect by comparing to related work and raises the nuanced point of potential feature redundancy, requiring deeper feature space analysis. Both go beyond superficial comments and delve into deeper analytical points.\n\n**Analytical Depth - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate strong analytical depth. Assistant A provides in-depth analysis of practical limitations and theoretical gaps, referencing specific parts of the paper and connecting them to broader concepts. Assistant B offers deep analysis of novelty and potential redundancy, requiring a nuanced understanding of related work and feature space analysis. Both assistants go beyond surface-level critiques and demonstrate a thorough and deep engagement with the paper's core aspects.\n**Better Assistant:** Tie\n- Tie: Both assistants demonstrate equally strong analytical depth by providing insightful and nuanced critiques that go beyond surface-level observations and delve into deeper aspects of the paper's methodology and contribution.\n\n```\n**Analytical Depth - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate strong analytical depth. Assistant A provides in-depth analysis of practical limitations and theoretical gaps, referencing specific parts of the paper and connecting them to broader concepts. Assistant B offers deep analysis of novelty and potential redundancy, requiring a nuanced understanding of related work and feature space analysis. Both assistants go beyond surface-level critiques and demonstrate a thorough and deep engagement with the paper's core aspects.\n**Better Assistant:** Tie\n- Tie: Both assistants demonstrate equally strong analytical depth by providing insightful and nuanced critiques that go beyond surface-level observations and delve into deeper aspects of the paper's methodology and contribution.\n```\n\n```\n**Communication Clarity - Evidence Analysis:**\n- From Assistant A:\n  - Clarity: \"The most pressing limitation is the practical applicability of the COMM method due to the large visual feature length.\" (Review) - Clear and direct statement of a key weakness.\n  - Clarity: \"This limitation is explicitly stated in the paper, where the authors mention the concatenation of features and the resulting token sequence (lines 296-298).\" (Review) - Specific reference to paper location for evidence.\n  - Clarity: \"Furthermore, the reliance on two distinct visual encoders introduces a computational overhead...\" (Review) - Clear articulation of another weakness.\n- From Assistant B:\n  - Clarity: \"First, the novelty of the proposed method is somewhat limited.\" (Review) - Clear and direct statement of a key weakness.\n  - Clarity: \"While the authors combine CLIP and DINOv2, the core idea of merging multiple features from different encoders is not entirely new, as similar approaches have been explored in models like Qwen-VL.\" (Review) - Clear explanation and example to support the point about limited novelty.\n  - Clarity: \"Second, the paper lacks a comprehensive comparison with other state-of-the-art MLLMs.\" (Review) - Clear articulation of another weakness.\n- Comparative Assessment:\n  - Both assistants communicate their points with excellent clarity. They use direct and concise language, clearly state their critiques and suggestions, and provide specific examples and references to support their points. Both reviews are easy to understand and follow.\n\n**Communication Clarity - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate excellent communication clarity. They articulate their points directly and concisely, providing clear explanations, specific examples, and references to the paper to support their critiques and suggestions. Both reviews are well-structured and easy to understand.\n**Better Assistant:** Tie\n- Tie: Both assistants exhibit equally strong communication clarity by clearly and effectively conveying their critiques and suggestions in a well-structured and easily understandable manner.\n\n```\n**Communication Clarity - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate excellent communication clarity. They articulate their points directly and concisely, providing clear explanations, specific examples, and references to the paper to support their critiques and suggestions. Both reviews are well-structured and easy to understand.\n**Better Assistant:** Tie\n- Tie: Both assistants exhibit equally strong communication clarity by clearly and effectively conveying their critiques and suggestions in a well-structured and easily understandable manner.\n```\n\n```\n**Comprehensive Analysis:**\nBoth Assistant A and Assistant B provide strong and insightful reviews of the paper. They both identify key strengths and weaknesses, offer constructive suggestions, and ask relevant questions.  Assistant A emphasizes practical limitations and theoretical gaps, focusing on the feature vector size and lack of theoretical grounding. Assistant B emphasizes limitations in novelty, comparative evaluation, computational cost analysis, and potential redundancy.  Both reviews are well-structured, clearly written, and demonstrate a thorough understanding of the paper.  The strengths of Assistant A lie in its deep dive into the practical implications of the architecture and the theoretical underpinnings. The strengths of Assistant B lie in its detailed critique of the evaluation and comparative analysis, and its focus on novelty and redundancy.  Neither review is definitively superior across all aspects; they are both strong and complementary.\n\n**Overall Judgment:**\n**Evidence-Based Reason:** Both Assistant A and Assistant B provide reviews of comparable high quality. They both demonstrate strong technical accuracy, offer valuable constructive feedback, exhibit analytical depth in their critiques, and communicate their points with excellent clarity. While they focus on slightly different sets of weaknesses, both reviews are equally insightful and contribute significantly to a comprehensive evaluation of the paper.  Neither review is clearly superior; they are both excellent and provide complementary perspectives.\n**Better Assistant:** Tie\n- If Overall Tie: Both reviews are comparable in overall quality because they both provide strong, insightful, and complementary critiques, demonstrating a thorough understanding of the paper and offering valuable feedback for improvement.\n```"}
{"id": "tc1jaWpi7M", "year": 2024, "paper_context": "\\title{Completing Visual Objects via Bridging Generation and Segmentation}\n\n\\begin{abstract}\n\nThis paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named \\textbf{\\textsl{MaskComp}}, delineates the completion process through iterative stages of generation and segmentation. \nIn each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images.\nWe demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, \\eg, ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.\n\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\n\\def\\tabdesignchoice#1{\n\\begin{table*}[#1]\n\t\\centering\n \t\\subfloat[\n\t\\textbf{Segmentation model}.\n\t\\label{tab:segm model}\n\t]{\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|ccc}\n                        Model & CLIPSeg & SEEM & SAM \\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 19.9 & 18.1 & 16.9\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t\\subfloat[\n\t\\textbf{IMD step number}.\n\t\\label{tab:step number}\n\t]{\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccccc}\n        $T$ & 1 & 3 & 5 & 7 \\\\ [.1em]\n        \\shline\n        FID & 24.7 & 19.4 & 16.9 & 16.1  \\\\\n    \\end{tabular}\n\\end{center}}\\end{minipage}\n}\n\\subfloat[\n\t\\textbf{\\# of sampled images}.\n\t\\label{tab:image number}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccc}\n        N & 4 & 5 & 6 \\\\ [.1em]\n        \\shline\n    FID & 17.4 & 16.9 & 16.8 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n    \\subfloat[\n\t\\textbf{Condition gating}.\n\t\\label{tab:gating}\n\t]{\n\t\t\\begin{minipage}{0.2\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        Gating & \\Checkmark & \\XSolidBrush \\\\ [.1em]\n                        \\shline\n\t\t\t\t\t\tFID & 16.9 & 18.2\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t\\caption{\\textbf{Design choices for IMD on AHP dataset.} We ablate (a) the impact of different segmentation networks, (b) IMD step number, (c) the number of sampled images in the segmentation stage, and (d) the gating operation in the CompNet.}\n\t\\label{tab:design choices}\n\\end{table*}}\n\\def\\tableablation#1{\n\\begin{table*}[#1]\n\t\\centering\n\t\\subfloat[\n\t\\textbf{Conditioned mask}.\n\t\\label{tab:conditioned mask}\n\t]{\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccccc}\n        Mask & Partial & Intermed. & Complete \\\\ [.1em]\n        \\shline\n        FID & 16.9 & 15.3 & 12.7   \\\\\n    \\end{tabular}\n\\end{center}}\\end{minipage}\n}\n\t\\subfloat[\n\t\\textbf{Occlusion rate}.\n\t\\label{tab:occ}\n\t]{\n\t\t\\begin{minipage}{0.27\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        Occ. & 20\\% & 40 \\% & 60 \\% & 80\\%\\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 13.4 & 15.7 & 17.2 & 29.9\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n \\subfloat[\n\t\\textbf{Inference time}.\n\t\\label{tab:time}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.2\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccc}\n        Comp. & Gen. & Segm. & Total \\\\ [.1em]\n        \\shline\n    Second &  14.3 & 1.2 & 15.5 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n  \\subfloat[\n\t\\textbf{Amodal baseline}.\n\t\\label{tab:amodal}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.25\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccc}\n        Model & Baseline & MaskComp \\\\ [.1em]\n        \\shline\n    FID & 29.4 & 16.9 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t\\caption{\\textbf{Ablation of MaskComp on AHP dataset}. We ablate (a) the different conditioning masks during inference, (b) the occlusion rate during inference, (c) the inference time of each component in an IMD step, and (d) the performance compared with the amodal baseline.}\n\t\\label{tab:ablation}\n\\end{table*}}\n\\def\\tablemoreablation#1{\n\\begin{table*}[#1]\n\t\\centering\n\t\\subfloat[\n\t\\textbf{Iteration for diffusion}.\n\t\\label{tab:iter diff}\n\t]{\n\t\t\\begin{minipage}{0.3\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        Iter & 20 & 40 & 50 \\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 16.9 & 15.7 & 15.1\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t \\subfloat[\n\t\\textbf{Occlusion type}.\n\t\\label{tab:occ type}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.3\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|ccc}\n        Occ. & Rectangle & Oval & Object \\\\ [.1em]\n        \\shline\n    FID & 15.3 & 15.1 & 16.9 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n \\subfloat[\n\t\\textbf{Availablility of complete object}.\n\t\\label{tab:complete data}\n\t]{\n\t\t\\centering\n\t\t\\begin{minipage}{0.3\\linewidth}{\\begin{center}\n    \\tablestyle{1.5pt}{1.2}\n    \\begin{tabular}{l|cc}\n        Comp. & \\Checkmark & \\XSolidBrush \\\\[.1em]\n        \\shline\n    FID & 16.9 & 19.4 \\\\\n    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\\\\\n \\subfloat[\n\t\\textbf{Voting strategies}.\n\t\\label{tab:voting}\n\t]{\n\t\t\\begin{minipage}{0.45\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        Strategy & Logits (V) & Logits (M)  & Mask (V)  & Mask (M)\\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 16.9 & 17.2 & 17.6 & 17.0\\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n  \\subfloat[\n\t\\textbf{Mask loss}.\n\t\\label{tab:mask loss}\n\t]{\n\t\t\\begin{minipage}{0.3\\linewidth}{\\begin{center}\n\t\t\t\t\t\\tablestyle{1.5pt}{1.2}\n\t\t\t\t\t\\begin{tabular}{l|cccc}\n                        $\\mathcal{L}_{mask}$ & \\Checkmark & \\XSolidBrush \\\\ [.1em]\n                        \\shline\n\t\t\t\t\tFID & 16.9 & 17.7 \\\\\n                    \\end{tabular}\n\t\t\\end{center}}\\end{minipage}\n\t}\n\t\\caption{\\textbf{More ablation of MaskComp.} We report the performance with the AHP dataset. (a) We ablate the iteration number of the diffusion model. (b) We report the performance with different types of occlusion. (c) We report the performance of MaskComp trained with or without the complete objects. (d) We ablate voting strategies. V: voting. M: Mean. (e) We ablate the effectiveness of adding intermediate supervision to predict the complete mask.}\n\t\\label{tab:more ablation}\n\\end{table*}}\n\n\\begin{figure*}[t]\n    \\centering    \n    \\includegraphics[width=\\linewidth]{figs/teaser.pdf}\n    \\caption{\\textbf{Illustration of iterative mask denoising (IMD).} Starting from an initial partial object and its corresponding mask, IMD utilizes alternating generation and segmentation stages to progressively refine the partial mask until it converges to the complete mask. With the complete mask as the condition, the final complete object can be seamlessly generated.\n    }\n    \\label{fig:teaser}\n\\end{figure*}\n\n\n\nIn recent years, creative image editing has attracted substantial attention and seen significant advancements. Recent breakthroughs in image generation techniques have delivered impressive results across various image editing tasks, including image inpainting \\citep{Xie_2023_CVPR}, composition \\citep{yang2023paint} and colorization \\citep{Chang_2023_CVPR}. However, another intriguing challenge lies in the domain of object completion (\\cref{fig:teaser0}). This task involves the restoration of partially occluded objects within an image, representing the image as a layered stack of objects and background, which can potentially enable a number of more complicated editing tasks such as object layer switching. Unlike other conditional generation tasks, \\eg, image inpainting, which only generates and integrates complete objects into images, object completion requires seamless alignment between the generated content and the given partial object, which imposes more challenges to recover realistic and comprehensive object shapes.\n \nTo guide the generative model in producing images according to a specific shape, additional conditions can be incorporated \\citep{Koley_2023_CVPR,Yang_2023_CVPR}. Image segmentation has been shown to be a critical technique for enhancing the realism and stability of generative models by providing pixel-level guidance during the synthesis process. Recent research, as exemplified in the latest study by Zhang et al. \\citep{zhang2023adding}, showcases that, by supplying object segmentations as additional high-quality masks for shaping the objects, it becomes possible to generate complex images of remarkable fidelity.\nIn this paper, we present MaskComp, a novel approach that bridges image generation and segmentation for effective object completion. MaskComp is rooted in a fundamental observation: the quality of the resulting image in the mask-conditioned generation is directly influenced by the quality of the conditioned mask \\citep{zhang2023adding}. That says the more detailed the conditioned mask, the more realistic the generated image. \nBased on this observation, unlike prior object completion methods that solely rely on partially visible objects for generating complete objects, MaskComp introduces an additional mask condition combined with an iterative mask denoising (IMD) process, progressively refining the incomplete mask to provide comprehensive shape guidance to object completion.\n\nOur approach formulates the partial mask as a noisy form of the complete mask and the IMD process is designed to iteratively denoise this noisy partial mask, eventually leading to the attainment of the complete mask. As illustrated in \\cref{fig:teaser}, each IMD step comprises two crucial stages: generation and segmentation. The generation stage's objective is to produce complete object images conditioning on the visible portion of the target object and an object mask. Meanwhile, the segmentation stage is geared towards segmenting the object mask within the generated images and aggregating these segmented masks to obtain a superior mask that serves as the condition for the subsequent IMD step. By seamlessly integrating the generation and segmentation stages, we demonstrate that each IMD step effectively operates as a mask-denoising mechanism, taking a partially observed mask as input and yielding a progressively more complete mask as output. Consequently, through this iterative mask denoising process, the originally incomplete mask evolves into a satisfactory complete object mask, enabling the generation of complete objects guided by this refined mask.\n\nThe effectiveness of MaskComp is demonstrated by its capacity to address scenarios involving heavily occluded objects and its ability to generate realistic object representations through the utilization of mask guidance. In contrast to recent progress in the field of image generation research, our contributions can be succinctly outlined as follows:\n\n\\begin{itemize}\n    \\item We explore and unveil the benefits of incorporating object masks into the object completion task. A novel approach, MaskComp, is proposed to seamlessly bridge the generation and segmentation.\n    \n    \\item We formulate the partial mask as a form of noisy complete mask and introduce an iterative mask denoising (IMD) process, consisting of alternating generation and segmentation stages, to refine the object mask and thus improve the object completion. \n    \n    \\item We conduct extensive experiments for analysis and comparison, the results of which indicate the strength and robustness of MaskComp against previous methods, \\eg, Stable Diffusion. \n\\end{itemize}\n\n\n\\section{Related Works}\n\n\\noindent\\textbf{Conditional image generation.}\nConditional image generation \\cite{lee2022autoregressive,gafni2022make,li2023gligen} involves the process of creating images based on specific conditions. These conditions can take various forms, such as layout \\citep{li2020bachgan,sun2019image,zhao2019image}, sketch \\citep{Koley_2023_CVPR}, or semantic masks \\citep{gu2019mask}. For instance, Cascaded Diffusion Models \\citep{ho2022cascaded} utilize ImageNet class labels as conditions, employing a two-stage pipeline of multiple diffusion models to generate high-resolution images. Meanwhile, in the work by \\citep{sehwag2022generating}, diffusion models are guided to produce novel images from low-density regions within the data manifold. Another noteworthy approach is CLIP \\citep{radford2021clip}, which has gained widespread adoption in guiding image generation in GANs using text prompts \\citep{galatolo2021generating, gal2022stylegan, zhou2021lafite}. In the realm of diffusion models, Semantic Diffusion Guidance \\citep{liu2023more} explores a unified framework for diffusion-based image generation with language, image, or multi-modal conditions. Dhariwal et al. \\citep{dhariwal2021diffusion} employ an ablated diffusion model that utilizes the gradients of a classifier to guide the diffusion process, balancing diversity and fidelity. Furthermore, Ho et al. \\citep{ho2022classifier} introduce classifier-free guidance in conditional diffusion models, incorporating score estimates from both a conditional diffusion model and a jointly trained unconditional diffusion model. \n\n\\noindent\\textbf{Object segmentation.}\nIn the realm of segmentation, traditional approaches have traditionally leaned on domain-specific network architectures to tackle various segmentation tasks, including semantic, instance, and panoptic segmentation \\citep{fcn, deeplabv1, mask-rcnn, spatial-instance, associate-instance, solo, panoptic-deeplab, max-deeplab, li2023towards,li2023panoramic,li2023robust,li2023rethinking,li2023paintseg,li2022video,li2022hybrid}. However, recent strides in transformer-based methodologies, have highlighted the effectiveness of treating these tasks as mask classification challenges \\citep{maskformer, knet, mask2former, detr}. MaskFormer \\citep{maskformer} and its enhanced variant \\citep{mask2former} have introduced transformer-based architectures, coupling each mask prediction with a learnable query. Unlike prior techniques that learn semantic labels at the pixel level, they directly link semantic labels with mask predictions through query-based prediction. Notably, the Segment Anything Model (SAM) \\citep{kirillov2023segment} represents a cutting-edge segmentation model that accommodates diverse visual and textual cues for zero-shot object segmentation. Similarly, SEEM \\citep{zou2023segment} is another universal segmentation model that extends its capabilities to include object referencing through audio and scribble inputs. By leveraging those foundation segmentation models, \\eg, SAM and SEEM, a number of downstream tasks can be boosted \\citep{ma2023segment,cen2023segment,yu2023inpaint}.\n\n\n\n\\section{MaskComp}\n\n\\label{sec:IMD}\n\n\\subsection{Problem Definition and Key Insight}\nWe address the object completion task, wherein the objective is to predict the image of a complete object $I_c\\in\\mathbb{R}^{3\\times H\\times W}$, based on its visible (non-occluded) part $I_p\\in\\mathbb{R}^{3\\times H\\times W}$. \n\nWe first discuss the high-level idea of the proposed \\textbf{I}terative \\textbf{M}ask \\textbf{D}enoising (IMD) and then illustrate the module details in \\cref{sec:object completion} and \\cref{sec:object segmentation}. The core of IMD is based on an essential observation: In the mask-conditioned generation, the quality of the generated object is intricately tied to the quality of the conditioned mask. As shown in \\cref{fig:mask_condition}, we visualize the completion result of the same partial object but with different conditioning masks. We notice a more complete object mask condition will result in a more complete and realistic object image. \nBased on this observation, high-quality occluded object completion can be achieved by providing a complete object mask as the condition.\n\n\\begin{figure}[t]\n\\includegraphics[width=\\linewidth]{figs/mask_condition.pdf}\n\\caption{Object completion with different mask conditions.}\n\\label{fig:mask_condition}\n\\end{figure}\n\n\\begin{figure*}[t]\n    \\centering    \n    \\includegraphics[width=\\linewidth]{figs/pipeline.pdf}\n    \\caption{\\textbf{Illustration of CompNet (generation stage of MaskComp).} The CompNet aims to recover the complete object $I_c$ from the partial object $I_p$ and a mask $M$. An object encoder is utilized to extract partial token $c_p$ which is gated and fed to the ControlNet to form the complete token $c_c$. The complete token $c_c$ serves as the condition to the diffusion U-Net to guide the conditional denoising process. In addition, a pre-diffusion mask is predicted from the partial token to encourage the object encoder to capture shape information.}\n    \\label{fig:pipeline}\n\\end{figure*}\n\\subsection{Iterative Mask Denoising}\nHowever, in real-world scenarios, the complete object mask is not available. To address this problem, we propose the IMD process which leverages intertwined generation and segmentation processes to approach the partial mask to the complete mask gradually. \nGiven a partially visible object $I_p$ and its corresponding partial mask $M_p$, the conventional object completion task aims to find a generative model $\\mathcal{G}$ such that $I_c\\leftarrow\\mathcal{G}(I_p)$, where $I_c$ is the complete object. Here, we additionally add the partial mask $M_p$ to the condition  $I_c\\leftarrow\\mathcal{G}(I_p, M_p)$, where $M_p$ can be assumed as an addition of the complete mask and a noise $M_p=M_c+\\Delta$. By introducing a segmentation model $\\mathcal{S}$, we can find a mask denoiser $\\mathcal{S}\\circ\\mathcal{G}$ from the object completion model:\n\\begin{equation}\nM_c\\leftarrow\\mathcal{S}\\circ\\mathcal{G}(I_p, M_c+\\Delta)\n\\label{equ:mask denoiser}\n\\end{equation}\nwhere $M_c=\\mathcal{S}(I_c)$.\nStarting from the visible mask $M_0=M_p$, as shown in \\cref{fig:teaser}, we repeatedly apply the mask denoiser $\\mathcal{S}\\circ\\mathcal{G}$ to gradually approach the visible mask $M_p$ to complete mask $M_c$. In each step, the input mask is denoised with a stack of generation and segmentation stages. Specifically, as the $\\mathcal{S}\\circ\\mathcal{G}(\\cdot)$ includes a generative process, we can obtain a set of estimations of denoised mask $\\{M_t^{(k)}\\}$. Here, we utilize a function $\\mathcal{V}(\\cdot)$ to find a more complete and reasonable mask from the $N$ sampled masks and leverage it as the input mask for the next iteration to further denoise. The updating rule can be written as:\n\\begin{equation}\nM_{t}^{(k)}=\\mathcal{S}\\circ\\mathcal{G}(I_p,\\hat{M}_{t-1}),\\,\\,\\,\n\\hat{M_t}=\\mathcal{V}(M_t^{(1)},\\cdots,M_t^{(N)})\n\\end{equation}\nwhere $N$ is the number of sampled images in each iteration. With a satisfactory complete mask $\\hat{M}_T$ after $T$ iterations, the object completion can be achieved accordingly by $\\mathcal{G}(I_p,\\hat{M}_T)$. The mathematical explanation of the process will be discussed in \\cref{sec:discussion}. \n\n\n\\subsection{Generation Stage}\n\\label{sec:object completion}\nWe introduce \\textbf{\\textsl{CompNet}} as the generative model $\\mathcal{G}$ which aims to recover complete objects based on partial conditions. We build CompNet based on popular ControlNet \\cite{zhang2023adding} while making fundamental modifications to enable object completion. As shown in \\cref{tab:form diff}, the target of ControlNet is to generate images strictly based on the given conditions, \\ie, $I_p\\leftarrow\\mathcal{G}(I_p,M_p)$, making it unable to complete object. Differently, CompNet is designed to recover the object. With a segmentation network, it can act as a mask denoiser to refine the conditioned mask, \\ie, $M_c\\leftarrow\\mathcal{S}\\circ\\mathcal{G}(I_p,M_p)$.\n\n\\begin{table}[t]\n    \\centering\n    \\scalebox{0.77}{\n    \\begin{tabular}{c|ccc}\n    \\hline\n    Method  & Objective & Objective with Segm. &  Object Comp.\\\\\n    \\hline\n    ControlNet & $I_p\\leftarrow\\mathcal{G}(I_p,M_p)$ & $M_p\\leftarrow\\mathcal{S}\\circ\\mathcal{G}(I_p,M_p)$ & \\textcolor{red}{\\XSolidBrush}\\\\\n    CompNet & $I_c\\leftarrow\\mathcal{G}(I_p,M_p)$ & $M_c\\leftarrow\\mathcal{S}\\circ\\mathcal{G}(I_p,M_p)$ & \\textcolor{mygreen}{\\Checkmark}\\\\\n    \\hline\n    \\end{tabular}}\n    \\caption{Objective difference with ControlNet.}\n    \\label{tab:form diff}\n\\end{table}\n\n\\noindent\\textbf{Mask condition.}\nAs illustrated on the left side of \\cref{fig:pipeline}, we begin with a complete object $I_c$ and its corresponding mask $M_c$. Our approach commences by occluding the complete object, retaining only the partially visible portion as $I_p$. Recall that the mask-denoising procedure initiates with the partial mask $M_p$ and culminates with the complete mask $M_c$. To facilitate this iterative denoising, the model must effectively handle any mask that falls within the interpolation between the initial partial mask and the target complete mask. Consequently, we introduce a mask $M$ with an occlusion rate positioned between the partial and complete masks as a conditioning factor for the generative model. The partial mask $M_p$ is achieved by considering the occluded areas in both occlusion processes. The interpolated mask $M$ is generated by using one of the occlusions. \n\n\\noindent\\textbf{Diffusion model.}\nDiffusion models have achieved notable progress in synthesizing unprecedented image quality and have been successfully applied to many text-based image generation works \\citep{rombach2022high,zhang2023adding}. For our object completion task, the complete object can be generated by leveraging the diffusion process.\n\nSpecifically, the diffusion model generates image latent $x$ by gradually reversing a Markov forward process. As shown in Figure~\\ref{fig:pipeline}, starting from $x_0=\\mathcal{E}(I_c)$, the forward process yields a sequence of increasing noisy tokens $\\{x_\\tau|\\tau\\in[1,T_{\\mathcal{G}}]\\}$, where $x_\\tau=\\sqrt{\\Bar{\\alpha_\\tau}}y_0+\\sqrt{1-\\Bar{\\alpha_\\tau}}\\epsilon$, $\\epsilon$ is the Gaussian noise, and $\\alpha_\\tau$ decreases with the timestep $\\tau$. For the denoising process, the diffusion model progressively denoises a noisy token from the last step given the conditions $c=(I_p,M,E)$ by minimizing the following loss function: $\\mathcal{L}=\\mathbb{E}_{\\tau,x_0,\\epsilon}\\|\\epsilon_\\theta(x_\\tau,c,\\tau)-\\epsilon\\|_2^2$. $I_p$, $M$, and $E$ are the partial object, conditioned mask, and text prompt respectively.\n\\begin{figure*}[t]\n    \\centering    \n    \\includegraphics[width=\\linewidth]{figs/mask.pdf}\n    \\caption{\n    We calculate the mask probability map by averaging and normalizing the masks of sampled images. We show a cross-section of the lower leg to better visualize (shown as \\textcolor{darkyellow}{yellow}). \n    }\n    \\label{fig:mask}\n\\end{figure*}\n\n\\noindent\\textbf{CompNet architecture.}\nPrevious work \\citep{zhang2023adding} has demonstrated an effective way to add additional control to generative diffusion models. We follow this architecture and make necessary modifications to adapt the architecture to object completion. As shown in \\cref{fig:pipeline}, given the visible object $I_p$ and the conditioning mask $M$, we first concatenate them and extract the partial token $c_p$ with an object encoder. \nDifferent from ControlNet \\citep{zhang2023adding} assuming the condition is accurate, the object completion task relies on incomplete conditions. Specifically, in the early diffusion steps, the condition information is vital to complete the object. Nevertheless, in the later steps, inaccurate information in the condition can degrade the generated object. To tackle this problem, we introduce a time-variant gating operation to adjust the importance of conditions in the diffusion steps. We learn a linear transform $f: \\mathbb{R}^{C}\\rightarrow\\mathbb{R}^{1}$ upon the time embedding $e_t\\in\\mathbb{R}^{C}$ and then apply it to the partial token as $f(e_t)\\cdot c_p$ before feeding it to the ControlNet. In this way, the importance of visible features can be adjusted as the diffusion steps forward. The time embedding used for the gating operation is shared with the time embedding for encoding the diffusion step in the stable diffusion.\n\nTo encourage the object encoder to capture shape information, we introduce an auxiliary path to predict the complete object mask from the partial token $c_p$. Specifically, a feature pyramid network \\cite{lin2017feature} is leveraged as the mask decoder which takes $c_p$ and the multi-scale features from the object encoder as input and outputs a pre-diffusion mask $M_{pre}$. We encourage mask completion with supervision as\n\\begin{equation}\n    \\mathcal{L}_{mask}=\\mathcal{L}_{dice}(M_c,M_{pre})+\\lambda_{ce}\\mathcal{L}_{ce}(M_c,M_{pre})\n\\end{equation}\nwhere $\\mathcal{L}_{dice}$ and $\\mathcal{L}{ce}$ are Dice loss \\cite{li2019dice} and BCE loss respectively. $\\lambda_{ce}$ is a constant.\n\n\\subsection{Segmentation Stage}\n\\label{sec:object segmentation}\nIn the segmentation stage, illustrated in \\cref{fig:mask} (a), our approach initiates by sampling $N$ images denoted as $\\{I_t^{(k)}\\}^N_{k=1}$ from the generative model, where $t$ is the IMD step. Subsequently, we employ an off-the-shelf object segmentation model denoted as $\\mathcal{S}(\\cdot)$ to obtain the shapes (object masks) $\\{M_t^{(k)}\\}$ from these sampled images.\n\nTo derive an improved mask for the subsequent IMD step, we seek a function $\\mathcal{V}(\\cdot)$ that can produce a high-quality mask prediction from the set of $N$ generated masks. Interestingly, though the distribution of sampled images is complex, we notice the distribution of masks has good properties.\nIn \\cref{fig:mask} (b), we provide a visualization of the probability map associated with a set of object masks with the same conditions, which is computed by taking the normalized average of the masks.\nTo enhance the visualization of this probability distribution, we focus on a specific cross-section of the fully occluded portion in image $I_p$ (the lower leg, represented as a \\textcolor{darkyellow}{yellow} section) and visualize the probability as a function of the horizontal coordinate which demonstrates an obvious unimodal and symmetric property. Leveraging this observation, we can find an improved mask by taking the high-probability region. \nThe updating can be achieved by conducting a voting process across the $N$ estimated masks, as defined by the following equation:\n\\begin{equation}\n    \\hat{M}_t[i,j] = \\begin{cases}\n                1, & \\text{if}\\quad \\frac{\\sum_{k=1}^NM_t^{(k)}[i,j]}{N} \\geq \\tau \\\\\n                0, & \\text{otherwise}\n        \\end{cases}\n\\end{equation}\nwhere $[i,j]$ denotes the coordinate, and $\\tau$ is the threshold employed for the mask voting process.\n\n\n\n\n\n\\begin{figure}[t]\n\\centering   \n\\includegraphics[width=\\linewidth]{figs/dist.pdf}\n\\caption{\\textbf{Mutual-benificial sampling.}}\n\\label{fig:mutual}\n\\end{figure}\n\n\\begin{table*}[t]\n\\centering\n\\scalebox{0.9}{\n\\begin{tabular}{l|p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}|p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}} \n\\hline\n\\multirow{2}*{Method} & \\multicolumn{4}{c|}{AHP \\citep{zhou2021human}} &\n\\multicolumn{4}{c}{DYCE \\citep{ehsani2018segan}}\\\\\n\\cline{2-9}\n~ & FID-G $\\downarrow$ & FID-S $\\downarrow$ & Rank $\\downarrow$ & Best $\\uparrow$ & FID-G $\\downarrow$ & FID-S $\\downarrow$ & Rank $\\downarrow$ & Best $\\uparrow$\\\\\n\\hline\nControlNet & 40.2 & 45.4 & 3.4 & 0.10 & 42.4 & 49.4 & 3.4 & 0.08 \\\\\nKandinsky 2.1 & 43.9 & 39.2 & 3.2 & 0.11 & 44.3 & 47.7 &  3.4 & 0.06 \\\\\nStable Diffusion 1.5 & 35.7 & 41.4 & 3.2 & 0.12 & 31.2 & 43.4 & 3.4 & 0.11 \\\\\nStable Diffusion 2.1 & 30.8 & 39.9 & 3.1 & 0.14 & 30.0 & 41.1 & 3.0 & 0.12 \\\\\n\\bf MaskComp (Ours) & \\bf16.9 & \\bf21.3 &\\bf2.1 & \\bf0.53 & \\bf 20.0 & \\bf25.4 & \\bf1.9 & \\bf0.63  \\\\\n\\hline\n\\end{tabular}}\n\\caption{\\textbf{Quantitative evaluation on object completion task}. The computing of FID-G and FID-S only considers the object areas within ground truth and foreground regions segmented by SAM, respectively, to eliminate the influence of the generated background. The Rank denotes the average ranking in the user study. The Best denotes the percentage of samples that are ranked as the best. $\\downarrow$ and $\\uparrow$ denote the smaller the better and the larger the better respectively.}\n\\label{tab:main results}\n\\end{table*}\n\n\\subsection{Discussion}\n\\label{sec:discussion}\nIn this section, we will omit the conditioned partial image $I_p$ for simplicity.\n\n\\noindent\\textbf{Joint modeling of mask and object.}\nIn practical scenarios where the complete object mask $M_c$ is unavailable, modeling object completion through a marginal probability $p(I_c|M_c)$ becomes infeasible. Instead, it necessitates the more challenging joint modeling of objects and masks, denoted as $p(I,M)$, where the images and masks can range from partial to complete. Let us understand the joint distribution by exploring its marginals. Since the relation between mask and image is one-to-many (each object image only has one mask while the same mask can be segmented from multiple images), the $p(M|I)$ is actually a Dirac delta distribution $\\delta$ and only the $p(I|M)$ is a real distribution. This way, the joint distribution of mask and image is discrete and complex, making the modeling difficult. To address this issue, we introduce a slack condition to the joint distribution $p(I,M)$ that \\textit{the mask and image can follow a many-to-many relation}, which makes its marginal $p(M|I)$ a real distribution and permits $p(I|M)$ to predict image $I$ that has a different shape as the conditioned $M$ and vice versa. \n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/compare.pdf}\n    \\caption{\\textbf{Qualitative comparison against ControlNet, Kandinsky and Stable Diffusion}. The background is filtered out for better visualization.}\n    \\label{fig:compare}\n\\end{figure*}\n\n\\noindent\\textbf{Mutual-beneficial sampling.}\nAfter discussing the joint distribution that we are targeting, we introduce the mathematical explanation of MaskComp. MaskComp introduces the alternating modeling of two marginal distributions $p(I|M)$ (generation stage) and $p(M|I)$ (segmentation stage), which is actually a Markov Chain Monte Carlo-like (MCMC-like) process and more specifically Gibbs sampling-like. It samples the joint distribution $p(I,M)$ by iterative sampling from the marginal distributions. Two core insights are incorporated in MaskComp: (1) providing a mask as a condition can effectively enhance object generation and (2) fusing the mask of generated object images can result in a more accurate and complete object mask. Based on these insights, we train CompNet to maximize $p(I|M)$ and leverage mask voting to maximize the $p(M|I)$. As shown in \\cref{fig:mutual}, MaskComp develops a mutual-beneficial sampling process from the joint distribution $p(I,M)$, where the object mask is provided to boost the image generation and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. Through alternating sampling from the marginal distributions, we can effectively address the object completion task.\n\n\n\n\\section{Experiment}\n\n\\subsection{Experimental Settings}\n\\noindent\\textbf{Dataset.}\nWe evaluate MaskComp on two popular datasets: AHP \\citep{zhou2021human} and DYCE \\citep{ehsani2018segan}. \nAHP is an amodal human perception dataset with 56,302 images with annotations of integrated humans.\nDYCE is a synthetic dataset with photo-realistic images and the natural configuration of objects in indoor scenes.\nFor both datasets, the non-occluded object and its corresponding mask for each object are available. We train MaskComp on the AHP and a filtered subset of OpenImage v6 \\citep{kuznetsova2020open}. OpenImage is a large-scale dataset offering heterogeneous annotations. We select a subset of OpenImage that contains 429,358 objects as a training set of MaskComp.\n\\tableablation{t}\n\n\\noindent\\textbf{Evaluation metrics.}\nIn accordance with previous methods \\citep{zhou2021human}, we evaluate image generation quality Fréchet Inception Distance (FID). The background is removed with object masks before evaluation. \nAs the FID score cannot reflect the object completeness, we further conduct a user study, leveraging human assessment to compare the quality and completeness of images. During the assessment, given a partial object, the participants are required to rank the generated object from different methods based on their completeness and quality.\n\n\\noindent\\textbf{Implementation details.}\nFor the generation stage, we train the CompNet with frozen Stable Diffusion \\citep{rombach2022high} on the AHP dataset for 50 epochs. The learning rate is set for 1e-5. We adopt $\\mathrm{batchsize}=8$ and an Adam \\citep{loshchilov2017adamw} optimizer. \nThe image is resized to $512\\times 512$ for both training and inference. \nThe object is cropped and resized to have the longest side 360 before sticking on the image. \nFor a more generalized setting, we train the CompNet on a subset of the OpenImage \\citep{kuznetsova2020open} dataset for 36 epochs. We generate text prompts using BLIP \\citep{li2022blip} for all experiments (prompts are necessary to train ControlNet). For the segmentation stage, we leverage SAM \\citep{kirillov2023segment} as $\\mathcal{S}(\\cdot)$. We vote mask with a threshold of $\\tau=0.5$. During inference, if no other specification, we conduct the IMD process for 5 steps with $N=5$ images for each step. We give the class label as the text prompt to facilitate the CompNet to effectively generate objects. All baseline methods are given the same text prompts during the experiments. The code will be made publicly available.\n\n\\subsection{Main Results}\n\n\\noindent\\textbf{Quantitative results.}\nWe compare the MaskComp with state-of-the-art methods, ControlNet \\citep{zhang2023adding}, Kandinsky 2.1 \\citep{kandinsky2}, Stable Diffusion 1.5 \\citep{rombach2022high} and Stable Diffusion 2.1 \\citep{rombach2022high} on AHP \\citep{zhou2021human} and DYCE \\citep{ehsani2018segan} dataset. The results in \\cref{tab:main results} indicate that MaskComp consistently outperforms other methods, as evidenced by its notably lower FID scores, signifying the superior quality of its generated content. We conducted a user study to evaluate object completeness in which participants ranked images generated by different approaches. MaskComp achieved an impressive average ranking of 2.1 and 1.9 on the AHP and DYCE datasets respectively. Furthermore, MaskComp also generates the highest number of images ranked as the most complete and realistic compared to previous methods. We consider the introduced mask condition and the proposed IMD process benefits the performance of MaskComp, where the additional conditioned mask provides robust shape guidance to the generation process and the proposed iterative mask denoising process refines the initial conditioned mask to a more complete shape, further enhancing the generated image quality.   \n\n\\noindent\\textbf{Qualitative results.}\nWe present visual comparisons among ControlNet, Kandinsky 2.1, Stable Diffusion 1.5, and Stable Diffusion 2.1, illustrated in \\cref{fig:compare}. Our visualizations showcase MaskComp's ability to produce realistic and complete object images given partial images as the condition, whereas previous approaches exhibit noticeable artifacts and struggle to achieve realistic object completion. In addition, without mask guidance, it is common for previous methods to generate images that fail to align with the partial object.\n\n\\begin{figure*}[t]\n    \\centering    \n    \\includegraphics[width=\\linewidth]{figs/vis_imd.pdf}\n    \\caption{\\textbf{Visualization of the IMD process.} For each step, we randomly demonstrate one generated image and the averaged mask for all generated images. We omit the input mask which has the same shape as the input occluded object.\n    }\n    \\label{fig:vis_imd}\n\\end{figure*}\n\n\\tabdesignchoice{t}\n\n\\subsection{Analysis}\n\\label{sec:analysis}\nIn this section, we provide an experimental analysis of MaskComp. All the results are evaluated with GT masks to filter out the background, \\ie, FID-G.\n\n\\noindent\\textbf{Performance with different mask conditions.}\nWe evaluated the quality of generated images when conditioned on the same partial images along with three distinct types of masks: (1) partial mask (mask of the partial image), (2) intermediate mask (less occlusion than partial), and (3) complete mask. As shown in \\cref{tab:conditioned mask}, the model achieves its highest performance when it is conditioned with complete object masks, whereas relying solely on partial masks yields less optimal results. These results provide strong evidence that the quality of the conditioned mask significantly influences the quality of the generated images.\n\n\\noindent\\textbf{Performance with different occlusion rates.}\nWe perform ablation studies to assess the resilience of MaskComp under varying occlusion levels. As presented in \\cref{tab:occ}, we evaluate MaskComp at different occlusion levels (proportion of the obscured area relative to the complete object) ranging from 20\\% to 80\\%, and the results indicate that its performance does not degrade significantly up to 60\\% occlusion.\n\n\n\\noindent\\textbf{Inference time.} \\cref{tab:time} reports the inference time of each component in IMD (with a single NVIDIA V100 GPU). Although MaskComp's throughput is reduced due to the inclusion of multiple diffusion processes in each IMD step, it is capable of attaining a higher degree of accuracy in visual object completion. Based on our empirical experiments, reducing the number of diffusion steps during the first few IMD steps can increase model speed without sacrificing much performance. With this idea incorporated into MaskComp, the average running time could be reduced to 2/3 of the original time with FID slightly increasing by 0.50. While beyond the scope of this study, we expect more advanced techniques could be explored to optimize the tradeoff between model speed and performance.\n\n\\noindent\\textbf{Comparison to amodal segmentation baseline.}\nAmodal segmentation has a similar objective to the proposed IMD process. To demonstrate the effectiveness of MaskComp, we construct an amodal baseline that generates amodal masks from the SOTA amodal segmentation method \\cite{tran2022aisformer} and then utilize ControlNet to generate images based on the amodal masks. s shown in \\cref{tab:amodal}, we notice that our method outperforms the amodal baseline by a considerable margin, which could be attributed to the strong mask completion capability of the proposed IMD process.\n\n\\begin{table}[t]\n    \\centering\n    \\scalebox{0.95}{\n    \\begin{tabular}{c|ccccc}\n        Noise degree & Iter. 1 & Iter. 3 & Iter. 5 & Iter. 7 & Iter. 9 \\\\\n        \\toprule\n        15\\% area & 28.4 & 22.7 & 18.9 & 17.2 & 16.5 \\\\\n        10\\% area & 26.4 & 21.4 & 18.1 & 17.0 & 16.4 \\\\\n        5\\% area & 24.9 & 19.6 & 17.0 & 16.2 & 16.0 \\\\\n        No noise & 24.7 & 19.4 & 16.9 & 16.1 & 15.9 \n    \\end{tabular}}\n    \\caption{Performance against segmentation errors on AHP dataset.}\n    \\label{tab:robust}\n\\end{table}\n\n\\noindent\\textbf{Impact of different segmentation networks.} \nWe adopt SAM to obtain object masks at the segmentation stage. To study the impacts of different segmenters, we replace SAM with two smaller segmentation networks, CLIPSeg \\cite{luddecke2022image} and SEEM \\cite{zou2023segment}. \\cref{tab:segm model} shows that the FID score with CLIPSeg (19.9) is slightly higher than with SAM (16.9), but remains competitive against other state-of-the-art methods, e.g., Stable Diffusion 2.1 (30.8 reported in \\cref{tab:main results}). MaskComp is an iterative mask denoising (IMD) process that progressively refines a partial object mask to boost image generation. The results support our hypothesis that the impact of the segmenter is modest.\n\n\\noindent\\textbf{Design choices in IMD.}\nWe conduct experiments to ablate the design choices in IMD and their impacts on the completion performance. We first study the effect of IMD step number. With a larger step number, IMD can better advance the partial mask to the complete mask. As shown in \\cref{tab:step number}, we notice that the image quality keeps increasing and slows down at a step number of 5. In this way, we choose 5 as our IMD step number. After that, we ablate the number of sampled images in the segmentation stage in \\cref{tab:image number}. We notice more sampled images generally leading to a better performance. We leverage an image number of 5 with the efficiency consideration. As we leverage the diffusion-based method for image generation, we ablate the iterations for the diffusion process. As shown in \\cref{tab:gating}, we notice the gating operation improves the generation quality by 1.3 FID, indicating the necessity of conditional gating.\n\\noindent\\textbf{Robustness to segmentation errors.} \nWe conduct experiments to manually add random errors to masks. As shown in \\cref{tab:robust}, we ablate on the number of iterations and the degree of segmentation error. We observe that segmentation errors will increase the convergence iteration number while not affecting the final performance significantly. As IMD is a reciprocal process intended to provide effective control for later-generated masks to be refined based on adaptive feedback, mask errors are mitigated and not propagated.\n\\noindent\\textbf{Visualization of iterative mask denoising.}\nTo provide a clearer depiction of the IMD process, as depicted in \\cref{fig:vis_imd}, we present visualizations of the generated image and the averaged mask for each step. In the initial step, we observe the emergence of artifacts alongside the object. As we progress through the steps, both the image and mask quality exhibit continuous improvement.\n\n\n\n\n\\section{Conclusion}\n\nIn this paper, we introduce MaskComp, a novel approach for object completion. MaskComp addresses the object completion task by seamlessly integrating conditional generation and segmentation, capitalizing on the crucial observation that the quality of generated objects is intricately tied to the quality of the conditioned masks. We augment the object completion process with an additional mask condition and propose an iterative mask denoising (IMD) process. This iterative approach gradually refines the partial object mask, ultimately leading to the generation of satisfactory objects by leveraging the complete mask as a guiding condition. Our extensive experiments demonstrate the robustness and effectiveness of MaskComp, particularly in challenging scenarios involving heavily occluded objects.\n\n\n\n\n\n\n\n\n", "DeepReviewer": "\n## Summary:\n\nThis paper introduces MaskComp, a novel approach to object completion that iteratively refines object masks through alternating stages of image generation and segmentation. The core idea is to leverage the synergy between these two processes, where the generation stage, conditioned on a partial object and its mask, produces a completed object, and the segmentation stage, applied to the generated image, provides a more accurate mask. This iterative process, termed Iterative Mask Denoising (IMD), progressively improves the mask quality, leading to more precise object completion. The method builds upon ControlNet, a conditional image generation model, and utilizes a pre-trained Stable Diffusion model for the generative component. For the segmentation stage, MaskComp employs the Segment Anything Model (SAM). The authors demonstrate the effectiveness of MaskComp on two datasets, AHP and DYCE, showing superior performance compared to existing methods like ControlNet and Stable Diffusion. The method's key contribution lies in its iterative refinement strategy, which effectively combines generation and segmentation to achieve high-quality object completion. The paper presents a clear methodology, supported by experimental results, and highlights the potential of iterative approaches for addressing complex image editing tasks. However, the paper also acknowledges certain limitations, such as the computational cost of the iterative process and the reliance on paired data for training. Overall, the paper presents a valuable contribution to the field of object completion, offering a novel and effective approach that combines generation and segmentation in a synergistic manner. The results are compelling, but the paper also identifies areas for future research, such as improving the efficiency of the method and reducing its dependence on paired training data. The authors also suggest that the method could be used for other editing tasks such as object layer switching, which could be a promising direction for future work.\n\n\n## Soundness:\n\n3.0\n\n\n## Presentation:\n\n3.0\n\n\n## Contribution:\n\n2.5\n\n\n## Strengths:\n\nI find the core strength of this paper to be the innovative approach of combining image generation and segmentation in an iterative manner for object completion. The proposed Iterative Mask Denoising (IMD) process is a novel and effective way to progressively refine object masks, leading to superior completion results. The idea of using the generated image to improve the mask, and then using the improved mask to generate a better image, is a clever way to leverage the strengths of both generative and segmentation models. The paper clearly articulates this methodology, making it easy to understand the core contribution. The experimental results, which demonstrate the superiority of MaskComp over existing methods like ControlNet and Stable Diffusion, provide strong empirical support for the proposed approach. The authors have conducted sufficient experiments on two datasets, AHP and DYCE, to validate their claims. The paper is also well-written and easy to follow, which enhances its accessibility and impact. The authors have clearly explained the motivation behind their work, the details of their method, and the results of their experiments. The use of ControlNet as a base model, while also a weakness, is also a strength in that it provides a strong foundation for the proposed method. The authors have made fundamental modifications to ControlNet to enable object completion, which demonstrates their understanding of the underlying techniques. The use of Stable Diffusion as the generative model also contributes to the strength of the method, as it is a powerful and widely used model. The authors have also clearly stated the limitations of their method, which is a sign of good scientific practice. Overall, I believe the paper presents a significant contribution to the field of object completion, offering a novel and effective approach that combines generation and segmentation in a synergistic manner. The results are compelling, and the paper is well-written and easy to follow. The authors have clearly articulated their contribution and have provided strong empirical support for their claims.\n\n\n## Weaknesses:\n\nAfter a thorough examination of the paper, I have identified several key weaknesses that warrant careful consideration. First, the proposed method's efficiency is a significant concern. The iterative nature of the IMD process, which involves alternating between generation and segmentation stages, introduces a substantial computational overhead. As the paper states, \"The IMD process is repeated for a fixed number of steps (T), with each step involving the generation and segmentation stages.\" (Method -> Iterative Mask Denoising (IMD) Process). The experimental section further reveals that, during inference, the IMD process is conducted for 5 steps, with 5 images generated for each step. The inference time for each component per IMD step is 14.3 seconds for generation and 1.2 seconds for segmentation, resulting in a total of 15.5 seconds per step. This means that the total inference time for the entire process is 77.5 seconds per image. This computational cost is substantial and could limit the practical application of the method in real-time scenarios or when processing large datasets. This is a valid concern, and I have high confidence in this assessment, based on the explicit description of the iterative process and the provided inference times. Second, the method's reliance on paired data for training is a significant limitation. While the paper does not explicitly state that paired data is required, the problem formulation and the training process strongly suggest that paired or near-paired data is used. The paper states, \"We address the object completion task, wherein the objective is to predict the image of a complete object $I_c$ based on its visible (non-occluded) part $I_p$.\" (Method -> Problem Definition and Key Insight). This implies a mapping from partial to complete objects, which typically requires paired data. Furthermore, the training is conducted on the AHP dataset and a filtered subset of OpenImage, both of which likely contain paired or near-paired information. This reliance on paired data restricts the applicability of the method in real-world scenarios where such data is often unavailable. This is a partially valid concern, and I have medium confidence in this assessment, based on the problem formulation and the training datasets used. Third, the method's dependence on a pre-trained object completion method, specifically ControlNet, raises concerns about the fairness of comparisons with methods that do not use such priors. The paper states, \"We introduce \\textbf{\\textsl{CompNet}} as the generative model $\\mathcal{G}$ which aims to recover complete objects based on partial conditions. We build CompNet based on popular ControlNet \\cite{zhang2023adding} while making fundamental modifications to enable object completion.\" (Method -> Generation Stage). The performance of MaskComp is therefore influenced by the quality of the base model, Stable Diffusion, which is used in a frozen state. The comparison with methods that are not based on ControlNet, such as Kandinsky 2.1 and Stable Diffusion, may not be entirely fair, as MaskComp benefits from the pre-training of ControlNet. This is a valid concern, and I have high confidence in this assessment, based on the explicit statement about building CompNet on ControlNet. Fourth, the method's requirement for ground-truth text prompts is a significant limitation for real-world applications. The paper mentions using BLIP to generate text prompts for training ControlNet, stating, \"For a more generalized setting, we train the CompNet on a subset of the OpenImage \\citep{kuznetsova2020open} dataset for 36 epochs. We generate text prompts using BLIP \\citep{li2022blip} for all experiments (prompts are necessary to train ControlNet).\" (Experiments -> Implementation details). The fact that all baseline methods are given the same text prompts during experiments further supports the claim that the method relies on these prompts. While the method description itself doesn't explicitly mention text prompts as input to the generation stage, the reliance on ControlNet and Stable Diffusion, which are text-conditional, makes this a valid concern. Obtaining such prompts is not always feasible in real-world scenarios, limiting the method's applicability. This is a valid concern, and I have high confidence in this assessment, based on the mention of using BLIP for prompt generation and the statement about prompts being necessary for training ControlNet. Finally, the experimental evaluation is limited by the choice of baselines. The paper compares MaskComp with ControlNet, Kandinsky 2.1, and Stable Diffusion, which are not specifically designed for object completion. The lack of comparisons with recent object completion methods is a significant weakness, as it does not fully demonstrate the method's superiority over state-of-the-art techniques in this specific domain. This is a valid concern, and I have high confidence in this assessment, based on the list of baselines in the experimental section and the absence of comparisons with dedicated object completion methods. Additionally, the evaluation is primarily focused on human-related datasets and relatively simple objects, which does not fully demonstrate the method's robustness and scalability. Testing on more intricate objects with varying shapes, textures, and occlusion patterns is necessary to validate its broader applicability. This is a partially valid concern, and I have medium confidence in this assessment, based on the description of the datasets used and the visual examples provided in the paper.\n\n\n## Suggestions:\n\nBased on the identified weaknesses, I propose several concrete and actionable improvements. First, to address the efficiency issue, future work should explore techniques to reduce the number of iterations required for convergence. This could involve incorporating more sophisticated mask refinement strategies or using more efficient generative and segmentation models. For example, instead of a fixed number of iterations, the process could terminate when the change in mask quality between iterations falls below a certain threshold. Additionally, investigating the possibility of parallelizing the generation and segmentation steps could also lead to significant speedups. The current approach processes each stage sequentially, which may not be optimal. Exploring methods to perform these steps concurrently or with minimal dependencies could lead to significant speedups. Second, to mitigate the reliance on paired data, future research should explore methods to reduce or eliminate this requirement. This could involve exploring self-supervised learning methods or leveraging existing large-scale datasets with unpaired data. Techniques like cycle-consistency or adversarial training could be explored to learn the mapping between partial and complete objects without requiring explicit paired examples. Furthermore, investigating the use of synthetic data or data augmentation techniques could also help to mitigate the need for large amounts of real-world paired data. The authors should also consider exploring the use of pre-trained models or transfer learning to reduce the need for large datasets. Third, to address the dependence on ControlNet, future work should explore methods to reduce the reliance on this specific pre-trained model. This could involve exploring alternative base models or developing a method that is less dependent on a specific pre-trained architecture. A more rigorous evaluation would involve comparing MaskComp against methods that start from a similar baseline, or alternatively, providing a detailed ablation study that isolates the contribution of the IMD pipeline from the base model. Furthermore, the paper should explore the sensitivity of MaskComp to the choice of the base model. For example, how would MaskComp perform with different base models, and what are the trade-offs in terms of computational cost and performance? This would provide a more comprehensive understanding of the method's robustness and generalizability. Fourth, to address the requirement for ground-truth text prompts, the authors should acknowledge that obtaining such prompts is not always feasible and explore alternative approaches to address this issue. For instance, the authors could investigate the use of automatically generated text prompts or explore methods that do not rely on text prompts at all. The paper should also discuss the impact of inaccurate or misleading text prompts on the performance of MaskComp. How does the method behave when the provided text prompt does not accurately describe the object in the image? This analysis would provide a more realistic assessment of the method's applicability in practical scenarios. Additionally, the authors could consider incorporating techniques for prompt refinement or correction to mitigate the impact of noisy prompts. Finally, to improve the evaluation, the authors should include comparisons with more recent and specialized object completion methods. This would provide a more comprehensive evaluation of MaskComp's performance and demonstrate its advantages over state-of-the-art object completion techniques. The authors should also consider using a wider range of evaluation metrics that are specifically designed for object completion, such as metrics that measure the accuracy of the completed object's shape and appearance. This would provide a more detailed and nuanced assessment of the method's performance. The evaluation should also be expanded to include more complex objects and scenes. This could involve using datasets with a wider variety of object categories, including those with intricate shapes, textures, and occlusion patterns. The authors should also consider evaluating the method's performance on scenes with multiple objects and varying levels of occlusion. This would provide a more comprehensive assessment of the method's robustness and scalability. Additionally, the authors should analyze the failure cases of the method and discuss the limitations of the current approach. This would help in identifying areas for future improvement and provide a more balanced view of the method's capabilities.\n\n\n## Questions:\n\nBased on my analysis, I have several questions that I believe are crucial for a deeper understanding of the method. First, what is the efficiency of the proposed method in terms of processing time per image, and how does this compare to existing methods? While the paper provides the inference time per IMD step, a more direct comparison with other methods would be beneficial. Second, how does the proposed method perform on other datasets, such as the dataset used in ControlNet, or other datasets specifically designed for object completion? This would provide a more comprehensive evaluation of the method's generalizability. Third, how does the proposed method handle cases where the partial object is heavily occluded or has significant variations in appearance compared to the complete object? Are there any limitations in terms of the types of objects or scenes that the method can handle effectively? Fourth, what are the specific conditions under which the iterative refinement process fails to converge to a good solution? Are there specific types of objects or occlusions that are particularly challenging for the method? A more thorough analysis of these failure cases would provide valuable insights into the strengths and weaknesses of the proposed approach. Fifth, the paper mentions that the proposed method can be used for other editing tasks such as object layer switching. Can the authors provide more details on how this can be achieved and show some examples of such applications? This would demonstrate the versatility of the method and its potential for broader impact. Finally, how does the computational cost of the proposed method compare to existing methods? Are there any potential optimizations that could be made to improve its efficiency? This is a critical question, given the iterative nature of the method. These questions target key uncertainties and methodological choices, seeking clarification of critical assumptions and limitations.\n\n\n## Rating:\n\n5.5\n\n\n## Confidence:\n\n3.5\n\n\n## Decision:\n\nReject", "other": "\n## Summary:\n\nThis paper introduces MaskComp, a novel approach to object completion that iteratively refines object masks through alternating stages of generation and segmentation. The core idea is that the quality of mask-conditioned generation is heavily influenced by the quality of the conditioned mask. Therefore, MaskComp starts with a partial object mask and iteratively improves it, using the refined mask to guide the generation of a more complete object. The process begins with a partial object and its mask, which are fed into a generative model. The generated image is then segmented to produce an improved mask, and this process is repeated. The authors demonstrate that this iterative combination of generation and segmentation effectively functions as a mask denoiser. The method leverages a pre-trained Stable Diffusion model for image generation and an off-the-shelf segmentation model, specifically the Segment Anything Model (SAM), for mask refinement. The authors conduct experiments on two popular datasets, AHP and DYCE, and compare MaskComp against several state-of-the-art methods, including ControlNet and Stable Diffusion. The results show that MaskComp achieves better performance in terms of image quality and object completeness. The paper also includes ablation studies to analyze the impact of different segmentation networks and the robustness of the method to segmentation errors and varying occlusion rates. The authors claim that their method is effective for object completion and has the potential to be applied to other tasks. Overall, the paper presents a technically sound and novel approach to object completion, with promising empirical results. However, there are some limitations that need to be addressed, such as the reliance on an off-the-shelf segmentation model and the lack of a detailed analysis of the computational cost. The paper is well-written and easy to follow, and the figures and tables are helpful in understanding the results. The experiments are comprehensive and convincing, and the authors provide clear explanations of the method and the experiments. The main contribution of the paper is the iterative mask denoising approach, which is simple yet effective, and the idea of iteratively refining the mask through alternating generation and segmentation stages is interesting and has the potential to be applied to other tasks. The paper also provides a detailed analysis of the impact of different segmentation networks on the performance of MaskComp, which is a valuable contribution to the field. The authors also explore the robustness of the method to segmentation errors and varying occlusion rates, which is important for real-world applications. The paper is a valuable contribution to the field of object completion and has the potential to be applied to other tasks.\n\n\n## Soundness:\n\n2.75\n\n\n## Presentation:\n\n2.75\n\n\n## Contribution:\n\n2.75\n\n\n## Strengths:\n\nI find several aspects of this paper to be particularly strong. The core idea of iteratively refining object masks through alternating generation and segmentation is both novel and technically sound. This approach, which the authors term iterative mask denoising (IMD), is a clever way to leverage the strengths of both generative and segmentation models. The method is relatively simple to implement, yet it yields impressive results, demonstrating the effectiveness of the approach. The use of a pre-trained Stable Diffusion model for image generation and an off-the-shelf segmentation model (SAM) makes the method accessible and easy to reproduce. The experimental results are compelling, showing that MaskComp outperforms several state-of-the-art methods on two popular datasets. The quantitative evaluations, using metrics like FID, are well-designed and comprehensive, providing strong evidence for the effectiveness of the proposed method. The qualitative results are also convincing, with visual examples that clearly demonstrate the ability of MaskComp to complete objects more accurately than other methods. Furthermore, the ablation studies are well-executed and provide valuable insights into the impact of different design choices. The authors explore the impact of different segmentation networks, the robustness of the method to segmentation errors, and the performance under varying occlusion rates. These analyses demonstrate the robustness of the method and provide a deeper understanding of its behavior. The paper is also well-written and easy to follow, with clear explanations of the method and the experiments. The figures and tables are well-designed and helpful in understanding the results. Overall, the paper presents a strong contribution to the field of object completion, with a novel and effective method, compelling experimental results, and a thorough analysis of the approach. The iterative nature of the method, combined with the use of pre-trained models, makes it a practical and accessible solution for object completion tasks. The authors have successfully demonstrated the potential of iterative mask refinement for improving generative models, and this work is likely to inspire further research in this area.\n\n\n## Weaknesses:\n\nWhile I find the paper to be a strong contribution, there are several weaknesses that I have identified through my analysis. Firstly, the paper relies on an off-the-shelf segmentation model, specifically SAM, which introduces a dependency on the accuracy of this external model. While the paper does include an ablation study comparing SAM with other segmentation models like CLIPSeg and SEEM, this analysis is not sufficiently detailed to fully address the potential limitations. The paper does not explore the impact of segmentation errors on the iterative refinement process. Although the authors show that MaskComp is robust to some segmentation errors, they do not analyze how errors in segmentation propagate through the iterative process and how this impacts the final object completion quality. This is a critical oversight, as the iterative nature of the method means that errors in segmentation could potentially be amplified in subsequent iterations. The paper also lacks a detailed analysis of the computational cost of MaskComp. While the authors provide the inference time for each component, they do not delve into the time complexity or memory requirements of the method. The iterative nature of the process, involving both generation and segmentation steps, suggests that it could be computationally expensive, and the paper does not provide a detailed analysis of the time spent in each stage. This makes it difficult to assess the practicality of the method for real-world applications. Furthermore, the paper does not explore the potential of applying MaskComp to other tasks beyond object completion. While the authors mention that the method has the potential to be applied to other tasks, they do not provide any concrete examples or experiments to support this claim. This limits the impact of the paper, as the method could potentially be more widely applicable. The paper also lacks a clear definition of the problem it aims to address. While the authors define the input as the partially visible object, they do not explicitly state whether the background is part of the input or the expected output. The focus seems to be on the object itself, but this could be made clearer. Additionally, the paper does not adequately address how the proposed method handles complex backgrounds. Object completion can be a challenging task, especially when the background is intricate or contains patterns similar to the object itself. The paper lacks a discussion of the potential limitations and failure modes of the method in such scenarios. While the paper shows results on datasets with varying backgrounds, there's no specific analysis or discussion about how the method handles complex backgrounds or potential failure modes related to background complexity. Finally, the paper does not provide a detailed analysis of the computational cost of MaskComp. While the authors provide the inference time for each component, they do not delve into the time complexity or memory requirements of the method. The iterative nature of the process, involving both generation and segmentation steps, suggests that it could be computationally expensive, and the paper does not provide a detailed analysis of the time spent in each stage. This makes it difficult to assess the practicality of the method for real-world applications. These weaknesses, while not invalidating the core contributions of the paper, highlight areas where further research and analysis are needed to fully realize the potential of the proposed method. I have high confidence in these identified weaknesses, as they are directly supported by the paper's content and the lack of specific analyses.\n\n\n## Suggestions:\n\nBased on the weaknesses I have identified, I recommend several concrete improvements for this paper. Firstly, the authors should conduct a more thorough analysis of the impact of the segmentation model's accuracy on the overall performance of MaskComp. This should include experiments using different segmentation models with varying levels of accuracy, and a quantitative analysis of how these differences affect the final object completion quality. Specifically, the authors should explore how errors in segmentation propagate through the iterative process and how this impacts the final result. This analysis should also consider the robustness of the method to segmentation failures, and whether the iterative process can recover from initial segmentation errors. The authors should also investigate the sensitivity of the method to different segmentation model architectures and training data. Furthermore, the paper needs a detailed analysis of the computational cost of MaskComp. This should include a breakdown of the time spent in each stage of the iterative process, as well as an analysis of the memory requirements. The authors should also compare the computational cost of MaskComp to other object completion methods. This analysis should also consider the scalability of the method to larger images and more complex scenes. The authors should also explore potential optimizations to reduce the computational cost of the method, such as using more efficient generation and segmentation models, or reducing the number of iterations required for convergence. A discussion of the trade-off between computational cost and performance would also be beneficial. Additionally, the authors should explore the potential of applying MaskComp to other tasks beyond object completion. This could include tasks such as image inpainting, scene completion, or 3D object reconstruction. The authors should provide concrete examples and experiments to demonstrate the applicability of the method to these other tasks. This would significantly broaden the impact of the paper and highlight the versatility of the proposed approach. The authors should also discuss the potential challenges and limitations of applying MaskComp to these other tasks, and how these challenges could be addressed. The authors should also provide a more precise explanation of the object completion task, particularly regarding the input and expected output. This should include a clear explanation of whether the input consists solely of the object and its mask, or if the background is also considered. Furthermore, the authors need to specify the expected behavior of the model when dealing with heavily occluded objects or noisy masks. For example, they could define the method's behavior in cases where only 20% of the object is visible, or when the input mask has a certain percentage of error. It would also be beneficial to discuss the method's limitations and provide examples of scenarios where the method might fail. This would help to set realistic expectations for the performance of the proposed approach. The authors should also clarify whether the goal is to recover the original object or to generate a plausible completion, and how this is handled in the training process. To improve the discussion of the method's performance with complex backgrounds, the authors should include a more thorough analysis of how the method handles different types of backgrounds. This could involve testing the method on a variety of datasets with varying background complexities, and reporting the results separately for each type. The authors should also discuss the potential for the method to confuse object boundaries with similar patterns in the background, and how this issue is addressed. For example, they could analyze the impact of background complexity on the quality of the completed object, and provide examples of both successful and unsuccessful cases. It would be beneficial to include a quantitative analysis of the method's performance on images with complex backgrounds, such as those with cluttered scenes or textures similar to the object. This would provide a more comprehensive understanding of the method's strengths and limitations. Finally, the authors should consider including a more detailed analysis of the computational cost of the iterative mask denoising process. While the method is described as simple, the iterative nature of the process may introduce significant computational overhead. The authors should provide a quantitative analysis of the time and memory requirements of the method, and compare it to other object completion techniques. This would help to evaluate the practical applicability of the method in real-world scenarios. Furthermore, the authors could explore potential optimizations to reduce the computational cost of the method, such as using more efficient segmentation models or reducing the number of iterations. This would make the method more accessible and useful for a wider range of applications.\n\n\n## Questions:\n\nBased on my analysis, I have several questions that I believe are important for further understanding and development of this work. Firstly, how does the choice of segmentation model affect the overall performance of MaskComp? While the paper includes an ablation study comparing different segmentation models, I am curious if the authors have explored the impact of using segmentation models with significantly different architectures or training data. Specifically, have they considered using segmentation models that are trained on datasets that are different from the object completion datasets used in this paper? This would provide a more comprehensive understanding of the method's robustness to segmentation errors. Secondly, what is the computational cost of MaskComp compared to other object completion methods? The paper provides the inference time for each component, but I am interested in a more detailed analysis of the time complexity and memory requirements of the method. How does the computational cost scale with the size of the input image and the number of iterations? Is the method scalable to larger images and more complex scenes? Thirdly, can MaskComp be applied to other tasks beyond object completion? While the authors mention that the method has the potential to be applied to other tasks, I am curious about the specific challenges and limitations of applying MaskComp to tasks such as image inpainting, scene completion, or 3D object reconstruction. What modifications would be necessary to adapt the method to these other tasks? Fourthly, what is the impact of the initial mask quality on the performance of MaskComp? The paper explores the robustness to segmentation errors, but I am curious about the impact of the initial mask quality on the convergence of the iterative process. How does the method perform when the initial mask is significantly different from the ground truth mask? Finally, how does the method handle complex backgrounds? The paper shows results on datasets with varying background complexities, but I am curious about the potential limitations and failure modes of the method in such scenarios. How does the method perform when the background contains textures or colors that closely resemble the object? These questions are crucial for a deeper understanding of the method's capabilities and limitations, and I believe that addressing them would significantly strengthen the paper.\n\n\n## Rating:\n\n5.75\n\n\n## Confidence:\n\n4.0\n\n\n## Decision:\n\nAccept", "other_type": "DeepReviewer_Standard", "v.s.": "A", "result": "```\n**Technical Accuracy - Evidence Analysis:**\n- From Assistant A:\n  [Direct quotes and specific examples]\n  - Assistant A claims: \"The iterative nature of the IMD process, which involves alternating between generation and segmentation stages, introduces a substantial computational overhead.\"\n  - Evidence from Paper: \"Inference time... Gen. 14.3, Segm. 1.2, Total 15.5 Second\" and \"During inference, if no other specification, we conduct the IMD process for 5 steps with N=5 images for each step.\"\n  - Analysis: Assistant A accurately points out the computational cost by referencing the iterative nature and providing evidence from the paper's inference time analysis. The calculation of 77.5 seconds (15.5s * 5 steps) is also correct based on the paper's description of 5 IMD steps.\n\n  - Assistant A claims: \"Second, the method's reliance on paired data for training is a significant limitation... This implies a mapping from partial to complete objects, which typically requires paired data.\"\n  - Evidence from Paper: \"We address the object completion task, wherein the objective is to predict the image of a complete object $I_c$ based on its visible (non-occluded) part $I_p$.\" and \"We train MaskComp on the AHP and a filtered subset of OpenImage v6.\"\n  - Analysis: Assistant A infers the need for paired data from the problem definition and training datasets. While the paper doesn't explicitly state \"paired data\", the task of predicting a complete object from a partial one strongly suggests a need for corresponding pairs during training. This is a reasonable and technically accurate inference.\n\n  - Assistant A claims: \"Third, the method's dependence on a pre-trained object completion method, specifically ControlNet, raises concerns about the fairness of comparisons...\"\n  - Evidence from Paper: \"We build CompNet based on popular ControlNet \\cite{zhang2023adding} while making fundamental modifications to enable object completion.\"\n  - Analysis: Assistant A correctly identifies the dependency on ControlNet and raises a valid point about the fairness of comparisons. The paper explicitly states building upon ControlNet.\n\n  - Assistant A claims: \"Fourth, the method's requirement for ground-truth text prompts is a significant limitation for real-world applications... We generate text prompts using BLIP \\citep{li2022blip} for all experiments (prompts are necessary to train ControlNet).\"\n  - Evidence from Paper: \"We generate text prompts using BLIP \\citep{li2022blip} for all experiments (prompts are necessary to train ControlNet).\" and \"All baseline methods are given the same text prompts during the experiments.\"\n  - Analysis: Assistant A accurately points out the use of text prompts and their necessity for ControlNet, which is confirmed by the paper.\n\n  - Assistant A claims: \"Finally, the experimental evaluation is limited by the choice of baselines... The paper compares MaskComp with ControlNet, Kandinsky 2.1, and Stable Diffusion, which are not specifically designed for object completion.\"\n  - Evidence from Paper: Table 1 lists baselines as \"ControlNet, Kandinsky 2.1, Stable Diffusion 1.5, Stable Diffusion 2.1\".\n  - Analysis: Assistant A correctly identifies the baselines used and accurately points out that they are not specifically designed for object completion, which is a valid technical critique of the evaluation.\n\n- From Assistant B:\n  [Direct quotes and specific examples]\n  - Assistant B claims: \"Firstly, the paper relies on an off-the-shelf segmentation model, specifically SAM, which introduces a dependency on the accuracy of this external model.\"\n  - Evidence from Paper: \"For the segmentation stage, we leverage SAM \\citep{kirillov2023segment} as $\\mathcal{S}(\\cdot)$.\" and Table 1a \"Model & CLIPSeg & SEEM & SAM\".\n  - Analysis: Assistant B correctly identifies the use of SAM and points out the dependency on its accuracy. The paper does use SAM and also explores other segmentation models in ablation.\n\n  - Assistant B claims: \"The paper does not explore the impact of segmentation errors on the iterative refinement process... Although the authors show that MaskComp is robust to some segmentation errors, they do not analyze how errors in segmentation propagate through the iterative process...\"\n  - Evidence from Paper: Table 5 \"Robustness to segmentation errors\" and \"We observe that segmentation errors will increase the convergence iteration number while not affecting the final performance significantly. As IMD is a reciprocal process intended to provide effective control for later-generated masks to be refined based on adaptive feedback, mask errors are mitigated and not propagated.\"\n  - Analysis: Assistant B's claim is partially inaccurate. The paper *does* explore robustness to segmentation errors in Table 5 and discusses the mitigation of errors due to the iterative process. However, Assistant B is correct that the analysis might not be deeply detailed in terms of error propagation *mechanisms*.\n\n  - Assistant B claims: \"The paper also lacks a detailed analysis of the computational cost of MaskComp... While the authors provide the inference time for each component, they do not delve into the time complexity or memory requirements of the method.\"\n  - Evidence from Paper: Table 2c \"Inference time... Gen. 14.3, Segm. 1.2, Total 15.5 Second\".\n  - Analysis: Assistant B is correct that the paper provides inference time but lacks a deeper analysis of time complexity and memory requirements.\n\n  - Assistant B claims: \"Furthermore, the paper does not explore the potential of applying MaskComp to other tasks beyond object completion. While the authors mention that the method has the potential to be applied to other tasks, they do not provide any concrete examples or experiments to support this claim.\"\n  - Evidence from Paper: \"Unlike other conditional generation tasks, \\eg, image inpainting... object completion requires seamless alignment...\" and \"This task involves the restoration of partially occluded objects within an image, representing the image as a layered stack of objects and background, which can potentially enable a number of more complicated editing tasks such as object layer switching.\" and \"The authors also suggest that the method could be used for other editing tasks such as object layer switching, which could be a promising direction for future work.\" (from Assistant A's summary of the paper).\n  - Analysis: Assistant B is correct. The paper mentions potential for other tasks like object layer switching but doesn't provide experiments or detailed exploration.\n\n  - Assistant B claims: \"The paper also lacks a clear definition of the problem it aims to address. While the authors define the input as the partially visible object, they do not explicitly state whether the background is part of the input or the expected output.\"\n  - Evidence from Paper: \"We address the object completion task, wherein the objective is to predict the image of a complete object $I_c\\in\\mathbb{R}^{3\\times H\\times W}$, based on its visible (non-occluded) part $I_p\\in\\mathbb{R}^{3\\times H\\times W}$.\" and \"The background is removed with object masks before evaluation.\"\n  - Analysis: Assistant B raises a valid point about the clarity of problem definition regarding background. While the paper defines input and output objects, the role of background isn't explicitly detailed in the problem definition section, although it's mentioned in evaluation.\n\n  - Assistant B claims: \"Additionally, the paper does not adequately address how the proposed method handles complex backgrounds. Object completion can be a challenging task, especially when the background is intricate or contains patterns similar to the object itself.\"\n  - Evidence from Paper: Visual examples in Figure 4 show completions on various backgrounds, but no specific analysis on background complexity is provided.\n  - Analysis: Assistant B is correct. The paper doesn't have a dedicated section or experiment analyzing performance with varying background complexity.\n\n**Technical Accuracy - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate good technical accuracy overall. Assistant A is slightly more precise in identifying and interpreting technical details, especially regarding the baselines and the nature of the evaluation. Assistant B makes a minor inaccuracy regarding the paper's exploration of segmentation errors, although the core point about depth of analysis is still valid. Both assistants correctly identify key limitations and aspects of the method.\n**Better Assistant:** Assistant A\n\n**Constructive Value - Evidence Analysis:**\n- From Assistant A:\n  [Direct quotes and specific examples]\n  - Suggestions: \"explore techniques to reduce the number of iterations\", \"investigating the possibility of parallelizing\", \"explore self-supervised learning methods\", \"exploring alternative base models\", \"investigate the use of automatically generated text prompts\", \"include comparisons with more recent and specialized object completion methods\", \"using a wider range of evaluation metrics\", \"evaluation should also be expanded to include more complex objects and scenes\", \"analyze the failure cases\".\n  - Analysis: Assistant A provides a comprehensive list of actionable suggestions. They are specific (e.g., \"parallelizing generation and segmentation steps\"), feasible (e.g., \"explore self-supervised learning\"), and directly address the weaknesses identified (efficiency, paired data reliance, baselines, evaluation).\n\n- From Assistant B:\n  [Direct quotes and specific examples]\n  - Suggestions: \"conduct a more thorough analysis of the impact of the segmentation model's accuracy\", \"paper needs a detailed analysis of the computational cost\", \"explore the potential of applying MaskComp to other tasks\", \"provide a more precise explanation of the object completion task\", \"improve the discussion of the method's performance with complex backgrounds\", \"consider including a more detailed analysis of the computational cost of the iterative mask denoising process\".\n  - Analysis: Assistant B also provides constructive suggestions, focusing on deeper analysis and broader application. Suggestions are actionable (e.g., \"experiments using different segmentation models\"), and address identified weaknesses (segmentation dependency, computational cost, task scope, problem definition, background complexity).\n\n- Comparative Assessment:\n  Both assistants offer valuable and constructive feedback. Assistant A's suggestions are slightly more specific and diverse, covering a broader range of potential improvements, including efficiency, data dependency, baselines, and evaluation scope. Assistant B's suggestions are more focused on deeper analysis of specific aspects like segmentation impact, computational cost, and background complexity, and broadening application scope. Both are valuable, but A's are slightly more actionable in terms of immediate next steps for improvement.\n\n**Constructive Value - Judgment:**\n**Evidence-Based Reason:** Both assistants provide constructive feedback that would be helpful for the authors to improve their paper. Assistant A's suggestions are slightly more diverse and actionable, offering a broader range of concrete steps for improvement. Assistant B's suggestions are also valuable, focusing on deeper analysis and broader application, but are slightly less immediately actionable compared to A's.\n**Better Assistant:** Assistant A\n\n**Analytical Depth - Evidence Analysis:**\n- From Assistant A:\n  [Direct quotes and specific examples]\n  - Analysis: Assistant A's review demonstrates a good analytical depth by dissecting the paper into key components (efficiency, data reliance, baselines, text prompts, evaluation). For each weakness, Assistant A provides detailed reasoning and connects it back to specific parts of the paper. The review goes beyond surface-level comments and delves into the implications of methodological choices. For example, the analysis of paired data reliance and the fairness of comparisons with baselines shows a deeper understanding of the research context.\n\n- From Assistant B:\n  [Direct quotes and specific examples]\n  - Analysis: Assistant B's review also shows good analytical depth. It focuses on critical aspects like segmentation model dependency, computational cost, task scope, problem definition clarity, and background complexity. Assistant B's analysis is detailed in pointing out the lack of exploration in these areas and explaining why they are important weaknesses. For example, the detailed explanation of why segmentation error propagation is a critical oversight demonstrates analytical depth.\n\n- Comparative Assessment:\n  Both assistants demonstrate strong analytical depth. They both identify key weaknesses and provide reasoned explanations. Assistant A's analysis is slightly broader in scope, covering more distinct areas of concern. Assistant B's analysis is slightly deeper in certain areas, like the potential impact of segmentation errors and background complexity, providing more detailed reasoning within those specific areas. Both reviews go beyond simply listing weaknesses and provide substantial analytical justification.\n\n**Analytical Depth - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate a strong analytical depth, thoroughly examining key aspects of the paper and providing detailed reasoning for their critiques. Assistant A's analysis is slightly broader, covering a wider range of concerns. Assistant B's analysis is slightly deeper in specific areas, providing more detailed reasoning within those areas. Both reviews are commendably analytical.\n**Better Assistant:** Tie\n\n**Communication Clarity - Evidence Analysis:**\n- From Assistant A:\n  [Direct quotes and specific examples]\n  - Analysis: Assistant A's review is clearly written and well-organized. Points are articulated logically and are easy to follow. The use of bullet points for strengths and weaknesses, and numbered suggestions, enhances clarity. Reasoning is well-explained, and examples are effectively used to illustrate points. The language is precise and avoids ambiguity.\n\n- From Assistant B:\n  [Direct quotes and specific examples]\n  - Analysis: Assistant B's review is also clearly written and well-organized. Points are articulated clearly, and the structure is easy to follow. The use of bullet points for strengths and weaknesses, and numbered suggestions, also enhances clarity. Reasoning is well-explained, and examples are effectively used. The language is also precise and clear.\n\n- Comparative Assessment:\n  Both assistants communicate their points with excellent clarity. Both reviews are well-structured, use clear and concise language, and effectively explain their reasoning. There is no significant difference in communication clarity between the two reviews. Both are easy to read and understand.\n\n**Communication Clarity - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate excellent communication clarity. Their reviews are well-structured, clearly written, and easy to understand. Both use precise language and effectively explain their reasoning. There is no discernible difference in the clarity of communication.\n**Better Assistant:** Tie\n\n**Comprehensive Analysis:**\nAssistant A and Assistant B both provide strong and insightful reviews of the paper. Both identify key strengths and weaknesses, and offer constructive suggestions for improvement.\n\nAssistant A's review is slightly stronger in Technical Accuracy and Constructive Value. Assistant A is more precise in technical details and offers a slightly broader and more actionable set of suggestions.\n\nAssistant B's review is equally strong in Analytical Depth and Communication Clarity, and very close in the other aspects. Assistant B provides a slightly deeper analysis in certain specific areas like segmentation and background complexity.\n\nBoth reviews are well-written, well-reasoned, and demonstrate a good understanding of the paper. The differences are subtle, but Assistant A edges out slightly due to a slightly higher score in Technical Accuracy and Constructive Value.\n\n**Overall Judgment:**\n**Evidence-Based Reason:** Both assistants provide high-quality reviews that are technically accurate, constructive, analytically deep, and clearly communicated. While both are excellent, Assistant A is slightly better due to a marginally stronger performance in Technical Accuracy and offering slightly more diverse and actionable suggestions for improvement. The difference is not substantial, but based on the evidence, Assistant A is slightly more effective overall.\n**Better Assistant:** Assistant A\n```"}
{"id": "syoLhUJmth", "year": 2024, "paper_context": "\\title{From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models}\n\n\\begin{abstract}\n  Multi-modal Large Language Models (MLLMs) have made significant strides in expanding the capabilities of Large Language Models (LLMs) through the incorporation of visual perception interfaces. Despite the emergence of exciting applications and the availability of diverse instruction tuning data, existing approaches often rely on CLIP or its variants as the visual branch, and merely extract features from the deep layers. However, these methods lack a comprehensive analysis of the visual encoders in MLLMs. In this paper, we conduct an extensive investigation into the effectiveness of different vision encoders within MLLMs. Our findings reveal that the shallow layer features of CLIP offer particular advantages for fine-grained tasks such as grounding and region understanding. Surprisingly, the vision-only model DINO, which is not pretrained with text-image alignment, demonstrates promising performance as a visual branch within MLLMs. By simply equipping it with an MLP layer for alignment, DINO surpasses CLIP in fine-grained related perception tasks. Building upon these observations, we propose a simple yet effective feature merging strategy, named \\textbf{COMM}, that integrates \\textbf{C}LIP and DIN\\textbf{O} with \\textbf{M}ulti-level features \\textbf{M}erging, to enhance the visual capabilities of MLLMs. We evaluate \\textbf{COMM} through comprehensive experiments on a wide range of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination. Experimental results demonstrate the superior performance of \\textbf{COMM} compared to existing methods, showcasing its enhanced visual capabilities within MLLMs.\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\n\n\\label{sec:intro}\nLarge Language Models (LLMs)~\\cite{chatgpt, openai2023gpt4, touvron2023llama, llama2, taori2023stanford, chiangvicuna} have made significant strides in the domains of language understanding and generation, achieving remarkable progress recently. Through instruction tuning~\\cite{wei2021finetuned,wang2022self}, existing LLMs demonstrate their versatility as general-purpose models capable of handling a wide range of tasks. This capability unlocks their potential zero-shot learning ability, enabling seamless task switching guided by instructions. Building upon the promising performance of LLMs, researchers are now motivated to enhance their capabilities by incorporating visual signals as inputs. This extension allows the generation of textual outputs that are closely related to visual content, opening up exciting possibilities in the realm of vision-language understanding.\n\nTo this end, Flamingo~\\cite{alayrac2022flamingo} and BLIP2~\\cite{li2023blip2} align the powerful LLMs with a frozen visual encoder to understand visual inputs and perform various vision-language tasks. A series of following works, LLaVA~\\cite{liu2023llava}, InstructBLIP~\\cite{dai2023instructblip}, MiniGPT-4~\\cite{zhu2023minigpt} and mPLUG-OWL~\\cite{ye2023mplug} further improve the ability to follow human instructions by constructing multi-modal instruction-following datasets for training. However, these methods are built on image-level alignments, which suffer from the limited fine-grained understanding (such as region description~\\cite{liu2017referring} and reasoning~\\cite{zellers2019recognition}) and severe object hallucination problem~\\cite{li2023obj_Hallucination}. To this end, GPT4ROI~\\cite{zhang2023gpt4roi} proposes instruction tuning on region-of-interest and unlocks the region-level multimodal capacities. Kosmos-2~\\cite{peng2023kosmos} and Shikra~\\cite{chen2023shikra} further integrate the grounding abilities into LLMs and unlock the referential ability in dialogue, \\emph{i.e.}, enable the user to point to the object or region as input and the model responds with spatial coordinates of bounding boxes. Such grounding capacity can fulfill numerous vision-language tasks, which is a great progress in MLLMs.  \n\nDespite a wide variety of exciting methods and applications, most of existing multi-modal LLMs employ CLIP~\\cite{radford2021learning} or its variants ~\\cite{sun2023eva} as the visual branch, where the features output from the deep layers (\\emph{e.g.}, the penultimate layer) are usually employed as inputs to the language decoders. However, it still lacks analysis that: \\emph{Whether using the Vanilla CLIP features as visual encoder is the best way for MLLMs?} Though the visual encoder of CLIP is apparently well aligned with the word embedding space by image-text contrastive learning, it fails to learn more detailed pixel-level information such as color and positioning due to the global supervision of image captions, which might hinder the fine-grained perception ability in MLLMs. Besides, existing MLLMs have quite unbalanced visual and language encoders (\\emph{e.g.}, ViT-Large-300M vs. Vicuna-7B/13B). Since the language models have succeeded in scaling up the model size with progressively powerful language abilities, the short plate of the Buckets Effect for MLLMs lies in the visual models, which fails to demonstrate emerging capabilities, and suffer from domain gap and limited zero-shot ability. Consequently, it is critical to enhance the visual capabilities for boosting MLLMs. \n\nThis paper presents an extensive investigation into different visual encoders for MLLMs. Four typical visual foundation models are considered, \\emph{i.e.}, image-text contrastive learning CLIP, image-only contrastive learning DINOv2~\\cite{oquab2023dinov2}, masked image modeling MAE~\\cite{he2022masked} and supervised learning DeiT~\\cite{touvron2021training}. We evaluate the performance on commonly-used vision-language tasks including visual grounding, object hallucination, visual question answering, image captioning and MME benchmark. \nOur analysis reveals that different layers of features exhibit varying biases towards local and global patterns. Shallow layer features containing low-level detailed information prove beneficial for fine-grained perception tasks such as grounding and positioning ability, while deep layer features are superior at global understanding. To enhance representation, we propose a multi-level feature merging strategy that incorporates both low-level and high-level features. Surprisingly, when equipped with an MLP layer for alignment, the vision-only model DINOv2 shows promise as a visual branch for MLLMs. We attribute this to the fine-grained localization information captured by DINOv2. Conversely, MAE and DeiT perform inferiorly as visual branches for MLLMs. MAE learns limited semantic information, while DeiT's strong supervised training makes the alignment with the textual space challenging. Based on the above observations, we propose a fusion strategy that integrates \\textbf{C}LIP and DIN\\textbf{O} with \\textbf{M}ulti-level features \\textbf{M}erging), dubbed as \\textbf{COMM}, for boosting the visual branches of MLLMs.\nExperimental results demonstrate clear advantages of the proposed model over existing approaches and highlight the enhanced visual capabilities brought by \\textbf{COMM}. In a nutshell, the contributions of this paper are summarized as follows:\n\\begin{itemize}\n    \\item We are the first to extensively investigate the effectiveness of different visual encoders for MLLMs. Based on the analysis that shallow layer features contain low-level detailed information which is helpful for fine-grained tasks, we propose a multi-level feature fusion strategy to incorporate low-level and high-level features for improving representation.\n    \\item Our analysis indicates that vision-only DINOv2 achieves promising results in MLLMs with only an MLP layer for alignment. Considering fine-grained pixel information in DINOv2 and global semantic information in CLIP, we propose \\textbf{COMM} to fuse the visual embeddings of these two models to enhance visual capabilities for boosting MLLMs.\n    \\item Extensive experiments on a wide range of tasks including visual grounding, referring expression generation, object hallucination, visual question answering and image captioning demonstrate the superiority of \\textbf{COMM} over existing works.\n\\end{itemize}\n\n\n\\section{Related Work}\n\n\\textbf{Multi-modal Large Language Model.} LLMs~\\cite{dai2019transformer,brown2020language} have garnered significant attention in both academia and industry due to their remarkable understanding and generative abilities. The success of LLMs has motivated researchers to explore the integration of vision into these models, leading to the development of powerful multi-modal LLMs (MLLMs). Flamingo~\\cite{alayrac2022flamingo} employs a cross-attention module to extract visual contexts, which are concatenated with text token as input for LLMs. LLaVA~\\cite{liu2023visual} and FROMAGe~\\cite{koh2023grounding} leverage the vision encoder of CLIP to extract visual features, which is aligned to text features using a single linear layer and then input to LLMs. Models such as BLIP-2~\\cite{li2023blip}, mPLUG-OWL~\\cite{ye2023mplug}, MiniGPT-4~\\cite{zhu2023minigpt} and InstructBLIP~\\cite{dai2023instructblip} employ Q-former to extract text-aligned visual features for LLMs. Recently, some interesting works extend LLMs to image retrieval~\\cite{koh2023grounding}, video understanding~\\cite{zhang2023video}, audio~\\cite{su2023pandagpt}, biomedical analysis~\\cite{li2023llava}, control systems~\\cite{driess2023palme}. \n\nIn recent studies, there has been a growing interest in extending MLLMs to improve their fine-grained understanding abilities through region-level image-text alignment. Kosmos-2~\\cite{peng2023kosmos} addresses this by constructing a large-scale dataset of grounded region-text pairs, enabling the integration of grounding abilities into LLMs. GPT4RoI~\\cite{zhang2023gpt4roi} reformulates the bounding box as a spatial instruction format and extracts visual features based on region-of-interest, facilitating region-level multi-modal understanding. Shikra~\\cite{chen2023shikra} proposes a unified model that handles spatial coordinates to possess referential abilities in dialogue contexts. Ferret~\\cite{you2023ferret} and ViP-LLaVA~\\cite{cai2023making} further extends with a broader range of free-form shapes for referring, including points, boxes, sketches and scribbles.\nAdditionally, Qwen~\\cite{Qwen-VL} presents a set of MLLMs that demonstrate remarkable performance across various tasks. However, previous works have predominantly focused on extracting visual features solely from the last few layers of the CLIP model, resulting in an emphasis on global image properties. In this study, we draw attention to the fact that features extracted from shallower layers exhibit a stronger focus on localized properties, which we argue can be more potent in comprehending object locations and image details. Additionally, while CLIP primarily learns globally aligned features, advanced vision-alone models such as DINOv2 excel in capturing more fine-grained vision features. We posit that leveraging these fine-grained vision features can effectively enhance the capabilities of MLLMs, as demonstrated in our analysis. To further advance this line of research, we introduce a novel fusion module that expands and enhances the visual branches, thereby aiming to significantly improve the performance of MLLMs.\n\n\\textbf{Large Vision Foundation Model.} Recent progresses in training vision foundation models with large-scale image data focus on contrastive learning, masked image modeling and supervised training. For one thing, contrastive learning can be conducted in an image-only or image-text manner. DINOv2~\\cite{oquab2023dinov2} pretrains the image encoder on large curated image data, which shows a superior understanding of object parts and scene geometry across image domains. Image-text contrastive learning as CLIP~\\cite{radford2021learning} and EVA-CLIP~\\cite{sun2023eva} employs the natural language as weak supervision to guide the learning of visual features. \nFor another, BEiT~\\cite{HangboBao2021BEiT} predicts discrete tokens based on a pre-trained image tokenizer while iBOT~\\cite{zhou2021ibot} proposes an online image tokenizer. MAE~\\cite{he2022masked} proposes a masked autoencoder for reconstructing image pixels. Besides, DeiT III~\\cite{Touvron2022DeiTIR} proposes a training recipe to achieve promising performance.\nRecent MLLMs employ the vision encoder of CLIP/EVA-CLIP without considering the properties of specific visual models. \nIn this paper, we are the first to re-examine the effectiveness of existing visual models in MLLMs and propose a simple yet effective fusion strategy for boosting visual capabilities.\n\n\n\\section{Analysis of the Visual Branch in MLLMs}\n\nPrevious MLLMs~\\cite{liu2023llava,liu2023improvedllava,zhu2023minigpt,dai2023instructblip,ye2023mplug,peng2023kosmos,chen2023shikra,Qwen-VL,you2023ferret} usually utilize the vision encoder of CLIP as their visual branch. Typically, these models extract features from the last few layers, such as the penultimate layer, which are then fed into an alignment network. Subsequently, the aligned features are concatenated with text tokens to serve as input for the LLMs. While the image-text pretraining of CLIP aligns well with the language model, it primarily learns image-level features but overlooks the richer pixel-level features due to the constraint of limited fine-grained information in the guided captions. Moreover, the deep-layer features primarily focus on global image properties and inadequately explore the intricate details of local object parts. As depicted in Fig.~\\ref{fig:feature}, the visual features extracted from the shallow layers of CLIP and the deep visual features obtained from the visual-only model DINOv2 contain more detailed information regarding local objects, such as shape or texture. Leveraging these detailed features may enhance the MLLMs' fine-grained perception abilities. \n\n\\textbf{Evaluation Settings.} For further analysis, we conduct a series of quantitative experiments using different kinds of visual models, \\emph{i.e.}, image-text contrastive learning CLIP, image-only contrastive learning DINOv2, masked image modeling MAE and supervised learning DeiT. In specific, the visual features extracted from different layers of visual models (based on ViT-Large) are aligned using a linear projection layer and then concatenated with text tokens as the input for LLMs (here we use Vicuna-7B~\\cite{chiangvicuna}). The overall architecture and training process follow Shikra~\\cite{chen2023shikra} but with fewer iterations (9400 iterations, batch size 16 on 4 A800) to save the computation cost. Then, we measure the capability of the trained MLLMs on referring expression comprehension (REC)~\\cite{chen2023shikra}, referring expression generation (REG)~\\cite{peng2023kosmos} and object hallucination benchmark (POPE)~\\cite{li2023obj_Hallucination}. \nDetailed descriptions of these tasks can be referred to Sec.~\\ref{sec:experiments}.\n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=1\\textwidth]{feature.png}\n\\caption{Feature correspondence visualization by computing the cosine similarity of different visual tokens extracted from the shallow and deep layers of CLIP and DINOv2. }\\label{fig:feature}\n\\end{figure}\n\n\\textbf{CLIP as the Visual Branch of MLLMs.} As depicted in Fig.~\\ref{fig:rec}, we observe that different layers of features exhibit varying biases towards grounding and understanding abilities. For instance, the shallow features demonstrate relatively higher accuracy in terms of REC and reach their optimal value at layer 12. Conversely, the deep features achieve higher accuracy in terms of POPE, indicating superior understanding ability. Notably, the relatively deep features (layer 16) display the best REG CIDEr score, showcasing promising region understanding capabilities. Consequently, instead of solely relying on deep features as done in previous works, we argue that integrating both shallow and deep features is crucial for MLLMs with improved overall performance.\n\nWe further explore various merging modes of low-level and high-level features. Denoting the output features from each transformer layer of ViT with a depth of $N$ as $\\mathbf{z}=[ z_1,..,z_i,...,z_N ]$, we discuss several multi-level feature merging (MFM) strategies for combining shallow and deep features, namely: \n\n$\\bullet$ \\emph{Mean(half)}: averaging output patch token features in the second half of the backbone as $z=(z_{N/2}+\\cdots+z_{N})/(N/2)$. \n\n$\\bullet$ \\emph{Mean(all)}: averaging features output by all layers as $z=(z_{1}+\\cdots+z_{N})/N$. \n\n$\\bullet$ \\emph{Layerscale(all)}: learning a scale parameter as the weight to sum features output by all layers as $z=w_1z_{1}+\\cdots+w_Nz_{N}$, where $w_i$ refers to the weight assigned to the $i$-th layer feature and all these weights are dynamically updated and summed up to 1. \n\n$\\bullet$ \\emph{LLN-Layerscale(all)}: using a linear-layernorm module to align the feature space between different layers’ features and then summed by \\emph{Layerscale} as $z=w_1\\mathrm{LLN}(z_{1})+\\cdots+w_N\\mathrm{LLN}(z_{N})$.\n\n$\\bullet$ \\emph{Conv-Layerscale(all)}: using a convolution and bn module to align the feature space between different layers’ features and then summed by \\emph{Layerscale} as $z=w_1\\mathrm{Conv}(z_{1})+\\cdots+w_N\\mathrm{Conv}(z_{N})$. \n\nFig.~\\ref{fig:clipanddino} (a) and (b) shows that simply averaging all shallow and deep features of CLIP can \\emph{de facto} achieve a satisfactory accuracy and \\emph{LLN-Layerscale} strategy further improves performance. With \\emph{LLN-Layerscale} as MFM module, the performance of CLIP can be evidently improved on commonly-used vision-language tasks as shown in Table~\\ref{tab:fusion}. \n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n \\subfloat[Average REC accuracy.]{\n\\includegraphics[width=0.33\\columnwidth]{analysis_rec2.png}}\n\\subfloat[Average POPE accuracy.]{\\includegraphics[width=0.33\\columnwidth]{analysis_pope2.png}}\n\\subfloat[Average REG CIDEr.]{\\includegraphics[width=0.33\\columnwidth]{analysis_reg2.png}}\n\\caption{Average REC, POPE accuracy and REG CIDEr for using different layers of features extracted from various vision models (CLIP, DINOv2 and MAE), as input to MLLMs. Shikra uses the 23rd layer features of CLIP and we reproduce its results with fewer iterations (denoted as Shikra$^*$).}\n\\label{fig:rec}\n\\end{figure}\n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n \\subfloat[ REC acc for CLIP.]{\n\\includegraphics[width=0.26\\columnwidth]{recclip.png}}\n\\subfloat[ POPE acc for CLIP.]{\n\\includegraphics[width=0.248\\columnwidth]{popeclip.png}}\n\\subfloat[REC acc for DINO.]{\\includegraphics[width=0.247\\columnwidth,height=0.222\\columnwidth]{recdino.png}}\n\\subfloat[POPE acc for DINO.]{\\includegraphics[width=0.248\\columnwidth,height=0.222\\columnwidth]{popedino.png}}\n\\caption{Average REC and POPE accuracy for merging different layers of features with mutli-feature merging (MFM) strategies as input to MLLMs for visual backbones of CLIP and DINOv2. }\n\\label{fig:clipanddino}\n\\end{figure}\n\n\\textbf{DINOv2 as the Visual Branch of MLLMs.} \nTo leverage the rich fine-grained visual information present in DINOv2, but not inherently aligned with text, we employ a non-linear Multi-Layer Perceptron (MLP) module to align the image features with the word embedding space. Fig.~\\ref{fig:rec}  demonstrates that the deep-layer features of DINOv2 exhibit superior grounding abilities, as evidenced by higher REC accuracy, and display satisfactory understanding abilities, as indicated by favorable POPE and REG results. Additionally, we explore the efficacy of multi-level feature merging to enhance performance. In contrast to CLIP, the merging of shallow features from DINOv2 leads to a significant performance degradation. Specifically, in Fig.~\\ref{fig:clipanddino}(c) and (d), it is evident that \\emph{Mean(all)} performs notably worse than \\emph{Mean(19-24)} in terms of both REC and POPE accuracy, indicating that the shallow representations lack sufficient semantic information. Building upon the \\emph{LLN-Layerscale} approach, the incorporation of the MLP module for a more potent connection between the visual and text spaces demonstrates a clear improvement in performance. Table~\\ref{tab:fusion} showcases the substantial performance gains achieved by employing \\emph{LLN-Layerscale-MLP} as Multi-Level Feature Merging (MFM) module across various vision language tasks. Further detailed ablation studies on the MLP module are in Section~\\ref{sec:mae}. \n\\begin{table}[!t]\n\\centering\n\\setlength{\\tabcolsep}{2.8pt}\n\\caption{Comparison of the visual model using CLIP, DINOv2 with Multi-level Feature Merging (MFM) and \\textbf{COMM} to incorporate visual embeddings of both models on VL tasks. CLIP baseline use the 23rd layer features, which follows Shikra but with fewer training iterations. DINOv2 baseline is w/o MLP module.\nMME CS and PS indicate cognition and perception score, respectively.\n}\n\\label{tab:fusion}\n\\scalebox{0.82}{\n\\begin{tabular}{l|cccccccc}\n\\toprule\nVisual Model  &Avg REC& Avg POPE &COCO & Flickr30k &MME CS&MME PS   & VQAv2&OK-VQA  \\\\\n\\cmidrule(lr){1-9}\nCLIP&47.3&82.3&125.0&80.7&209.6&1107.8&68.8&44.2 \\\\\nDINOv2&54.8&78.3&118.0&68.9&261.8&930.5&63.1&41.9\\\\\n\\cmidrule(lr){1-9}\nCLIP w/ MFM&  70.0    & 83.4 & 125.8 & 81.0&296.6&1164.4 &69.5  & 44.7 \\\\\nDINOv2 w/ MFM\n& \\textbf{72.8}&83.3&123.4&76.3 &  252.9 &1086.8&68.0 & 42.1 \\\\\n \\cmidrule(lr){1-9}\n\\textbf{COMM}\n& \\textbf{72.8} & \\textbf{83.6}&\\textbf{127.3}&\\textbf{81.9}  & \\textbf{360.4}&  \\textbf{1234.9}&  \\textbf{70.1}& \t\\textbf{45.0}\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{table}\n\n\\textbf{MAE and DeiT as the Visual Branch of MLLMs.} Fig.~\\ref{fig:rec} shows that MAE features achieve acceptable REC accuracy, but suffers large performance drop on POPE and REG evaluation. This is because MAE features lack sufficient semantic information for global or regional understanding. Therefore, MAE is not suitable as the visual branch for MLLMs. DeiT performs even worse than MAE (details in Section~\\ref{sec:mae}). We speculate that this is because supervised training is too strong, which learns a specialized visual space that is difficult to align with the word embedding space.\n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=1\\columnwidth]{overall2.pdf}\n\\caption{Overview of our \\textbf{COMM}. The image is input to the vision encoder of CLIP and DINOv2, and the features from shallow and deep layers are incorporated by multi-level feature merging. The features of DINOv2 are aligned with an MLP and concatenated with features of CLIP, which are input to a linear layer. Then the fused features are concatenated with text tokens as input to LLMs.}\n\\label{fig:overall}\n\\end{figure}\n\n\n\n\\section{\\textbf{COMM}\n\n\\textbf{Architecture Overview.} In this section, we introduce the proposed \\textbf{COMM}, that integrates CLIP and DINO with Multi-level features Merging to enhance the visual capabilities of MLLMs. The overall framework is illustrated in Fig.~\\ref{fig:overall}, \\textbf{COMM} is incorporated into a vision-language instruction following model built upon the\nrecent advanced language and vision-language foundation models. Following the input instructions, our model takes vision and language as inputs to generate text responses following the input instructions.\nSpecifically, we adopt the visual encoder of CLIP and DINOv2 (based on ViT-Large) with our proposed fusion strategy as the visual branch, and Vicuna~\\cite{chiangvicuna} (7B/13B) as language decoder. The visual encoder is downsampled with rate 14, meaning that an image with resolution $H\\times W$ will be represented by a sequence of $\\frac{H}{14}\\times \\frac{W}{14}$ tokens.\nThe fused token features are projected using a linear layer and then concatenated with the instruction tokens as inputs to the language decoder, which is a generic interface to unify various vision-language tasks as text generation task.\n\nSpecifically, denote the visual encoder of CLIP and DINOv2 (ViT Large used) as $f_1$ and $f_2$, respectively. Given an input image $x$, we extract the patch token features output by all layers of CLIP as $f_1(x)=[v_1^{1},...,v_1^{i},...,v_1^{24}]$, where $v_1^i \\in \\mathbf{R}^{N\\times D}$, $N$ is the number of patch tokens and $D$ is the embedding dimension. The features output by the deep layers of DINOv2 are $f_2(x)=[v_2^{19},...,v_2^{i},...,v_2^{24}]$. Then we concatenate the features output by these two models as $\\mathbf{v}=[v_1^{1},...,v_1^{24},v_2^{19},...,v_2^{24}]$. A linear-layernorm module is employed to align the feature space between different layers’ features and layerscale is used to merge multiple layer features as\n\\begin{equation}\n    \\overline{v}_1 = \\sum_{i=1}^{24} \\alpha_i \\cdot \\mathrm{Linear}(\\mathrm{LN}(v_1^{i})), \\qquad \\overline{v}_2 = \\sum_{j=19}^{24} \\beta_j \\cdot \\mathrm{Linear}(\\mathrm{LN}(v_2^{j}))\n\\end{equation}\nwhere $\\alpha$ and $\\beta$ are the learnable scaling parameter. Then, we employ an MLP layer to project the features of DINOv2 and concatenate the output features with that of CLIP as $\\mathbf{\\overline{v}}=[\\overline{v}_1,\\mathrm{MLP}(\\overline{v}_2)]$. Then, a linear layer is employed to match the dimension of visual features to that of text features as $\\mathbf{\\hat{v}}=\\mathrm{Linear}(\\mathbf{\\overline{v}})$. Finally, fused visual features $\\mathbf{\\hat{v}}$ are concatenated with text tokens as input to LLMs.\n\n\n\\section{Experiments}\n\n\\label{sec:experiments}\nIn this section, we conduct extensive evaluation on four kinds of vision-language tasks to comprehensively evaluate the visual understanding ability of our model, namely, Referring Expression Comprehension, Referring Expression Generation, Object Hallucination Benchmark, and Visual Question Answering and Image Captioning.\n\n\\textbf{Training Details.} Similar to previous MLLM methods, \\textbf{COMM} is trained in two stages. In the first pretraining stage, we train the model on the reorganized vision-language dataset as~\\cite{chen2023shikra}, including public VQA, Image Captioning datset and several datasets containing positional annotation RefCOCO, visual gemone~\\cite{krishna2017visualgenome} and Visual-7W~\\cite{mani2020pointqa}. The first pretraining stage is conducted for 100K steps. In the second instruction tuning stage, we set the sampling ratio to 50\\% on LLaVA-Instruct-150K~\\cite{liu2023llava} and Shikra-RD~\\cite{chen2023shikra}. Instead of 224 × 224 resolution currently used by existing MLLMs, we use 336 × 336 resolution to reduce the information loss caused by image down-sampling and promote the fine-grained perception ability. In both stages, we freeze the visual encoder and tune all parameters in LLMs, alignment layer and multi-level feature fusion module. We adopt AdamW~\\cite{DBLP:conf/iclr/LoshchilovH19adamw} as the optimizer and cosine annealing\nscheduler~\\cite{DBLP:conf/iclr/LoshchilovH17cos} as learning rate scheduler with an initial learning rate of\n2e-5 and global batch size of 64. All training runs on 8 NVIDIA A800 GPUs. It takes around 100h for stage one training and 20h for stage two.\n\n\\begin{table*}[!t]\n\\centering\n\\caption{Results on standard referring expression comprehension (REC) task. Generalist VL models can perform various vision-language tasks.\nSpecialist models are designed specifically for localization tasks or generalist pretraining models that undergone finetuning. The results of Shikra, Qwen, Ferret and Griffon are from their papers.\n}\n\\label{tab:rec}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{l|l|cccccccc}\n\\toprule\n\\multirow{2}{*}{Model type}\n& \\multirow{2}{*}{Model}  & \\multicolumn{3}{c}{RefCOCO} & \\multicolumn{3}{c}{RefCOCO+} & \\multicolumn{2}{c}{RefCOCOg} \\\\\n &  & val & test-A & test-B & val & test-A & test-B & val-u & test-u \\\\\n\\cmidrule(lr){1-2}\\cmidrule(lr){3-10}\n\\multirow{9}{*}{\\tabincell{l}{Generalist VL SOTAs \\\\ (w/o finetuning)}}\n& OFA-L*      & 79.96 & 83.67 & 76.39 & 68.29 & 76.00 & 61.75 & 67.57 & 67.58  \\\\\n& VisionLLM-H   & - & 86.70     & - & - & - & - & - & - \\\\\n& Shikra-7B      & 87.01 & 90.61 & 80.24 & 81.60 & 87.36 & 72.12 & 82.27 & 82.19 \\\\\n& Shikra-13B     & 87.83 & 91.11 & 81.81 & 82.89 & 87.79 & 74.41 & 82.64 & 83.16  \\\\\n&Ferret-7B&87.49&91.35&82.45&80.78&87.38&73.14&83.93&84.76\\\\\n&Ferret-13B&89.48&92.41&84.36&82.81&88.14&75.17&85.83&86.34\\\\\n&Griffon-13B&88.00&92.10&81.90&81.50&88.20&73.30&82.90&84.30\\\\\n&Qwen-VL-7B&89.36& 92.26 &85.34 &83.12& 88.25& 77.21& 85.58& 85.48\\\\\n&Qwen-VL-7B-Chat& 88.55& 92.27& 84.51 &82.82& 88.59 &76.79& 85.96 &86.32\\\\\n& \\textbf{COMM-7B (Ours)}     &\\textbf{91.73}& \\textbf{94.06}&\\textbf{88.85}&\\textbf{87.21}&\\textbf{91.74}&\\textbf{81.39}&\\textbf{87.32}&\\textbf{88.33}\\\\\n\\cmidrule(lr){1-2}\\cmidrule(lr){3-10}\n\\multirow{3}{*}{\\tabincell{l}{Specialist SOTAs \\\\ (Specialist/Finetuned)}}\n& G-DINO-L     & 90.56 & 93.19 & 88.24 & 82.75 & 88.95 & 75.92 & 86.13 & 87.02 \\\\\n& UNINEXT-H    & 92.64 & 94.33 & 91.46 & 85.24 & 89.63 & 79.79 & 88.73 & 89.37 \\\\\n& ONE-PEACE    & 92.58 & 94.18 & 89.26 & 88.77 & 92.21 & 83.23 & 89.22 & 89.27 \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table*}\n\\begin{table*}[!t]\n\\centering\n\\setlength{\\tabcolsep}{4.5pt}\n\\caption{Results on standard referring expression generation (REG) task in CIDEr score. We reproduce the results of Shikra-7B using its officially released checkpoint. SLR is a finetuned listener-speaker model with an added reward-based module (SLR).\n}\n\\label{tab:reg}\n\\scalebox{0.95}{\n\\begin{tabular}{l|cccccccc}\n\\toprule\n \\multirow{2}{*}{Model}  & \\multicolumn{3}{c}{RefCOCO} & \\multicolumn{3}{c}{RefCOCO+} & \\multicolumn{2}{c}{RefCOCOg} \\\\\n  & val & test-A & test-B & val & test-A & test-B & val-u & test-u \\\\\n\\cmidrule(lr){1-9}\nSLR~\\cite{yu2017joint}&-&69.7&132.3&-&49.4&70.9&59.2&-\\\\\nSLR+Rerank~\\cite{yu2017joint}&-&77.5&132.0&-&52.0&73.5&66.2&-\\\\\n\\cmidrule(lr){1-9}\nShikra     &75.61&44.26&104.83&56.42& 40.98&68.25&62.71&65.58\\\\\nKosmos-2    & - & - & - & - & - & - & 62.3 & - \\\\\n\\textbf{COMM} (Ours)   &\\textbf{93.35}&\\textbf{54.95}&\\textbf{131.13}&\\textbf{70.00}&\\textbf{52.27}&\\textbf{79.05}&\\textbf{79.22}&\\textbf{77.96}\\\\ \n\\bottomrule\n\\end{tabular}}\n\\end{table*}\n\\subsection{Referring Expression Comprehension}\nTo evaluate the fine-grained understanding and positioning capability of our model, we investigate the referring expression comprehension task on benchmarks as RefCOCO~\\cite{kazemzadeh2014referitgame}, RefCOCO+~\\cite{mao2016refcocog} and RefCOCOg~\\cite{mao2016refcocog}, where models are asked to localize the object described with an expression. As shown in Table~\\ref{tab:rec}, compared with generalist VL models and previous SOTA MLLMs, \\textbf{COMM} achieves significant performance gain on all benchmarks, \\emph{i.e.}, \\textbf{COMM}-7B outperforms Shikra-13B and Qwen-VL-7B-Chat by 4.87\\% and 3.10\\% accuracy on average, respectively. With more powerful visual capabilities of our proposed fusion model, we can evidently surpass recent SOTA MLLMs in a more efficient way, \\emph{e.g.}, using a smaller LLM than Shikra (7B vs. 13B) and less training data than Qwen (3.6M vs. 1.4B). Besides, our generalist model even achieves comparable results with specialist SOTA methods, showing the superior grounding ability of our MLLMs.\n\n\\subsection{Referring Expression Generation}\nMoreover, we evaluate the ability to understand image regions or objects referred via inputting bounding boxes. Instead of referring image regions or objects via detailed text descriptions, directly referring to image regions via its bounding boxes is more effective and can reduce the ambiguity.\nThe experiments are conducted on the referring expression generation task with RefCOCO, RefCOCO+ and RefCOCOg, aiming to generate text descriptions of specific regions in the bounding box. Table~\\ref{tab:reg} shows that our model outperforms Shikra and Kosmos-2 by a considerable margin of 16.51 CIDEr and 16.92 CIDEr gain on RefCOCOg, demonstrating the effectiveness of our model for fine-grained understanding. Besides, \\textbf{COMM} even outperforms finetuned SLR on RefCOCO+ and RefCOCOg.\n\n\\begin{table}[!t]\n\\centering\n\\setlength{\\tabcolsep}{1.5pt}\n\\caption{Object hallucination benchmark using POPE evaluation pipeline \\cite{li2023obj_Hallucination}. The results of Shikra-7B are taken from its paper. Except for Shikra-7B, the other results are obtained from \\cite{li2023obj_Hallucination}.\n}\n\\label{tab:pope_results}\n\\scalebox{0.95}{\n\\begin{tabular}{l|ccccccc}\n\\toprule\nDatasets  &\\textbf{COMM}& Shikra & InstructBLIP  & MiniGPT-4 & LLaVA&MM-GPT & mPLUG-Owl \\\\\n\\cmidrule(lr){1-8}\nRandom\n&   87.29   & 86.90 & \\textbf{88.57} & 79.67 &50.37 &50.10 & 53.97 \\\\\nPopular\n&   \\textbf{86.50}    & 83.97  & 82.77 &69.73  &49.87 &50.00 &50.90  \\\\\nAdversarial\n&   \\textbf{84.50}    & 83.10  & 72.10  &65.17  &  49.70 &50.00& 50.67\\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\\begin{table*}[!t]\n\\centering\n\\renewcommand{\\tabcolsep}{0.9mm}\n\\caption{Results on visual question answering (VQA) and image captioning.\nFor VQA, we evaluate SOTA generalist models and our \\textbf{COMM} onVQAv2 and OK-VQA following the normalization rules. Shikra and LLaVA-1.5~\\cite{liu2023improvedllava} is based on the 13B variant.\nFor image captioning, we evaluate them on COCO and Flickr30k in CIDEr score.\nWe call Flamingo as FM for short.\n}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{l|l|cccccccc}\n\\toprule\n\\multicolumn{2}{c|}{Datasets} &\\textbf{COMM}&LLaVA-1.5&Qwen& Shikra  & FM-80B& BLIP-2 & Unified-IO  & VPGTrans\\\\\n\\cmidrule(lr){1-2}\\cmidrule(lr){3-10}\n\\multirow{4}{*}{VQA}\n& VQAv2$^\\text{val}$     &\\textbf{79.05}&-&-& 75.33  &   -   & 65.2 & -  & 65.2 \\\\\n& VQAv2$^\\text{dev}$     &\\textbf{81.04}&80.0&79.5& 77.36  & 56.3 & 65.0 & 77.9  & - \\\\\n& VQAv2$^\\text{std}$     &\\textbf{81.17}&-&-& 77.51  &  -  &  -  & -  & - \\\\\n& OK-VQA                 &\\textbf{59.18}&-&58.6& 47.16  & 50.6 & 45.9 & 54.0  & 45.0 \\\\\n\\cmidrule(lr){1-2}\\cmidrule(lr){3-10}\n\\multirow{2}{*}{Caption}\n& Flickr30k      &\\textbf{88.2}&-&85.8 & 73.9 &  67.2   &   -  &   -   &   -   \\\\\n& COCO           &\\textbf{132.7}&-&- & 117.5 & 84.3  &   -  & 122.3 &   -  \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\label{tab:vl}\n\\end{table*}\n\\subsection{Object Hallucination Benchmark}\nWe compare our model against the baseline models\non the hallucination evaluation dataset recently introduced by POPE~\\cite{li2023obj_Hallucination}, which randomly selects 500 images from COCO~\\cite{caesar2018coco}. Table~\\ref{tab:pope_results} shows that \\textbf{COMM} surpasses recent popular MLLMs with 1.44\\% and 4.95\\% higher accuracy on average than Shikra and InstrutBLIP, respectively. By enhancing the fine-grained visual capabilities, \\textbf{COMM} can effectively alleviate the object hallucination problem.\n\n\\subsection{Visual Question Answering and Image Captioning}\nWe evaluate \\textbf{COMM} on conventional VL tasks of VQA and Image Captioning. Specifically, image captioning requires the model to generate description for the given image and VQA asks the model to generate answer for the given image-question pair. For image captioning, we choose COCO~\\cite{chen2015cococap} and Flickr30K~\\cite{plummer2015flickr30ke} as benchmarks and report the CIDEr score. For VQA task, we experiment on VQAv2~\\cite{antol2015vqav2} and OK-VQA~\\cite{marino2019ok}. \nAs shown in Table~\\ref{tab:vl}, \\textbf{COMM} achieves state-of-the-art performance on image captioning task, \\emph{i.e.}, 88.2 CIDEr score on Flickr30K and 132.7 CIDEr score on COCO, even outperforms previous SOTA models with much more parameters (\\emph{e.g.}, Shikra-13B with 13B parameters) or much more training data (\\emph{e.g.}, Qwen with 1.4B data). For VQA task, our model also shows significant advantages compared to other MLLMs. On VQAv2 val, dev and std, our model achieves 79.05, 81.04 and 81.17 accuracy respectively, which surpasses recent proposed Shikra with the same training data and procedure by a large margin, demonstrating the effectiveness of merging visual embeddings of DINOv2 and CLIP for enhancing visual capabilities. Besides, our \\textbf{COMM} model outperforms Qwen with 1.54 and 0.58 accuracy gain on VQAv2 dev and OK-VQA respectively with less VQA training data, \\emph{i.e.}, we use 0.6M and Qwen with 3.6M. Training with more VQA data might further improve performance and we leave it as future work.\n\\subsection{Ablation Study}\n\\label{sec:mae}\n\\textbf{Ablation on the MLP of DINOv2.} We conduct ablation study on the design of the MLP module in DINOv2 for aligning visual and text embedding space. \nWe ablate on the number and the expanding ratio of MLP module.\nTable~\\ref{tab:mlp} shows that increasing the number of MLP to 2 can evidently improve performance, demonstrate the effectiveness of using a more powerful network to align the vision only model DINOv2 to the word embedding space. However, increasing the number beyond 2 suffers the degraded performance. For the expanding ratio, increasing to 8 can improve performance, while increasing to 16 does not achieve significant performance gain. Moreover, we experiment with one linear layer, which suffers severe performance degradation. Thus, non-linear MLP is necessary for aligning the features of vision-only DINOv2 to the word embedding space.\n\n\\textbf{Ablation on the visual model of MAE and DeiT.} As shown in Table~\\ref{tab:deit}, MAE and DeiT suffers from evident performance degradation. For one thing, the visual features of MAE lack sufficient semantic information for global or regional understanding. For another, the supervised training of DeiT is so strong that it learns specialized visual space, making it difficult to align with the word embedding space. \n\\begin{table}[!t]\n\\centering\n\\setlength{\\tabcolsep}{2pt}\n\\caption{Ablation study on the number and expanding ratio of MLP module. Experiments are conducted on referring expression comprehension and object hallucination benchmark on Random (R), Adversarial (A), and Popular (P).\n}\n\\label{tab:mlp}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|ccccccccc}\n\\toprule\n\\multirow{2}{*}{Visual Model}  & \\multicolumn{3}{c}{RefCOCO+} & \\multicolumn{2}{c}{RefCOCOg}& \\multicolumn{3}{c}{RefCOCO}  &POPE\\\\\n& test-A & test-B &val& test-u&  val-u  &  test-A & test-B &val  &A/P/R\\\\\n \\cmidrule(lr){1-10}\nDINOv2 w/ MLP Ratio 4\n& 75.3 &59.3&67.0&73.0&71.8&84.4&74.1&79.6&80.3/84.2/85.5  \\\\\n\\cmidrule(lr){1-10}\nDINOv2 w/ 2MLP Ratio 4\n& \\textbf{77.5}& \t\\textbf{60.3} &69.2&\t\\textbf{74.6} &\\textbf{74.7} &\\textbf{86.5} &75.3 &81.4 &\\textbf{82.4}/84.5/86.2 \\\\\nDINOv2 w/ 4MLP Ratio 4\n& 53.7& \t34.4 &45.3&\t49.0 &48.8&\t65.4 &48.0 &57.9 &79.2/82.9/84.6 \\\\\nDINOv2 w/ 8MLP Ratio 4\n& 8.2 & \t6.5 & \t7.4 & \t6.8 & 6.7 & 14.8 & 12.9 & 14.9& 56.0/55.3/59.0\\\\ \n\\cmidrule(lr){1-10}\nDINOv2 w/ MLP Ratio 8\n&77.4 &59.9 &\\textbf{69.7} &73.7 &73.3 &85.7&74.1 &80.9 &81.5/\\textbf{85.8}/\\textbf{86.7}\\\\\nDINOv2 w/ MLP Ratio 16\n&76.2 &60.2 &\\textbf{69.7} &74.5&74.6&85.7 &\\textbf{75.5} &\\textbf{81.5} &80.4/83.7/85.7\\\\ \n\\cmidrule(lr){1-10}\nDINOv2 w/ Linear\n&61.8& \t48.8 &\t55.1 &\t64.1 &\t62.9 &\t76.5 &\t67.0 &\t71.9& \t75.6/79.3/83.7 \\\\\n\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\\begin{table}[!t]\n\\centering\n\\setlength{\\tabcolsep}{5.3pt}\n\\caption{Comparison of the visual model using CLIP, DINOv2 with our multi-level feature merging (MFM), MAE and DeiT. MAE-20 denotes using the features output by the 20-th layer of MAE. DeiT-20 denotes using the features output by 20-th layer.\n}\n\\label{tab:deit}\n\\scalebox{1}{\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{l|ccccccccc}\n\\toprule\n\\multirow{2}{*}{Visual Model}  & \\multicolumn{3}{c}{RefCOCO+} & \\multicolumn{2}{c}{RefCOCOg}& \\multicolumn{3}{c}{RefCOCO}  &POPE\\\\\n& test-A & test-B &val& test-u&  val-u  &  test-A & test-B &val  &A/P/R\\\\\n\\cmidrule(lr){1-10}\nCLIP w/ MFM&  73.7&53.8 &64.3 &69.1 &70.3&83.8 &68.4&76.4&\\textbf{80.7}/\\textbf{84.2}/\\textbf{85.8} \n \\\\\n \\cmidrule(lr){1-10}\nDINOv2 w/ MFM\n& \\textbf{75.3} &\\textbf{59.3}&\\textbf{67.0}&\\textbf{73.0}&\\textbf{71.8}&\\textbf{84.4}&\\textbf{74.1}&\\textbf{79.6}&80.3/\\textbf{84.2}/85.5  \\\\\n\\cmidrule(lr){1-10}\nMAE-20&64.7&\t49.4&\t56.8\t&63.7&\t62.8&\t77.9&\t68.6\t&73.6\t&66.8/71.1/76.7\\\\\nMAE-22&65.9& \t50.0 &\t58.5 &\t64.2 &\t63.2& \t79.3& \t69.8& \t74.9 &\t68.0/71.2/77.5\\\\ \n\\cmidrule(lr){1-10}\nDeiT-20&18.4&\t13.0&\t15.9&\t17.0\t&16.2\t&29.0&\t21.6&\t25.7&\t66.2/69.6/77.9\\\\\nDeiT-22&25.3&\t15.4&\t19.4&\t22.6&\t21.8\t&36.9&\t25.3&\t32.0&\t67.9/71.6/78.7\\\\\n\\bottomrule\n\\end{tabular}%\n}}\n\\end{table}\n\\subsection{Demonstrations}\nAs shown in Fig.~\\ref{fig:case}, our \\textbf{COMM} model exhibits a multitude of promising capabilities including visual grounding, fine-grained region understanding and robustness to object hallucination. The first example showcases our strong fine-grained perception ability, which identifies implicit strawberries in a blender. The second example exhibits our strong visual grounding ability to successfully locates the object of sugar. The third case demonstrates our robustness to object hallucination. In contrast, Shikra fails on these challenging cases, showing the superior capabilities of our model. We provide additional demonstrations of our \\textbf{COMM} model in this section to demonstrate a multitude of promising capabilities including visual grounding, fine-grained region understanding and robustness to object hallucination. For instance, we showcase Referring Expression Comprehension in Fig.~\\ref{fig:caserec} and Object Hallucination in Fig.~\\ref{fig:caseobj}.\n\\begin{figure}[h!]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=1\\columnwidth]{rec_case.png}\n\\caption{Referring Expression Comprehension (REC) using our \\textbf{COMM}-7B. The task intends to localize a target object in an image described by a referring expression.}\n\\label{fig:caserec}\n\\end{figure}\n\\begin{figure}[!h]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=1\\columnwidth]{object.png}\n\\caption{Object hallucination using our \\textbf{COMM}-7B. This task aims to evaluate the robustness to object hallucination, \\emph{i.e.}, answer yes or no for the existence of questioned object.}\n\\label{fig:caseobj}\n\\end{figure}\n\\begin{figure}[!t]\n\\renewcommand{\\baselinestretch}{1.0}\n\\centering\n\\includegraphics[width=0.9\\columnwidth]{case2.png}\n\\caption{Qualitative comparison between Shikra with its official checkpoint and our \\textbf{COMM}.}\n\\label{fig:case}\n\\end{figure}\n\n\n\\section{Conclusion}\n\nThis paper presented an extensive investigation into the efficacy of different visual models when employed as the visual branch in MLLMs. Through a systematic analysis, we highlight the significance of shallow layer features, which capture low-level details that prove beneficial for grounding and positioning tasks. Furthermore, we recognize the potential of the vision-only model DINOv2, which leverages its inherent fine-grained pixel-level information for enhanced fine-grained perception in MLLMs when combined with an MLP layer for alignment purposes. Motivated by our analysis, we introduce a fusion approach to combine the visual features obtained from CLIP and DINOv2, thereby further augmenting the visual capabilities and performance of MLLMs. Through qualitative analysis and extensive quantitative experiments, we demonstrate the effectiveness of our proposed method, surpassing the performance of existing MLLM models across diverse benchmark datasets. Looking ahead, we encourage future research to explore the integration of more powerful vision models to enhance the capabilities of visual branches in MLLMs. We believe that this avenue of investigation holds the key to unlocking the potential of the next generation of MLLMs.\n\n\n\n\n", "DeepReviewer": "\n## Summary:\n\nThis paper introduces COMM, a novel approach to enhance the visual capabilities of Multimodal Large Language Models (MLLMs) by integrating the strengths of CLIP and DINOv2 through a multi-level feature merging strategy. The authors' core contribution lies in their investigation of different visual encoders within MLLMs, demonstrating that combining the global semantic understanding of CLIP with the fine-grained localization information from DINOv2 leads to improved performance across a range of vision-language tasks. The COMM method employs a multi-level feature merging (MFM) strategy, which aligns and combines features from different layers of both CLIP and DINOv2 using a linear layer, layer normalization, and learnable scaling parameters. For DINOv2, which lacks inherent text alignment, an MLP layer is used to project its features into the word embedding space. The authors evaluate their method on a variety of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination, demonstrating superior performance compared to existing methods. The empirical findings suggest that the proposed feature merging strategy effectively leverages the complementary strengths of CLIP and DINOv2, resulting in enhanced visual understanding within MLLMs. The paper's overall significance lies in its exploration of visual encoder effectiveness and the introduction of a practical method for improving visual capabilities in MLLMs. However, the paper also presents several limitations, particularly in terms of the practical applicability of the method due to the large feature vector size, the lack of a strong theoretical foundation, and the limited generalizability of the results. Despite these limitations, the paper provides valuable insights into the potential of combining different visual encoders for MLLMs and offers a promising direction for future research.\n\n\n## Soundness:\n\n2.75\n\n\n## Presentation:\n\n2.75\n\n\n## Contribution:\n\n2.75\n\n\n## Strengths:\n\nI find several aspects of this paper to be commendable. Firstly, the paper's core idea of combining CLIP and DINOv2 to leverage their complementary strengths is both intuitive and effective. The authors' analysis of the different biases of CLIP and DINOv2, with CLIP excelling in global understanding and DINOv2 capturing fine-grained details, provides a solid rationale for their approach. The empirical results, demonstrating superior performance on a range of vision-language tasks, validate the effectiveness of the proposed COMM method. Specifically, the performance gains on tasks like Referring Expression Comprehension (REC), Object Hallucination Benchmark (POPE), and Visual Question Answering (VQA) are significant and showcase the enhanced visual capabilities of MLLMs when using COMM. The authors' exploration of different feature merging strategies, including the use of linear-layernorm and learnable scaling parameters, is a valuable contribution. The use of an MLP layer to align DINOv2's features with the word embedding space is also a practical and effective solution. Furthermore, the paper is well-written and easy to follow, making the methodology and results accessible to a broad audience. The authors' focus on practical application and empirical validation is a strength, as it demonstrates the real-world potential of their approach. The paper's exploration of visual encoder effectiveness within MLLMs is a significant contribution, as it highlights the importance of carefully selecting and combining visual encoders to achieve optimal performance. Finally, the paper's demonstration of the COMM method's ability to enhance visual capabilities in MLLMs is a promising direction for future research in this field.\n\n\n## Weaknesses:\n\nDespite the strengths of this paper, I have identified several significant weaknesses that warrant careful consideration. The most pressing limitation is the practical applicability of the COMM method due to the large visual feature length. The authors explicitly state that they concatenate features from both CLIP and DINOv2, which are both based on ViT-Large. Given the image resolution of 336x336 and the downsampling rate of 14, each encoder outputs 576 tokens per layer. Concatenating features from multiple layers of both encoders results in a feature vector that can easily exceed the context window of standard LLMs like LLaMA2, which has a maximum input length of 4096 tokens. This is a critical issue, as it makes it difficult to apply the COMM method to most existing MLLMs without significant modifications or feature compression techniques. This limitation is explicitly stated in the paper, where the authors mention the concatenation of features and the resulting token sequence (lines 296-298). My analysis confirms that this concatenation, even with multi-level merging, leads to a feature vector that could easily exceed the typical context window of transformer-based LLMs, creating a significant bottleneck for practical deployment. This is a high-confidence concern, as it is directly supported by the paper's methodology and the known limitations of standard LLMs. \n\nFurthermore, the reliance on two distinct visual encoders introduces a computational overhead that may not be feasible for resource-constrained environments or real-time applications. The paper confirms the use of both CLIP and DINOv2 as visual encoders (lines 291-292). Processing two large visual encoders and handling the resulting large feature vector will undoubtedly increase the computational cost, making the method less practical for deployment in resource-constrained environments. This is another high-confidence concern, as it is a direct consequence of the method's design. The paper lacks a strong theoretical foundation to explain why the COMM method enhances the visual capabilities of MLLMs. While the authors provide some intuition about the complementary strengths of CLIP and DINOv2, there is no formal analysis of the feature spaces or a theoretical framework explaining the benefits of the specific merging strategy. The paper mentions that shallow layers capture local details and deep layers capture global semantics (lines 49-52), and that DINOv2 captures fine-grained localization (lines 53-55). However, there is no mathematical analysis of the feature spaces of CLIP and DINOv2, nor is there a formal justification for the chosen merging strategy. This lack of theoretical grounding makes it difficult to understand the underlying mechanisms of the method and limits the generalizability of the findings. This is a high-confidence concern, as it is evident from the absence of a formal theoretical framework in the paper. The COMM method's generalizability is also limited, as it is primarily evaluated within the Shikra framework. The paper explicitly mentions using Vicuna as the language decoder and following the training setup of Shikra (lines 291-294, 311-313). While the architecture description is general, the experimental setup heavily relies on the Shikra framework's training data and procedure. This makes it difficult to assess the robustness of the proposed method across different MLLM designs. This is a high-confidence concern, as it is directly supported by the paper's experimental setup. The COMM method also lacks significant innovation in terms of feature merging techniques. The paper describes the MFM strategy as involving linear-layernorm for alignment and learnable scaling parameters for merging (lines 59-61). For DINOv2, an MLP is used for alignment (lines 61-63). While effective, these techniques are relatively standard and do not represent a significant architectural innovation. This is a high-confidence concern, as it is evident from the description of the MFM strategy in the paper. The paper's evaluation is limited to a few general visual tasks, such as REC, REG, POPE, VQA, and Image Captioning (lines 320-322). It does not include more specialized domains like medical imaging or video understanding. This lack of evaluation on specialized tasks limits the understanding of COMM's performance in more complex and domain-specific scenarios. This is a high-confidence concern, as it is evident from the list of evaluated tasks in the paper. Finally, the paper lacks sufficient interpretability, as it does not provide a detailed analysis of how the different feature levels contribute to the overall performance. While the paper provides some high-level analysis of feature layer contributions (lines 137-138, 142-144) and shows performance improvements, there is no detailed analysis of how the learnable scaling parameters in the MFM strategy affect the contribution of different feature levels, or a deeper dive into the interaction between CLIP and DINOv2 features. This lack of interpretability makes it harder to understand the specific mechanisms behind the performance gains and limits the ability to further refine the method. This is a high-confidence concern, as it is evident from the lack of detailed analysis in the paper. The paper also fails to convincingly demonstrate the unique advantages of the proposed method over existing approaches, particularly given the increased computational cost and complexity. While the paper shows performance improvements, the simplicity of the method and lack of deep interpretability, coupled with potential computational costs, might not fully justify the claimed advantages. This is a medium-confidence concern, as it is a more subjective assessment of the paper's overall contribution. In summary, while the paper presents a promising approach, these limitations significantly impact its practical applicability and generalizability.\n\n\n## Suggestions:\n\nTo address the identified weaknesses, I recommend several concrete improvements. Firstly, the authors should explore techniques to reduce the dimensionality of the combined feature space. Given the concatenation of features from both CLIP and DINOv2, the resulting feature vector can be quite large, potentially exceeding the context window of standard LLMs. To mitigate this, the authors could investigate methods such as Principal Component Analysis (PCA) or autoencoders to compress the feature representation while preserving the most informative components. This would make the method more applicable to a wider range of MLLMs and deployment scenarios. Furthermore, the authors should investigate the possibility of using more efficient feature merging strategies that do not rely on the concatenation of all feature levels. For example, they could explore attention-based mechanisms to selectively combine the most relevant features from CLIP and DINOv2. This would allow the model to focus on the most informative features, reducing the dimensionality of the combined feature space and potentially improving performance. Another approach could be to investigate knowledge distillation techniques to transfer the knowledge of the combined model into a more compact representation. This would allow the authors to leverage the performance of the combined model while reducing the computational cost of deployment. To enhance the theoretical grounding of the COMM method, the authors should provide a more detailed analysis of the feature spaces of CLIP and DINOv2. This could involve visualizing the feature distributions, analyzing the correlation between different feature levels, and investigating the impact of different merging strategies on the resulting feature representation. A formal analysis of the information content of the different feature levels would also be beneficial. The authors should also explore the possibility of using techniques such as information bottleneck to identify the most informative features for the downstream tasks. This would provide a more solid theoretical foundation for the proposed method and help to understand why it works. To address the limited generalizability of the method, the authors should significantly broaden the evaluation of the COMM method to include a more diverse set of MLLM architectures and visual tasks. This should include evaluating the method on different LLM backbones, such as LLaMA3, and on more specialized tasks such as medical image analysis or video understanding. The authors should also provide a more detailed analysis of the computational cost of the proposed method, including the memory footprint and inference time. This would help to assess the practical feasibility of the method for real-world applications. Furthermore, the authors should provide a more detailed analysis of the interpretability of the method, including visualizing the contribution of different feature levels to the final prediction. This would help to understand the underlying mechanisms of the proposed method and provide insights for further improvements. Finally, the authors should explore more sophisticated feature fusion techniques to enhance the innovation of the method. This could involve exploring cross-attention mechanisms that allow for more fine-grained interaction between the features of CLIP and DINOv2, or investigating adaptive layer selection mechanisms, perhaps guided by a task-specific objective. These improvements would not only address the identified weaknesses but also enhance the overall impact and significance of the paper.\n\n\n## Questions:\n\nI have several questions regarding the methodology and results presented in this paper. Firstly, given the significant increase in visual feature length due to the concatenation of CLIP and DINOv2 features, what specific strategies do the authors propose to mitigate the context window limitations of standard LLMs? Have they explored any feature compression techniques or alternative merging strategies that could reduce the dimensionality of the combined feature space without sacrificing performance? Secondly, considering the lack of a formal theoretical framework, what specific analyses have the authors conducted to understand the interaction between the feature spaces of CLIP and DINOv2? Have they visualized the feature distributions or analyzed the correlation between different feature levels? What is the rationale behind the specific merging strategy, and how does it impact the information content of the resulting feature representation? Thirdly, given the limited evaluation of the COMM method on the Shikra framework, what are the authors' plans to evaluate the method on a more diverse set of MLLM architectures? What specific challenges do they anticipate when applying the method to different LLM backbones, and how do they plan to address these challenges? Fourthly, considering the relatively straightforward nature of the feature merging strategy, what are the authors' plans to explore more sophisticated feature fusion techniques? Have they considered attention-based mechanisms or adaptive layer selection mechanisms? What are the potential benefits and drawbacks of these more complex approaches? Fifthly, given the limited evaluation on general visual tasks, what are the authors' plans to evaluate the method on more specialized domains, such as medical image analysis or video understanding? What specific challenges do they anticipate when applying the method to these more complex and domain-specific scenarios? Finally, considering the lack of interpretability of the method, what specific analyses have the authors conducted to understand the contribution of different feature levels to the final prediction? Have they visualized the feature maps or used techniques like t-SNE to project the high-dimensional features into a lower-dimensional space? What are the specific mechanisms behind the performance gains observed with the COMM method?\n\n\n## Rating:\n\n4.5\n\n\n## Confidence:\n\n5.0\n\n\n## Decision:\n\nReject", "other": "\n## Summary:\n\nThis paper introduces COMM, a novel multi-modal large language model (MLLM) architecture that leverages a multi-level feature merging strategy to integrate the strengths of CLIP and DINOv2 visual encoders. The authors' core contribution lies in their empirical investigation of different visual encoders within MLLMs, revealing that shallow layer features of CLIP are particularly effective for fine-grained tasks like visual grounding, while DINOv2, when equipped with an MLP alignment layer, excels in fine-grained perception tasks. The proposed COMM architecture addresses this by merging multi-level features from both CLIP and DINOv2, aiming to enhance the visual capabilities of MLLMs. Specifically, COMM extracts features from all 24 layers of CLIP, aligns them using a linear-layernorm module, and merges them using a layerscale approach. For DINOv2, features from layers 19 to 24 are aligned using an MLP layer and then concatenated with the CLIP features. The final fused features are projected using a linear layer to match the dimension of text features. The authors evaluate COMM on a wide range of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination. The results demonstrate that COMM achieves strong performance across these tasks, particularly in visual question answering, where it achieves state-of-the-art results on VQAv2 and OK-VQA. The paper also includes an analysis of different visual encoders, showing that MAE and DeiT perform worse than CLIP and DINOv2 in the context of MLLMs. The authors further conduct ablation studies to validate the effectiveness of the proposed multi-level feature merging strategy. Overall, this paper presents a well-motivated and empirically supported approach to improving the visual capabilities of MLLMs by combining the strengths of different visual encoders through a multi-level feature merging strategy. The findings suggest that carefully combining features from different layers of complementary visual models can lead to significant performance gains in various vision-language tasks.\n\n\n## Soundness:\n\n2.5\n\n\n## Presentation:\n\n2.75\n\n\n## Contribution:\n\n2.5\n\n\n## Strengths:\n\nThis paper presents a compelling approach to multi-modal learning by combining CLIP and DINOv2 features, demonstrating a clear understanding of the strengths of each model. The empirical study on the effectiveness of features at different layers of CLIP and DINOv2 is a significant strength, providing valuable insights into how different layers contribute to various vision-language tasks. The finding that shallow layer features of CLIP are beneficial for fine-grained tasks, while DINOv2 excels in perception tasks when aligned with an MLP, is particularly noteworthy. The proposed COMM architecture effectively leverages these insights by merging multi-level features from both models, leading to strong performance across a wide range of benchmarks. The paper's experimental results are impressive, with COMM achieving state-of-the-art performance on VQAv2 and OK-VQA, demonstrating the effectiveness of the proposed feature merging strategy. The inclusion of ablation studies further strengthens the paper by validating the design choices and the contribution of the multi-level feature merging strategy. The paper is also well-written and easy to follow, making the methodology and findings accessible to a broad audience. The motivation for the work is clear and reasonable, addressing a critical need for improved visual capabilities in MLLMs. The authors have successfully demonstrated that combining the strengths of different visual encoders through a multi-level feature merging strategy can lead to significant performance gains in various vision-language tasks. The paper's focus on empirical analysis and thorough evaluation of the proposed method makes it a valuable contribution to the field of multi-modal learning.\n\n\n## Weaknesses:\n\nWhile the paper presents a strong empirical study and achieves impressive results, several limitations warrant careful consideration. First, the COMM architecture relies heavily on pre-trained CLIP and DINOv2 models, which are kept frozen during training. This reliance on pre-trained models, while practical, limits the exploration of the COMM merging strategy's potential when applied to vision encoders trained from scratch. As the paper states in the 'Training Details' section, 'In both stages, we freeze the visual encoder and tune all parameters in LLMs, alignment layer and multi-level feature fusion module.' This approach makes it difficult to isolate the impact of the merging strategy itself from the pre-existing knowledge embedded in CLIP and DINOv2. The paper lacks any experiments where a new vision encoder is trained from scratch using the COMM merging strategy, which would provide a clearer understanding of the architecture's effectiveness independent of the pre-training. This is a significant limitation, as it prevents a full assessment of the COMM architecture's generalizability and its potential when applied to different vision encoder architectures. My confidence in this weakness is high, as the paper explicitly states the use of pre-trained and frozen encoders and lacks any experiments to address this concern. Second, the paper's VQA results, while achieving state-of-the-art performance, are potentially confounded by the use of a significantly larger training dataset compared to previous works. The 'Training Details' section mentions the use of LLaVA-Instruct-150K and Shikra-RD for instruction tuning, which implies a larger training dataset than the 600K VQA data used in works like LLaVA. The paper does not explicitly state the total amount of VQA data used, but the inclusion of these datasets suggests a substantial increase. This makes it difficult to determine whether the performance gains are due to the proposed method or simply a result of the increased training data. The paper lacks experiments where the training data is reduced to 600K, which would allow for a more direct comparison with existing methods. Furthermore, the paper does not include a learning curve to show the performance of COMM with varying amounts of training data, which would be valuable in understanding the data efficiency of the proposed approach. This lack of control over the training data size makes it difficult to isolate the contribution of the COMM merging strategy. My confidence in this weakness is high, as the paper does not provide a direct comparison with previous works using the same training data size and lacks experiments with varying training data amounts. Third, the paper's performance on image captioning lags behind recent works, such as LLaVA-1.5, which is based on BLIP-2. As shown in Table 4, COMM achieves good performance on image captioning tasks, but it does not surpass all recent works. The paper lacks a detailed analysis of why the COMM architecture might be less effective for image captioning tasks compared to VQA. The paper suggests that the COMM merging design might be more suitable for VQA, but this explanation is not sufficient. There is no investigation into whether the multi-level feature merging strategy introduces noise or irrelevant information for caption generation, or whether there are specific characteristics of the captioning task that make it harder for the COMM model. This lack of analysis limits the understanding of the COMM architecture's limitations and its applicability to different vision-language tasks. My confidence in this weakness is high, as the paper's results in Table 4 show a clear performance gap in image captioning, and the paper lacks a detailed analysis to explain this discrepancy. Finally, the paper's analysis of different vision encoders is limited to CLIP, DINOv2, MAE, and DeiT. The paper does not include an analysis of more recent and potentially stronger vision encoders, such as EVA-CLIP, which have demonstrated superior performance in various vision tasks. This limits the comprehensiveness of the study and raises questions about whether the proposed COMM architecture would be as effective with other state-of-the-art vision encoders. The paper also lacks experiments exploring different combinations of layers from CLIP and DINOv2, focusing instead on different merging methods. This limits the understanding of the optimal layer combinations for different tasks. My confidence in this weakness is high, as the paper explicitly states the encoders analyzed and lacks experiments with other state-of-the-art encoders or different layer combinations.\n\n\n## Suggestions:\n\nTo address the identified weaknesses, I recommend several concrete improvements. First, the authors should conduct experiments where the COMM architecture is trained from scratch with a randomly initialized vision encoder. This would involve training a model with the same architecture as COMM, but without using pre-trained CLIP and DINOv2 weights. This would provide a clearer understanding of the effectiveness of the COMM architecture itself, independent of the pre-training. Furthermore, exploring different pre-training strategies for the vision encoder, such as contrastive learning or masked image modeling, could provide insights into the optimal conditions for COMM's effectiveness. This analysis would strengthen the paper's claims about the generalizability of the COMM approach and its applicability to various vision encoder architectures. Second, the authors should conduct experiments with a reduced training dataset for VQA tasks, comparable to that used in prior works like LLaVA (600K VQA data). This would allow for a more direct comparison and provide a clearer understanding of the actual contribution of the COMM merging strategy. Additionally, a detailed analysis of the learning curves and performance trends with varying amounts of training data would be valuable in determining the data efficiency of the proposed approach. This analysis should also explore the impact of different data augmentation techniques on the performance of COMM with reduced training data. The authors should also consider reporting results with a training dataset size of 600K to allow for a more direct comparison with existing methods. Third, the authors should investigate the reasons behind the observed performance gap in image captioning compared to models like LLaVA-1.5. This could involve analyzing the feature representations learned by COMM and comparing them to those of models that perform well on image captioning. This analysis could reveal whether the multi-level feature merging strategy is introducing noise or irrelevant information for caption generation. Furthermore, exploring alternative merging strategies or modifications to the COMM architecture specifically for image captioning could potentially improve its performance on this task. This could involve incorporating attention mechanisms or other techniques that are known to be effective for sequence generation tasks. Fourth, the authors should include an analysis of more recent and stronger vision encoders, such as EVA-CLIP, to provide a more comprehensive study. This would help to determine whether the proposed COMM architecture would be as effective with other state-of-the-art vision encoders. The authors should also explore different combinations of layers from CLIP and DINOv2, including using deep layers of CLIP and shallow layers of DINOv2, or using only DINOv2 features. This would help to understand the optimal layer combinations for different tasks. Finally, the authors should consider applying the COMM merging strategy to other vision encoders to demonstrate its generalizability and potential for broader impact. This would help to show that the COMM merging strategy is not limited to CLIP and DINOv2 and can be applied to other vision encoders to improve their performance in MLLMs. These changes would strengthen the paper's claims and provide a more complete understanding of the proposed method's effectiveness and limitations.\n\n\n## Questions:\n\nSeveral key questions arise from my analysis of this paper. First, given the reliance on pre-trained CLIP and DINOv2 models, what is the true contribution of the COMM merging strategy independent of the pre-existing knowledge embedded in these models? How would the performance of COMM change if the vision encoders were trained from scratch using the same architecture and training procedure? Second, how does the increased training data size for VQA tasks impact the performance gains achieved by COMM? What is the data efficiency of the proposed approach, and how much VQA data is needed to achieve a good performance? Would the performance of COMM be comparable to other methods if trained on a smaller dataset? Third, why does the COMM architecture lag behind recent works on image captioning, and what specific characteristics of the captioning task make it harder for the COMM model? Does the multi-level feature merging strategy introduce noise or irrelevant information for caption generation? Fourth, how would the performance of COMM change if more recent and stronger vision encoders, such as EVA-CLIP, were used? Would the proposed merging strategy be as effective with other state-of-the-art vision encoders? Fifth, what is the optimal combination of layers from CLIP and DINOv2 for different vision-language tasks? Have the authors explored other combinations of layers, such as using deep layers of CLIP and shallow layers of DINOv2, or using only DINOv2 features? Finally, how does the COMM merging strategy compare to more advanced feature merging techniques, such as those used in EVA-CLIP? Would incorporating attention-based mechanisms or learnable fusion weights improve the performance of COMM? These questions highlight key uncertainties and areas for further investigation that would strengthen the paper's conclusions and provide a more complete understanding of the proposed method.\n\n\n## Rating:\n\n5.25\n\n\n## Confidence:\n\n4.25\n\n\n## Decision:\n\nReject", "other_type": "DeepReviewer_Standard", "v.s.": "A", "result": "```\n**Technical Accuracy - Evidence Analysis:**\n- From Assistant A:\n  [Claim: \"practical applicability of the COMM method due to the large visual feature length...concatenation of features from both CLIP and DINOv2, which are both based on ViT-Large...feature vector that can easily exceed the context window of standard LLMs like LLaMA2, which has a maximum input length of 4096 tokens.\"]\n  [Evidence: \"authors explicitly state that they concatenate features from both CLIP and DINOv2...lines 296-298...concatenation, even with multi-level merging, leads to a feature vector that could easily exceed the typical context window of transformer-based LLMs\"]\n  [Analysis: Assistant A accurately points out a potential issue with feature concatenation leading to large input size, referencing lines 296-298 which describe feature concatenation. This is a valid technical concern regarding the practical implementation of COMM with standard LLMs.]\n- From Assistant B:\n  [Claim: \"COMM architecture relies heavily on pre-trained CLIP and DINOv2 models, which are kept frozen during training...limits the exploration of the COMM merging strategy's potential when applied to vision encoders trained from scratch.\"]\n  [Evidence: \"'Training Details' section... 'In both stages, we freeze the visual encoder and tune all parameters in LLMs, alignment layer and multi-level feature fusion module.'...paper lacks any experiments where a new vision encoder is trained from scratch using the COMM merging strategy\"]\n  [Analysis: Assistant B correctly identifies that the visual encoders are frozen during training, as stated in the 'Training Details' section.  The lack of experiments training vision encoders from scratch with COMM is also a valid observation regarding the scope of the paper's investigation.]\n- Comparative Assessment:\n  Both assistants raise technically accurate points based on the paper's description. Assistant A focuses on a potential practical limitation related to input size, while Assistant B highlights a limitation in the experimental scope regarding training vision encoders from scratch. Both are valid interpretations of the paper's content and methodology.\n\n**Technical Accuracy - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate strong technical accuracy by correctly interpreting the paper's methodology and identifying potential limitations based on the provided information. Assistant A accurately points out the feature length issue, and Assistant B correctly notes the frozen visual encoders and lack of training from scratch experiments.\n**Better Assistant:** Tie\n- Tie: Both reviews are equally accurate in their technical assessments and interpretations of the paper's methodology.\n\n---\n**Constructive Value - Evidence Analysis:**\n- From Assistant A:\n  [Suggestions: \"explore techniques to reduce the dimensionality of the combined feature space...PCA or autoencoders...efficient feature merging strategies...attention-based mechanisms...knowledge distillation...theoretical analysis...broader evaluation...computational cost analysis...interpretability analysis...more sophisticated feature fusion techniques.\"]\n  [Analysis: Assistant A provides a wide range of actionable and specific suggestions for improvement. Suggestions like PCA, autoencoders, attention-based merging, knowledge distillation, and broader evaluation are concrete and directly address the identified weaknesses. These suggestions are feasible and would help authors improve their work.]\n- From Assistant B:\n  [Suggestions: \"conduct experiments where the COMM architecture is trained from scratch with a randomly initialized vision encoder...experiments with a reduced training dataset for VQA tasks...investigate the reasons behind the observed performance gap in image captioning...include an analysis of more recent and stronger vision encoders, such as EVA-CLIP...explore different combinations of layers from CLIP and DINOv2...consider applying the COMM merging strategy to other vision encoders\"]\n  [Analysis: Assistant B also provides constructive and specific suggestions. Training from scratch, using reduced VQA data, investigating captioning gap, analyzing EVA-CLIP, exploring layer combinations, and applying COMM to other encoders are all actionable and relevant suggestions that would enhance the paper's robustness and scope. These suggestions are also feasible and directly address the weaknesses identified.]\n- Comparative Assessment:\n  Both assistants offer highly constructive and actionable feedback. Assistant A's suggestions are broader, covering dimensionality reduction, theoretical analysis, and interpretability. Assistant B's suggestions are more focused on experimental variations, such as training from scratch, data size reduction, and encoder comparisons. Both sets of suggestions are valuable and would help the authors improve their paper significantly.\n\n**Constructive Value - Judgment:**\n**Evidence-Based Reason:** Both assistants provide highly constructive and actionable feedback. Assistant A offers a broader range of suggestions covering both practical and theoretical aspects, while Assistant B focuses more on specific experimental variations. Both sets of suggestions are valuable and directly address the weaknesses they identified.\n**Better Assistant:** Tie\n- Tie: Both reviews are equally strong in providing constructive and actionable feedback. The suggestions from both assistants are highly relevant and would significantly benefit the authors in improving their work.\n\n---\n**Analytical Depth - Evidence Analysis:**\n- From Assistant A:\n  [Analysis: \"analysis of the different biases of CLIP and DINOv2, with CLIP excelling in global understanding and DINOv2 capturing fine-grained details...exploration of different feature merging strategies, including the use of linear-layernorm and learnable scaling parameters...use of an MLP layer to align DINOv2's features with the word embedding space...exploration of visual encoder effectiveness within MLLMs\"]\n  [Critiques: \"practical applicability of the COMM method due to the large visual feature length...computational overhead...lack of a strong theoretical foundation...limited generalizability...COMM method also lacks significant innovation in terms of feature merging techniques...evaluation is limited to a few general visual tasks...lacks sufficient interpretability...fails to convincingly demonstrate the unique advantages\"]\n  [Analysis: Assistant A demonstrates a good analytical depth by examining various aspects of the paper, including the motivation, methodology, experimental setup, results, and limitations. The review delves into the practical implications, theoretical underpinnings, generalizability, innovation, evaluation scope, interpretability, and justification of claims. The analysis is comprehensive and covers key elements of the paper.]\n- From Assistant B:\n  [Analysis: \"empirical study on the effectiveness of features at different layers of CLIP and DINOv2...finding that shallow layer features of CLIP are beneficial for fine-grained tasks, while DINOv2 excels in perception tasks when aligned with an MLP...proposed COMM architecture effectively leverages these insights by merging multi-level features from both models...ablation studies further strengthens the paper by validating the design choices\"]\n  [Critiques: \"COMM architecture relies heavily on pre-trained CLIP and DINOv2 models...VQA results...are potentially confounded by the use of a significantly larger training dataset...performance on image captioning lags behind recent works...analysis of different vision encoders is limited to CLIP, DINOv2, MAE, and DeiT.\"]\n  [Analysis: Assistant B also shows good analytical depth by focusing on the empirical aspects, experimental design, and result interpretation. The review analyzes the contributions of different layers, the effectiveness of COMM, the impact of pre-training, the potential confounding factor of training data size in VQA, the performance in captioning, and the scope of vision encoder analysis. The analysis is detailed and addresses important elements of the paper's empirical validation.]\n- Comparative Assessment:\n  Both assistants demonstrate a strong analytical depth by thoroughly examining key aspects of the paper. Assistant A provides a broader analytical perspective, covering more diverse aspects like theoretical foundation and interpretability. Assistant B focuses more deeply on the empirical validation and experimental design, particularly highlighting potential confounding factors and limitations in experimental scope. Both reviews show a comprehensive assessment of the paper's strengths and weaknesses.\n\n**Analytical Depth - Judgment:**\n**Evidence-Based Reason:** Both assistants exhibit strong analytical depth by thoroughly examining various crucial aspects of the paper. Assistant A provides a broader analysis covering theoretical and practical implications, while Assistant B delves deeper into the empirical validation and experimental design, identifying potential confounding factors and limitations in scope. Both reviews demonstrate a comprehensive and detailed assessment.\n**Better Assistant:** Tie\n- Tie: Both reviews are equally strong in their analytical depth. They both thoroughly examine key aspects of the paper, providing detailed and insightful analyses of its strengths and weaknesses from different but equally valuable perspectives.\n\n---\n**Communication Clarity - Evidence Analysis:**\n- From Assistant A:\n  [Clarity: \"paper is well-written and easy to follow, making the methodology and results accessible to a broad audience...The authors' focus on practical application and empirical validation is a strength, as it demonstrates the real-world potential of their approach.\"]\n  [Examples: Uses specific examples like \"large visual feature length\", \"context window of standard LLMs like LLaMA2\", \"PCA or autoencoders\", \"attention-based mechanisms\". Quotes lines from the paper to support claims.]\n  [Reasoning: Clearly explains the reasoning behind each critique and suggestion, linking them back to the paper's methodology and results. For example, the concern about feature length is directly linked to the concatenation method and LLM context window limitations.]\n- From Assistant B:\n  [Clarity: \"paper is also well-written and easy to follow, making the methodology and findings accessible to a broad audience...The motivation for the work is clear and reasonable, addressing a critical need for improved visual capabilities in MLLMs.\"]\n  [Examples: Uses specific examples like \"pre-trained CLIP and DINOv2 models\", \"LLaVA-Instruct-150K and Shikra-RD\", \"600K VQA data\", \"EVA-CLIP\". Quotes lines from the paper to support claims.]\n  [Reasoning: Clearly articulates the reasoning behind each critique and suggestion, providing specific examples and referencing sections of the paper. For instance, the concern about VQA results is linked to the potentially larger training dataset and lack of comparison with fixed data size.]\n- Comparative Assessment:\n  Both assistants communicate their feedback with excellent clarity. They both use specific examples, quote from the paper, and clearly explain their reasoning. Their points are well-articulated and easy to understand. Both reviews are structured logically and present their arguments effectively.\n\n**Communication Clarity - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate excellent communication clarity. They articulate their points clearly, use specific examples effectively, quote from the paper to support their claims, and provide well-explained reasoning for their critiques and suggestions. Both reviews are well-structured and easy to understand.\n**Better Assistant:** Tie\n- Tie: Both reviews are equally strong in communication clarity. They both effectively convey their feedback in a clear, concise, and well-supported manner.\n\n---\n**Comprehensive Analysis:**\nBoth Assistant A and Assistant B provide strong and insightful reviews of the paper. They both correctly identify the paper's contributions and limitations, and offer valuable suggestions for improvement.\n\nAssistant A's review is characterized by its breadth of analysis, covering a wider range of aspects including practical applicability, theoretical foundation, generalizability, innovation, interpretability, and justification of claims. Assistant A's suggestions are also broader, encompassing dimensionality reduction, theoretical analysis, and interpretability enhancements.\n\nAssistant B's review is distinguished by its depth of analysis in empirical validation and experimental design. It focuses more on the experimental setup, potential confounding factors like training data size, and limitations in experimental scope, such as the analysis of vision encoders and layer combinations. Assistant B's suggestions are more focused on specific experimental variations to strengthen the empirical evidence.\n\nBoth reviews are technically accurate, constructive, analytically deep, and clearly communicated. They approach the paper from slightly different angles, with Assistant A taking a broader perspective and Assistant B focusing more on empirical rigor. However, both perspectives are valuable and complementary.\n\n**Overall Judgment:**\n**Evidence-Based Reason:** Both Assistant A and Assistant B provide reviews of comparable high quality. They are both technically sound, offer constructive criticism, demonstrate analytical depth, and communicate clearly. While they emphasize slightly different aspects of the paper and offer different but equally valuable suggestions, neither review is demonstrably superior to the other in overall quality. Both reviews would be highly beneficial to the authors for improving their paper.\n**Better Assistant:** Tie\n- Tie: Both reviews are comparable in overall quality. They both provide strong, insightful, and constructive feedback, making them equally valuable for the authors. The slight differences in their focus and emphasis do not make one review definitively better than the other.\n```"}
{"id": "sqRgz88TM3", "year": 2024, "paper_context": "\\title{VFLAIR: A Research Library and Benchmark for Vertical Federated Learning}\n\n\\begin{abstract}\n  Vertical Federated Learning (VFL) has emerged as a collaborative training paradigm that allows participants with different features of the same group of users to accomplish cooperative training without exposing their raw data or model parameters. VFL has gained significant attention for its research potential and real-world applications in recent years, but still faces substantial challenges, such as in defending various kinds of data inference and backdoor attacks. Moreover, most of existing VFL projects are industry-facing and not easily used for keeping track of the current research progress. We also benchmark $11$ attacks and $8$ defenses performance under different communication and model partition settings and draw concrete insights and recommendations on the choice of defense strategies for different practical VFL deployment scenarios. \n  \n\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\nThe concept of Federated Learning (FL) was first introduced by Google in 2016~\\citep{McMahan2016fl} describing a cross-device scenario where millions of mobile users collaboratively train a shared model using their local private data\nwithout centralizing these data. This scenario is regarded as Horizontal FL (HFL)~\\citep{yang2019federatedbook} as data are partitioned by sample. In another type of FL, regarded as Vertical FL (VFL)~\\citep{yang2019federatedbook}, data are partitioned by feature. VFL is often applied in industrial collaborative learning scenarios where each organization controls disjoint features of a common group of users. In VFL, local data and local model are kept private at each participant. Instead, local model outputs and their gradients are transmitted between parties.\n\nVFL has drawn increasing attention from both academic and industry in recent years with hundreds of research papers published every year and a number of open-sourced projects released, including FATE~\\citep{FATE,liu2021fate}, Fedlearner~\\citep{Fedlearner}, PaddleFL~\\citep{PaddleFL}, Pysyft~\\citep{ryffel2018generic,romanini2021pyvertical}, FedTree~\\citep{fedtree2022}, and FedML~\\citep{he2020fedml}. Real-world industrial cases are also emerged in the field of advertising~\\citep{cai2020Bytedance,tencent2021} and finance~\\citep{cheng2020federated,Cheng2022DigitalEra} etc. However, mainstream VFL projects such as FATE are industrial grade and not designed for keeping up with research advances.  %, to name a few.\n\nMeanwhile, research interests for VFL have been growing rapidly over the past years, focusing on improving various aspects of VFL protocols, such as communication efficiency~\\citep{fu2021vf2boost,liu2022fedbcd,castiglia2022compressed,fu2022towards}, robustness to attacks~\\citep{liu2021rvfr,cheng2021secureboost,li2022label,zou2022defending,zou2023mutual,sun2022label,yang2022differentially}, model utility~\\citep{li2022semi,Yitao2022multiview,feng2020multi,feng2022semisupervised}, and fair incentive designs~\\citep{liu2021achieving,qi2022fairvfl}. % are two main lines of reserach work considering VFL. \nFor communication efficiency, methods like decrease communication rounds using multiple local updates between each round~\\citep{liu2022fedbcd,fu2022towards} or compress information~\\citep{castiglia2022compressed} have been proposed. As for data security and privacy, various attacks injected by one or multiple parties aiming to either steal other parties' private label~\\citep{li2022label,fu2021label,zou2022defending}, private features~\\citep{jin2021cafe,luo2021feature,li2022ressfl,jiang2022comprehensive,ye2022feature}, sensitive attributes~\\citep{Song2020Overlearning} and sample relations~\\citep{qiu2022your}, or negatively impact the model behavior~\\citep{liu2021rvfr,zou2022defending} have been put forward. Multiple defending methods have also been proposed to tackle these threats, including adding noise~\\citep{dwork2006DP,zou2022defending,li2022label}, sparsifying gradients~\\citep{aji2017sparse,fu2021label,zou2022defending}, discreting gradients~\\citep{fu2021label}, label differential privacy~\\citep{ghazi2021deep,yang2022differentially}, adding distance correlation regularizor~\\citep{sun2022label,vepakomma2019reducing}, disguising labels~\\citep{zou2022defending}, adding mutual information regularizer~\\citep{zou2023mutual}, adversarial training~\\citep{sun2021defending,li2022ressfl} or performing robust feature recovery~\\citep{liu2021rvfr}. However, each of these defenses are evaluated under specific tasks and settings, lacking of key insights and metrics on evaluating these defense strategies to defend all possible attacks in practical deployment.\n\nTo facilitate future research for VFL, we introduce a lightweight and comprehensive VFL framework, namely \\verb|VFLAIR|, which includes not only basic VFL training and inference for a variety of models and settings but also efficiency enhancement techniques and multiple defense methods that mitigate potential threats. Moreover, we perform extensive experiments on combinations of the above settings using multiple datasets to provide different perspectives on VFL efficiency and safety. We believe \\verb|VFLAIR| and these benchmark results will provide researchers with useful tools and guidance for their future work. Our contributions are summarized in the following:\n\n\\textbf{(1).} We design \\verb+VFLAIR+, a lightweight and extensible VFL framework that aims to facilitate research development of VFL (see \\cref{fig:VFLAIR}). We design standardized pipelines for VFL training and validation, supporting $13$ datasets, $29$ different local model architectures including linear regression, tree and neural networks, $6$ different global models, $2$ model partition settings, $5$ communication protocols, $1$ encryption method, $11$ attacks and $8$ defense methods, each implemented as a distinct module and can be easily extended.\n\n\\textbf{(2).} We propose new evaluation metrics and modules, and perform extensive experiments to benchmark various perspectives of VFL, from which we draw key insights on VFL system design choice, in order to promote future development and practical deployment of VFL. % Performance of VFL models trained with standard VFL training, attack performance of $11$ attacks, defense capability of $8$ defense methods are evaluated under both aggVFL and splitVFL with communication protocol using both FedSGD and FedBCD.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.89\\linewidth]{figures/VFLAIR_7_5.pdf}\n    \\caption{{An overview of the Components of VFLAIR.}} % After specifying the settings of VFL system, attacks and defense methods can be evaluated on top of the system. Overall evaluation metrics can be further exploit to get recommendation for selecting a proper defense.\n\\label{fig:VFLAIR}\n\\end{figure}\n\n\n\n\\section{Related Work}\n\n\nA number of open-source FL projects have been developed supporting VFL. FATE~\\citep{FATE,liu2021fate} is an industry-grade FL project which supports a variety of model architectures and secure computation protocols; Fedlearner~\\citep{Fedlearner} is specialized in advertising scenarios; PaddleFL~\\citep{PaddleFL} supports 2-party and 3-party VFL with MPC protection; Pysyft~\\citep{ryffel2018generic,romanini2021pyvertical} introduces PyVertical, which focus on SplitNN-type of VFL settings; FedTree~\\citep{fedtree2022} focuses on tree-based VFL only; FedML~\\citep{he2020fedml} supports basic training of VFL with logistic regression models. Real-world industrial applications have been witnessed in domains such as advertising~\\citep{cai2020Bytedance,tencent2021} and finance~\\citep{cheng2020federated,Cheng2022DigitalEra}. These works demonstrate the widespread interest and the practical significance of VFL. \nHowever, these works are often relatively heavy-weight as they are designed for industrial deployment. \nOn the other hand, most existing benchmarks on FL focus on HFL scenario~\\citep{chai2020fedeval,lai2022fedscale,zhang2023fedaudio}. For VFL, ~\\citep{kang2022framework} evaluates several defense strategies for data reconstruction attacks; SLPerf~\\citep{zhou2023slperf} focuses on benchmarking and comparing various kinds of splitNN scenarios like splitVFL. No existing work provides a comprehensive evaluation covering a variety of key aspects of VFL settings, including model performance, communication efficiency and robustness to attacks. Due to space limitation, %{\\color{red}{we only include previous works on VFL Framework and Benchmarks here}} but \n} \\tianyuan{No, at least currently, no.}\n\n\n\n\\section{VFL Framework}\n\nIn a typical VFL setting with $K$ parties, each party owns their local private feature $\\{X_{k}\\}_{k=1}^K$  and local model $\\{G_{k}\\}_{k=1}^K$ with parameters $\\{\\theta_{k}\\}_{k=1}^K$ respectively. Only one party controls the private label information $Y$ and is referred to as \\textit{active} party while other parties are referred to as \\textit{passive} parties.\nThe active party also controls a global trainable model parameterized by $\\varphi$ (splitVFL) or global non-trainable function $F$ (aggVFL) to aggregate each party's local model output. Note in tree-based VFL the global function is an aggregation function that identifies the optimal feature split based on feature splitting information received from all parties. %The classification task is also regarded as the main task since it is the purpose for the collaboration between parties and reflects the utility of the VFL model. \nWithout loss of generality, we assume that the $K^{th}$ party is the active party while other $K-1$ parties are passive parties.\nIn the collaborative training process of NN-based VFL, each party computes its local feature embedding $H_k=G_k(X_k,\\theta_k), k=1,\\dots,K$. The active party collects $\\{H_k\\}_{k=1}^K$ and gets the final prediction $\\hat{Y}=F(H_1,\\dots,H_K, \\varphi)$. %Based on whether $F$ is a trainable model or a non-trainable function, we follow~\\citep{liu2022vertical} to categorize VFL systems into splitVFL and aggVFL separately. \nThe loss $\\mathcal{L}=\\ell(Y,\\hat{Y})$ is calculated at the active party. The gradient w.r.s. to $H_k$ as $g_k = \\frac{\\partial \\mathcal{L}}{\\partial H_k}, k=1,\\dots,K$ are then calculated and transmitted back to each party by the active party. Using these gradients, each party performs local model updates by SGD using $\\nabla_{\\theta_k}\\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial \\theta_k}=\\frac{\\partial \\mathcal{L}}{\\partial H_k}\\frac{\\partial H_k}{\\partial \\theta_k}, k=1,\\dots,K$. Also the active party performs model update with SGD on global model $F$ if it is trainable using $\\nabla_{\\varphi}\\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial \\varphi}$. In the inference procedure, the same is done but without the backward gradient descent to get the prediction of labels. If the exchange of $H_k$ and $g_k$ is performed each round, such VFL protocol is referred as FedSGD protocol. On the other hand, if communication is done every $Q>1$ steps of local updates, such protocol is referred to as FedBCD~\\citep{liu2022fedbcd}. %{\\color{red}{Also, communication protocols that quantifies $H_k$ (using only $2^{b}$ bits for each element), or sparsifies $H_k$ by preserving only Top-k useful elements (using only $0<r<1$ proportion of the total elements), before information transmission to reduce communication cost, are referred to as Quantize~\\citep{castiglia2022compressed} and Top-k~\\citep{castiglia2022compressed} respectively. Further, cached FedBCD communication protocol using different batches of data for local update steps are termed CELU-VFL~\\citep{fu2022towards}.}}\n\n\n\n\n\\section{Overview of VFLAIR}\n\n\n\\textbf{Implemented Components.} An overview of the components of \\verb|VFLAIR| is shown in \\cref{fig:VFLAIR}.\n\\verb|VFLAIR| incorporates not only basic VFL training and testing process for both NN-based and tree-based VFL of various settings, but also multiple existing efficiency enhancement techniques, data leakage and model utility impairing attacks as well as defending methods that aim to mitigate potential threats. \\verb|VFLAIR| provides support for both aggVFL and splitVFL with easily adjustable model architectures. %For tree-based VFL, \\verb|VFLAIR| accommodates XGBoost and Random Forest for aggregation. \nCurrently, \\verb|VFLAIR| supports $5$ communication protocols %including FedSGD (vanilla protocol), FedBCD~\\citep{liu2022fedbcd}, CELU-VFL~\\citep{fu2022towards}, Quantize~\\citep{castiglia2022compressed} and Top-k~\\citep{castiglia2022compressed} \nto improve communication efficiency. Also, $11$ existing attacks and $8$ \ndefenses are supported. Moreover, \\verb|VFLAIR| supports the comprehensive assessment of defense performance using carefully designed metrics (see \\cref{sec:metrics_definition}), based on which defense strategy recommendations can be provided. Paillier Encryption~\\citep{cheng2021secureboost} is also supported to further protect transmitted results.\nIn total, $13$ datasets from  a diverse range of industrial domains, including but not limited to medical, financial, and recommendation\nare supported.\n\n\\textbf{How to use and extend.} \\verb|VFLAIR| is a light-weight and comprehensive VFL framework that can be launched on a single GPU or CPU (see \\cref{tab:hardware} for its system requirement compared to FATE). \\verb|VFLAIR| facilitates the easy integration of different datasets for model training and inference through simple dataset loading and partitioning functions. New attacks and defenses can be quickly incorporated into the framework thanks to the modular structure.  % VFL model utility and its susceptibility to attacks under various defense strategies by using $5$ carefully designed evaluation metrics. Furthermore,\n\n\n\n\\section{VFL Benchmark}\n\n\n\\subsection{VFL Settings, Models and Datasets} \nUsing \\verb|VFLAIR|, We benchmark the VFL main task performance using $13$ datasets including MNIST~\\citep{MNISTdataset}, CIFAR10~\\citep{krizhevsky2009learning}, CIFAR100~\\citep{krizhevsky2009learning}, NUSWIDE~\\citep{NUSWIDEdataset}, Breast Cancer~\\citep{street1993nuclear}, Diabetes~\\citep{Diabetes1999dataset}, Adult Income~\\citep{AdultIncome1996dataset}, Criteo~\\citep{guo2017deepfm_Criteo}, Avazu~\\citep{qu2018product_Avazu}, Cora~\\citep{mccallum2000automating_Cora}, News20~\\citep{lang1995News20},Credit~\\citep{Dua:2019} and Nursery~\\citep{Dua:2019}.\n\nWe explore $2$ distinct architectures, namely aggVFL and splitVFL, and comprehensively benchmark their performance. For global model $F$, a global softmax function is applied under aggVFL setting while a 1-layer fully-connected model serves as the global model for splitVFL setting (except for Cora dataset, for which a 1-layer graph convolution layer is applied). \nAdditionally, we investigate the impact of different communication protocols by comparing FedBCD~\\citep{liu2022fedbcd} ($Q=5$) and CELU-VFL~\\citep{fu2022towards} ($Q=5,W=5$), as well as compression mechanisms Quantize ($b=16$)~\\citep{castiglia2022compressed} and Top-k ($r=0.9$)~\\citep{castiglia2022compressed} to the conventional FedSGD, as discussed in \\cref{sec:vfl_setting} and further provide insights into the communication cost reduction achieved by communication efficient protocols,\nas well as the impact of FedBCD when various attacks and defenses are deployed. % demonstrating its effectiveness in enhancing efficiency.\nWe also evaluate the impact of the number of participating parties as well as the type of local model (logistic regression, tree, NN) on the main task performance of VFL. For tee-based VFL, we further benchmark both Random Forrest and XGBoost algorithms. Moreover, for \ntree-based VFL, we employ Paillier Encryption~\\citep{cheng2021secureboost} to protect transmitted information and measure its impact on computation efficiency. %such as labels and gradients. {\\color{red}{We measure the execution time along side the model performance across various tree-based VFL aggregation algorithms with and without the application of Paillier Encryption, shedding light on the execution overhead incurred by encryption.}} \n\n\n\\subsection{Attacks and Defenses} \\label{subsec:attacks}\n \n\\begin{table}[!tb]\n\\caption{Summary of attacks for NN-based VFL}\n\\label{tab:attacks}\n\\resizebox{0.998\\linewidth}{!}{\n    \\begin{tabular}{c|c|c|c|c}\n    \\toprule\n    Attack Type & Attack & \\shortstack{Requirements\\\\/ Limitations} & \\shortstack{Attacker\\\\Party} & Attack Performance (AP) \\\\\n    \\midrule\n    \\multirow{7}{*}{\\shortstack{Label\\\\Inference\\\\(LI)}} & \\shortstack{Norm-based Scoring (NS)~\\citep{li2022label}} & \\multirow{2}{*}{\\shortstack{binary classification,\\\\sample-level}} & \\multirow{7}{*}{passive} & \\multirow{2}{*}{\\shortstack{AUC of\\\\inferred labels}}\\\\\n    \\cline{2-2}\n    \\\\[-1em]\n     & \\shortstack{Direction-based Scoring (DS)~\\citep{li2022label}} &  &  & \\\\\n    \\cline{2-3}\n    \\cline{5-5}\n    \\\\[-1em]\n     & \\shortstack{Direct Label Inference (DLI)~\\citep{li2022label,zou2022defending}} & sample-level &  & \\multirow{5}{*}{\\shortstack{ratio of correctly\\\\inferred labels\\\\ \\,}}\\\\\n    \\cline{2-3}\n    \\\\[-1em]\n    & Batch-level Label Inference (BLI)~\\citep{zou2022defending} & - &  & \\\\\n    \\cline{2-3}\n    \\\\[-1em]\n    & Passive Model Completion (PMC)~\\citep{fu2021label} & \\multirow{2}{*}{\\shortstack{auxiliary labeled\\\\data for each class}} &  & \\\\\n    \\cline{2-2}\n    \\\\[-1em]\n    & Active Model Completion (AMC)~\\citep{fu2021label} &  &  & \\\\\n    \\cline{1-5}\n    \\\\[-1em]\n    \n    \n    \\multirow{3}{*}{\\shortstack{Feature\\\\Reconstruction\\\\(FR)}} & \\shortstack{Generative Regression Network (GRN)~\\citep{luo2021feature}} & \\shortstack{black-box} & \\multirow{3}{*}{\\shortstack{active}} & \\multirow{3}{*}{\\shortstack{$1-\\text{MSE}(U_0,U_{rec})$ }}\\\\\n    \\cline{2-3}\n    \\\\[-1em]\n     & \\shortstack{Training-based Back Mapping by\\\\model inversion (TBM)~\\citep{li2022ressfl}} & \\shortstack{white-box,\\\\auxiliary i.i.d. data} &  & \\\\\n    \\cline{1-5}\n    \\\\[-1em]\n    \\shortstack{Targeted\\\\Backdoor (TB)\\\\ \\,} & \\shortstack{Label Replacement\\\\Backdoor (LRB)~\\citep{zou2022defending}} & \\shortstack{$\\geq1$ sample of\\\\target class} & \\shortstack{passive\\\\ \\,\\\\ \\,} & \\shortstack{ratio of triggered samples\\\\inferred as target class}\\\\\n    \\cline{1-5}\n    \\\\[-1em]\n    \\multirow{2}{*}{\\shortstack{Non-targeted\\\\Backdoor (NTB)}} & Noisy-sample Backdoor (NSB)~\\citep{zou2023mutual} & - & \\multirow{2}{*}{passive} & \\multirow{2}{*}{\\shortstack{MP difference between total\\\\and noisy/missing samples}}\\\\\n    \\cline{2-3}\n    \\\\[-1em]\n     & Missing Feature (MF)~\\citep{liu2021rvfr} & - &  & \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\nWe benchmark the performance of $11$ attacks with $8$ defenses on $3$ datasets including MNIST~\\citep{MNISTdataset}, CIFAR10~\\citep{krizhevsky2009learning} and NUSWIDE~\\citep{NUSWIDEdataset}.\nFor these evaluations, we mainly consider a VFL setting with $1$ active party and $1$ passive party following original works~\\citep{li2022label,luo2021feature,li2022ressfl}, denoted as party $a,p$ respectively, with each party owning their local feature $X_a, X_p$ and local model $G_a, G_p$.\nThe local model output of the active and passive party are denoted as $H_a, H_p$ respectively.\nWe summarized the evaluated attacks in \\cref{tab:attacks}. Note in \\cref{tab:attacks}, NS and DS attacks can only be applied to binary classification scenarios; \"sample-level\" indicates that the attack requires gradient information for each sample, whereas \"batch-level\" means only batch-level gradients information are available; \"black-box\" indicates that the model is kept private at the party under attack, but can be queried by the attacker and honestly return the output to the attacker, whereas \"white-box\" means the attacker has access to the model; $\\text{MSE}(U_0,U_{rec})=\\mathbb{E}[(u_0^{(f)}-u_{rec}^{(f)})^2]$ where $u_0^{(f)},u_{rec}^{(f)}$ are the $f^{th}$ feature of original input $U_0$ and recovered input $U_{rec}$ respectively. LI, FR and NTB attacks are inference time attacks that are launched separately from VFL training procedure while only TB attacks are training time attacks. \n Defense methods are summarized in \\cref{tab:defense_param} with respective hyper-parameters. \n \n\n\\begin{table}[!tb]\n\\caption{Summary of defense methods and tested hyper-parameter values for NN-based VFL.} % \\tianyuan{parameter double check.}\n\\label{tab:defense_param}\n\\resizebox{0.998\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c|c|c|c}\n    \\toprule\n        Defense & Methodology & Hyper-parameter & Hyper-parameter Values \\\\\n    \\midrule\n        G-DP~\\citep{dwork2006DP,fu2021label,zou2022defending} & add noise to gradients or local prediction & DP Strength & $0.0001,0.001,0.01,0.1$\\\\\n        L-DP~\\citep{dwork2006DP,fu2021label,zou2022defending} & add noise to gradients or local prediction & DP Strength & $0.0001,0.001,0.01,0.1$\\\\\n        GS~\\citep{aji2017sparse,fu2021label,zou2022defending} & drop gradient elements close to $0$ & Sparsification Rate & $95.0\\%,97.0\\%,99.0\\%,99.5\\%$\\\\ %\n        GPer~\\citep{yang2022differentially} & perturb gradient with that of other class & Perturbation Strength & $0.0001,0.001,0.01,0.1$\\\\\n        dCor~\\citep{sun2022label,vepakomma2019reducing} & distance correlation regularization & Regularizer Strength & $0.0001,0.01,0.1,0.3$\\\\\n        CAE~\\citep{zou2022defending} & disguise label & Confusion Strength $\\lambda$ & $0.0,0.1,0.5,1.0$\\\\\n        DCAE~\\citep{zou2022defending} & discrete gradient in addition to CAE & Confusion Strength $\\lambda$ & $0.0,0.1,0.5,1.0$\\\\\n    MID~\\citep{zou2023mutual} & mutual information (MI) regularization & Regularizer Strength $\\lambda$ & $0.0,1e^{-8},1e^{-6},1e^{-4},0.01,0.1,1.0,1e^{2},1e^{4}$ \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\n\\subsection{Evaluation Metrics}\\label{sec:metrics_definition}\n\\textbf{Main Task Performance (MP).}\nMP is defined as the final model prediction accuracy on the test dataset, which reveals the utility of the VFL system.\n\n\\textbf{Communication and Computation Efficiency.} Number of communication rounds (\\#Rounds) and the amount of data transferred for each round (Amount) are used for measuring communication efficiency. Execution Time (Exec.Time) is used to measure computation efficiency. \n\n\\textbf{Attack Performance (AP).} %As we also include various kinds of attacks and defense in our platform and benchmark experiments, we define AP so as to evaluate the success rate of a given attack which also reflects the vulnerability of a VFL system to that given attack. \nThe definition of AP varies with respect to the type of the attack and is summarized in \\cref{tab:attacks}. \n\n\\textbf{Defense Capability Score (DCS).} %To evaluate the defense methods, \nIntuitively, an ideal defense should not compromise the utility of the original main task and should thwart the attack completely. Therefore, considering that both AP and MP are key metrics to evaluate defenses. We further propose \\textbf{Defense Capability Score (DCS)}, to directly compare all the defenses under one unified metric. Let $df=(\\text{AP},\\text{MP})$ represents the performance of a defense on an AP-MP graph, then we define its defense capability score (DCS) based on the distance between $df$ to an ideal defense $df^{*}=(\\text{AP}^{*}, \\text{MP}^{*})$.  MP$^{*}$ is the MP of VFL without defense and AP$^{*}$ is set to $0.0$ representing the performance of a completely incapable attacker. Then, we formulate the definition of DCS as:\n\\begin{equation} \\label{eq:dcs}\n \\setlength\\abovedisplayskip{0.05cm}\n \\setlength\\belowdisplayskip{0.05cm}\n    \\text{DCS} = \\frac{1}{1+D(df,df^{*})} = \\frac{1}{1+\\sqrt{(1-\\beta)(\\text{AP}-\\text{AP}^{*})^2+\\beta(\\text{MP}-\\text{MP}^{*})^2}},\n\\end{equation}\n\n\\begin{wrapfigure}{l}{0.36\\textwidth}\n    \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/score/DCS_LI_v4.pdf}\n    \\caption{A visual illustration example of DCS. The numbers on the contour lines are DCSs calculated with $\\beta=0.5$.} % \\yang{make the points larger and font bigger}\n    \\label{fig:dcs}\n\\end{wrapfigure}\n\nwhere $D(\\cdot)$ is a user-defined distance function. Here we use Euclidean distance with an adjustable trade-off weighting parameter $\\beta$. A visualization of DCS on an AP-MP graph with $\\beta=0.5$ can be seen in \\cref{fig:dcs}. %\\yang{add a sketch figure to illustrate the distance}\\tianyuan{DONE, but this figure takes up too much space.}). \nA point closer to the bottom-right corner of an AP-MP graph has a higher DCS score indicating a better defense capability, consistent with intuition. $\\beta=0.5$ is used in our experiments. %Also, $\\hat{m}_{j}^{*}$ denotes the \\textit{ideal MP} defined as the MP of a vanilla VFL without defense, representing no accuracy loss on the main task and $\\hat{a}_{j}^{*}$ denotes the \\textit{ideal AP} defined as the worst AP for each attack (each attack type) as defined above in \\cref{subsubsec:ap_mp} and \\cref{tab:attack_performance_definition}.\n\n\\textbf{Type-level Defense Capability Score (T-DCS).} T-DCS is the DCS score averaged by attack type. %proposed to evaluate the capability of a defense method on a certain type of attack. %we considered in \\cref{subsec:attacks} rather than on a single attack. \nTreating all $I_j$ attacks of the same attack type $j$ as equally important, we average DCS for each attack $i$ to get T-DCS for attack type $j$:% $j\\in \\mathcal{A}$:\n\\begin{equation} \\label{eq:t-dcs}\n \\setlength\\abovedisplayskip{0.05cm}\n \\setlength\\belowdisplayskip{0.05cm}\n    \\text{T-DCS}_j = \\frac{1}{I_j} \\sum_{i=1}^{I_j} \\text{DCS}_i .\n\\end{equation}\n\n\\textbf{Comprehensive Defense Capability Score (C-DCS).} C-DCS is a comprehensive assessment of the capability of a defense strategy with respect to all kinds of attacks and is a weighted average of T-DCS as shown in \\cref{eq:c-dcs}:\n\\begin{equation} \\label{eq:c-dcs}\n \\setlength\\abovedisplayskip{0.05cm}\n \\setlength\\belowdisplayskip{0.05cm}\n    \\text{C-DCS} = \\sum_{j\\in \\mathcal{A}} w_j \\text{T-DCS}_j, \\,\\, \\text{with} \\sum_{j \\in \\mathcal{A}} w_j = 1.0 \\, .\n\\end{equation}\nWeights $\\{w_j\\}_{j \\in \\mathcal{A}}$ can be tailored to user preference. In our experiments, we simply use an unbiased weight $w_j=\\frac{1}{|\\mathcal{A}|}$ for each attack type $j \\in \\mathcal{A}=\\{\\text{LI}, \\text{FR}, \\text{TB}, \\text{NTB}\\}$.\n\n\n\n\n\n\n\n\\section{{Evaluation Results}\n\n\n\n\n\n\\subsection{VFL Main Task Performance} \\label{subsec:MP}\nWe first comprehensively evaluate the impact of various settings on the performance of VFL.\n\n\\textbf{Model Partition.}\nThe splitVFL setting yields a comparable or slightly higher MP  compared to aggVFL on most datasets, due to the additional trainable layer serving as global model, evidenced by results from \\cref{tab:NN_MP,tab:4party_MP,tab:real_world_dataset_MP}.\n\n\\textbf{Communication Protocols.} %We compare $5$ different kinds of communication protocols in VFL. \nAs shown in \\cref{tab:NN_MP} and \\cref{tab:communication_MP}, compared to FedSGD, FedBCD and CELU-VFL exhibit comparable MP across all datasets with fewer communication rounds, supporting their efficacy in reducing communication overhead. Quantize and Top-k compress the transmitted data and successfully reduce the communication cost per round, but may result in an increase in communication rounds. %CELU-VFL also effectively reduces the \\#Rounds compared to FedSGD, and is slightly less than that of FedBCD\n\n\\textbf{Encryption.} %{\\color{red}{We compare the MP and execution time with and without HE protection using Paillier Encryption in \nFor tree-based VFL, we consider two models with and without Paillier Encryption using 512-bit key size in \\cref{tab:tree_MP}. Note that XGBoost with Paillier Encryption is equivalent to SecureBoost~\\citep{cheng2021secureboost}. Although MP values are consistent regardless of encryption, the execution time experiences a notable increase of 3 to 20 times when encryption is applied due to the additional encryption and decryption process. \n\n\\textbf{Number of Participants.}\nImpact of number of participants are shown in \\cref{tab:4party_MP}. \nA slightly lower MP is achieved using fewer communication rounds as the number of participants increases, demonstrating the increasing challenges brought by multi-party collaboration.\n\n\\textbf{Model Architectures.}\n% {\\color{red}{tree-based VFL performs better on Credit dataset, while NN-based VFL performs better on Nursery dataset.\n\n\\textbf{Real world datasets.}\nAdditional results on Criteo~\\citep{guo2017deepfm_Criteo}, Avazu~\\citep{qu2018product_Avazu}, Cora~\\citep{mccallum2000automating_Cora} and News20~\\citep{lang1995News20} datasets using domain specific models (e.g. Wide\\&Deep Model~\\citep{cheng2016wide} for Criteo and Avazu, GNN for Cora) are provided in \\cref{tab:real_world_dataset_MP}\n, as they are considered for typical VFL applications, such as in recommendation problems. %click through rate prediction in advertising, node classification in citation network and classification of news data\n\\begin{table}[!tb]\n\\caption{MP under $4$ different settings of NN-based VFL. $Q=5$ when FedBCD is applied. In \"\\#Rounds\" column, the first and second numbers are the communication rounds needed to reach the specified MP for FedSGD and FedBCD respectively.}\n\\label{tab:NN_MP}\n\\resizebox{0.998\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c||c|c|c||c|c|c}\n    \\toprule\n    Dataset & aggVFL, FedSGD & aggVFL, FedBCD & \\#Rounds & splitVFL, FedSGD & splitVFL, FedBCD & \\#Rounds \\\\\n    \\midrule\n    MNIST & 0.972$\\pm$0.001 & 0.971 $\\pm$0.001 & 150 / 113 & 0.973$\\pm$0.001 & \\textbf{0.974$\\pm$0.001} & 180 / 143 \\\\\n    NUSWIDE & 0.887$\\pm$0.001 & 0.882$\\pm$0.001 & 60 / 26 & \\textbf{0.888$\\pm$0.001} & 0.884$\\pm$0.001 & 60 / 29 \\\\ \n    Breast Cancer & 0.914$\\pm$0.033 & 0.919$\\pm$0.029 & 5 / 3 & \\textbf{0.925$\\pm$0.028} & 0.907$\\pm$0.045 & 5 / 4 \\\\\n    Diabetes & 0.755$\\pm$0.043 & 0.736$\\pm$0.021 & 15 / 13 & \\textbf{0.766$\\pm$0.024} & 0.746$\\pm$0.039 & 15 / 11 \\\\\n    Adult Income & 0.839$\\pm$0.006 & 0.841$\\pm$0.005 & 17 / 15 & 0.842$\\pm$0.004 & \\textbf{0.842$\\pm$0.005} & 30 / 13 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\n\\begin{table}[!tb]\n\\caption{MP under 2-party VFL verses MP under 4-party VFL  under $4$ different settings of NN-based VFL using FedSGD communication protocol. \"\\#Rounds\" has the same meaning as in \\cref{tab:NN_MP}.}\n\\label{tab:4party_MP}\n\\resizebox{0.998\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c|c||c|c||c|c}\n    \\toprule\n    Dataset  & ~ & aggVFL, 2-party & aggVFL, 4-party & splitVFL, 2-party & splitVFL, 4-party\\\\\n    \\midrule\n    \\multirow{2}{*}{CIFAR10} & MP & 0.790$\\pm$0.003 & 0.747$\\pm$0.003 & \\textbf{0.798$\\pm$0.010} & 0.762$\\pm$0.003 \\\\\n    ~ & \\#Rounds & 244$\\pm$16 & 205$\\pm$12 & 238$\\pm$14 & 173$\\pm$3 \\\\\n    \\midrule\n    \\multirow{2}{*}{CIFAR100} & MP & \\textbf{0.454$\\pm$0.006} & 0.417$\\pm$0.008 & 0.423$\\pm$0.005 & 0.382$\\pm$0.004 \\\\\n    ~ & \\#Rounds & 130$\\pm$11 & 124$\\pm$2 & 125$\\pm$2 & 100$\\pm$1 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\\begin{table}[!tb]\n\\caption{MP and execution time under 2 different types of tree-based VFL.} %\\yang{too many digits, round to fewer, especially for time}\n\\label{tab:tree_MP}\n\\resizebox{0.99\\linewidth}{!}{\n\\centering\n\\begin{tabular}{@{}c|c|c|c|c|c@{}}\n\\toprule\nDataset &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Random Forest \\\\ w/o Encryption\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}XGBoost\\\\ w/o Encryption\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Random Forest \\\\ w/ Encryption\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}XGBoost\\\\ w/ Encryption\\\\ (a.k.a. SecureBoost)\\end{tabular} \\\\ \\midrule\n\\multirow{2}{*}{Credit} &\n  MP &\n  0.816$\\pm$0.005 &\n  0.816$\\pm$0.004 &\n  0.816$\\pm$0.005 &\n  0.816$\\pm$0.004 \\\\\n &\n  \\multicolumn{1}{c|}{Exec.Time [s]} &\n  \\multicolumn{1}{c|}{138$\\pm$4} &\n  \\multicolumn{1}{c|}{366$\\pm$16} &\n  \\multicolumn{1}{c|}{410$\\pm$10} &\n  \\multicolumn{1}{c}{881$\\pm$6} \\\\ \n\\hline\n\\\\[-1em]\n\\multirow{2}{*}{Nursery} &\n  MP &\n  0.884$\\pm$0.010 &\n  0.890$\\pm$0.011 &\n  0.884$\\pm$0.010 &\n  0.890$\\pm$0.011 \\\\\n &\n  \\multicolumn{1}{c|}{Exec.Time [s]} &\n  \\multicolumn{1}{c|}{29$\\pm$2} &\n  \\multicolumn{1}{c|}{69$\\pm$4} &\n  \\multicolumn{1}{c|}{243$\\pm$5} &\n  \\multicolumn{1}{c}{1194$\\pm$21} \\\\ \n\\bottomrule\n\\end{tabular}\n}\n\\end{table}\n\n\\begin{table}[!tb]\n\\caption{\nMP, communication rounds (\\#Rounds), amount of information exchanged per round (Amount) under different communication protocols of NN-based VFL under aggVFL setting. %$Q=5$ when FedBCD and CELU-VFL are used, otherwise $Q=1$. For Quantize, $b=16$ while for Top-k, top $90\\%$ of elements are kept in forward local model prediction.\n'Total' column is the total amount that equals to \\#Rounds$\\times$Amount.}\n\\label{tab:communication_MP}\n\\resizebox{0.99\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c|cccc|cccc}\n    \\toprule\n        ~ & \\multicolumn{4}{c|}{MNIST} & \\multicolumn{4}{c}{NUSWIDE} \\\\\n        \\cline{2-9}\n        \\\\[-1em]\n        ~ & MP & \\#Rounds & Amount (MB) & Total (MB) & MP & \\#Rounds & Amount (MB) & Total (MB)\\\\ \n    \\midrule\n        FedSGD & \\textbf{0.972$\\pm$0.001} & 150 & 0.156 & 23.438 & \\textbf{0.887$\\pm$0.001} & 60 & 0.039 & 2.344 \\\\\n        FedBCD & 0.971$\\pm$0.001 & 113 & 0.156 & 17.656 & 0.882$\\pm$0.001 & 26 & 0.039 & 1.016 \\\\\n        Quantize & 0.959$\\pm$0.006 & 161 & 0.117 & 18.867 & 0.881$\\pm$0.002 & 94 & 0.029 & 2.754 \\\\\n        Top-k & 0.968$\\pm$0.001 & 150 & 0.148 & 22.266 & 0.887$\\pm$0.001 & 60 & 0.037 & 2.227 \\\\\n        CELU-VFL & 0.971$\\pm$0.002 & 105 & 0.156 & 16.406 & 0.880$\\pm$0.001 & 25 & 0.039 & 0.977 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\n\\begin{table}[!tb]\n\\caption{Comparison of aggVFL and splitVFL on MP, \\#Rounds, Amount, total communication cost, Exec.Time for reaching specified MP with $4$ real-world datasets of NN-based VFL with FedSGD communication protocol.}\n\\label{tab:real_world_dataset_MP}\n\\resizebox{0.99\\linewidth}{!}{\n    \\centering\n    \\begin{tabular}{c|ccccc|ccccc}\n    \\toprule\n        \\multirow{2}{*}{Dataset} & \\multicolumn{5}{c|}{aggVFL} & \\multicolumn{5}{c}{splitVFL} \\\\\n        \\cline{2-11}\n        \\\\[-1em]\n        ~ & MP & \\#Rounds & \\shortstack{Amount\\\\(MB)} & \\shortstack{Total\\\\(MB)} & Exec.Time [s] & MP & \\#Rounds & \\shortstack{Amount\\\\(MB)} & \\shortstack{Total\\\\(MB)} & Exec.Time [s]\\\\\n    \\midrule\n        Criteo & 0.715$\\pm$0.053& 2 & 0.125 & 0.250 & 0.190$\\pm$0.132 & 0.744$\\pm$0.001 & 3 & 0.125 & 0.375 & 0.234$\\pm$0.126\\\\\n        Avazu & 0.832$\\pm$0.001 & 5 & 0.125 & 0.625 & 0.517$\\pm$0.185 & 0.832$\\pm$0.001 & 9 & 0.125 & 1.125 & 1.203$\\pm$1.516\\\\ \n        Cora & 0.721$\\pm$0.004 & 11 & 0.145 & 1.591 & 0.205$\\pm$0.085 & 0.724$\\pm$0.012 & 13 & 0.145 & 1.880 & 0.270$\\pm$0.082 \\\\\n        News20-S5 & 0.882$\\pm$0.014 & 57 & 0.005 & 0.278 & 0.430$\\pm$0.076 & 0.893$\\pm$0.013 & 61 & 0.005 & 0.298 & 0.613$\\pm$0.269 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\n\\subsection{Attack and Defense Performance} \\label{subsec:attack_defense_performance}\n\n\\begin{figure}[!htb]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/cifar10/ALL_agg.pdf}\n  \\caption{MPs and APs for different attacks under defenses [CIFAR10 dataset, aggVFL, FedSGD]}\n  \\label{fig:cifar10_MPAP}\n\\end{figure}\n\nWe demonstrate attack and defense results of VFL on the AP-MP graph for each attack on MNIST, CIFAR10 and NUSWIDE datasets under aggVFL setting using FedSGD protocol in \\cref{fig:mnist_MPAP,fig:cifar10_MPAP,fig:nuswide_MPAP}.\nEach point in the figure represents a $(\\text{MP},\\text{AP})$ pair with the size of markers representing the relative magnitude of the corresponding defense hyper-parameter listed in \\cref{tab:defense_param}.\nNote that although we try to provide comprehensive evaluation for various defenses, we do not force defense onto attacks, meaning that if %we evaluate defense methods only on the specific attacks they were designed for. In other words, if \na defense mechanism is designed for mitigating label inference attacks only, we do not assess its effectiveness against FR attacks or backdoor attacks. \n\nWe further rank all the defenses of different hyper-parameters based on their C-DCS. Due to space limitation, we show representative results for NUSWIDE dataset in \\cref{tab:tiny_nuswide_dcs_ranking}.%Consequently, our experimental evaluations do not include assessments of GPer, CAE, and DCAE against FR attacks and NTB attacks, while GS is excluded from evaluations involving FR attacks.}}\n\n\n\\textbf{Attacks pose great threat to VFL.} Comparing the black squares illustrating the MP and AP of the attack against a VFL system without any defense in the sub-figures, we can observe that DS, DLI, BLI and TBM attacks are strong attacks with AP higher than $0.97$, while MF attacks are quite weak with AP below $0.1$.\n\n\\textbf{Defenses exhibit trade-offs between MP and AP.} For most of the attacks and defenses, we can observe an apparent trade-off between MP and AP, i.e. a lower AP is often gained with increasing harm of MP as defense strength grows, which can be controlled by adjusting defense hyper-parameters. \nAn increase of noise level in DP-G and DP-L, sparsification rate in GS, regularization hyper-parameter $\\alpha_d$ in dCor, confusional strength $\\lambda_2$ in CAE and DCAE, regularization hyper-parameter $\\lambda$ in MID or a decrease of DP budget $\\epsilon$ in GPer will lead to lower MP and AP.\n\n\\textbf{DCS rankings are consistent across various datasets and settings.} As shown in \\cref{tab:mnist_dcs_ranking,tab:nuswide_dcs_ranking,tab:cifar10_dcs_ranking}, the results of the C-DCS rankings are generally consistent across all $3$ datasets. %% namely MNIST, CIAFR10 and NUSWIDE, under the same aggVFL setting with FedSGD communication. Also, \nAs summarised in \\cref{fig:dcs_ranking_mean_std}, these results demonstrate the robustness of the proposed DCS metrics, as well as the stableness of relative performance of different defense methods.Note that, T-DCS$_{\\text{FR}}$ values are much lower than the T-DCS of other types, indicating that FR attacks are harder to defend than other attacks, which are consistent with human observation (see \\cref{fig:visualization_mnist_tbm} in \\cref{subsec:visualization_tbm}). %and more effective defense methods against FR attacks are in need.\n\n\\textbf{MID, L-DP and G-DP are effective on a wide spectrum of attacks.} MID demonstrates its capability of achieving a relatively lower AP while maintaining a higher MP compared to most other defenses as shown in \\cref{fig:mnist_MPAP,fig:nuswide_MPAP,fig:cifar10_MPAP} and  \\cref{tab:mnist_dcs_ranking,tab:nuswide_dcs_ranking,tab:cifar10_dcs_ranking}; L-DP and G-DP are also generally effective under most attacks with above average T-DCS and C-DCS; DCAE is effective in defending against LI attacks; %\\textbf{CAE is effective only against LI attacks that utilize merely the information of the current sample}%\\textbf{CAE consistently performs well across when defending against LI attacks that utilize merely the information of the current sample} by disguising label directly; \nGS demonstrates strong defense ability for most of the LI attacks but performs less than satisfactory on LRB attacks; GPer performs similar to DP-G and DP-L in defending against label related attacks; %as it targets at defending LI attacks and guarantees label-DP \\yang{what about other attac}ks?} \\tianyuan{Not evaluated... Do we need to include the explanation?}\ndCor is less effective in limiting AP under NTB attacks but is largely effective against PMC and AMC attacks as shown in \\cref{fig:mnist_MPAP,fig:nuswide_MPAP,fig:cifar10_MPAP}. \n\n\n\\begin{wrapfigure}{l}{0.55\\textwidth}\n  \\centering\n  \\includegraphics[width=0.99\\linewidth]{figures/mnist/mnist_rank_beta.pdf}\n  \\caption{Change of C-DCS ranking with the change of $\\beta$. [MNIST dataset, aggVFL, FedSGD]}\n  \\label{fig:change_beta_dcs_mnist}\n\\end{wrapfigure}\n\\textbf{Change in $\\beta$ does not significantly impact the C-DCS ranking.} $\\beta$ in \\cref{eq:dcs} represents users' trade-off preference on AP and MP when evaluating defenses, and can be adjusted. Here we use $\\beta =0.5$ for our main results. %If the user wants to place more attention on maintaining a high MP, hyper-parameter $\\beta$ in \\cref{eq:dcs} can be adjusted to a value larger than $0.5$. \n\\cref{fig:change_beta_dcs_mnist,fig:change_beta_dcs_cifar10,fig:change_beta_dcs_nuswide} show the change of the ranking results with the change of $\\beta$. Overall the relative rankings are not significantly impacted by $\\beta$, demonstrating the stableness of the comparison results among various defenses. As $\\beta$ grows to large values, e.g. $\\geq 0.9$, the metric places overly strong weight on MP, resulting in more variations on the rankings. Specifically, dCor ranks higher with the increase of $\\beta$ thanks to its better MP preservation at the cost of a weaker AP limitation.\n\\begin{table}[!tb]\n\\caption{T-DCS and C-DCS for All Defenses [NUSWIDE dataset, aggVFL, FedSGD]}\n\\label{tab:tiny_nuswide_dcs_ranking}\n\\resizebox{0.998\\linewidth}{!}{\n  \\centering\n   \\begin{tabular}{cc|cccccc|c}\n    \\toprule\n    \\textbf{\\makecell{Defense\\\\Name}} & \\textbf{\\makecell{Defense\\\\Parameter}} & \\bm{$T\\text{-}DCS_{LI_{2}}$} &\n    \\bm{$T\\text{-}DCS_{LI_{5}}$} & \\bm{$T\\text{-}DCS_{LI}$} & \\bm{$T\\text{-}DCS_{FR}$} & \\bm{$T\\text{-}DCS_{TB}$} & \\bm{$T\\text{-}DCS_{NTB}$} & \\bm{$C\\text{-}DCS$} \\\\ \n    \\midrule \n    MID  & 10000  & 0.7358 & 0.8559 & \\textbf{0.8159} & 0.5833 & \\textbf{0.7333} & 0.8707 & 0.7508  \\\\\n    MID  & 1.0    & 0.7476 & 0.8472 & 0.8140 & 0.5833 & 0.7331 & 0.8700 & 0.7501  \\\\\n    MID  & 100    & 0.7320 & 0.8536 & 0.8130 & 0.5833 & 0.7326 & \\textbf{0.8711} & 0.7500  \\\\\n    G-DP & 0.1    & 0.7375 & 0.8262 & 0.7966 & 0.5863 & 0.7282 & 0.8675 & 0.7447  \\\\\n    L-DP & 0.1    & 0.7389 & 0.8177 & 0.7915 & 0.5863 & 0.7258 & 0.8603 & 0.7410  \\\\\n    MID  & 0.1    & 0.7516 & 0.8259 & 0.8011 & 0.5833 & 0.7172 & 0.8563 & 0.7395  \\\\\n    MID  & 0.01   & 0.7280 & 0.8092 & 0.7822 & 0.5844 & 0.7151 & 0.8627 & 0.7361  \\\\\n    dCor & 0.3    & \\textbf{0.7641} & 0.8411 & 0.8155 & 0.5834 & 0.7289 & 0.8051 & 0.7332  \\\\\n    dCor & 0.0001 & 0.6496 & 0.6340 & 0.6392 & \\textbf{0.5864} & 0.6307 & 0.8287 & 0.6712 \\\\\n    \\hline\n    \\\\[-1em]\n    GS   & 99.0   & 0.7404 & 0.8060 & 0.7841 & -      & 0.6415 & 0.8408 & -      \\\\\n    CAE  & 1.0    & 0.6863 & 0.7822 & 0.7502 & -      & 0.6830  & -           \\\\\n    DCAE & 0.0    & 0.6669 & \\textbf{0.8660} & 0.7996 & -      & 0.6816 & -      & -       \\\\\n    GPer & 0.01   & 0.7386 & 0.8412 & 0.8070 & -      & 0.7193 & -      & -       \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}\n\\textbf{splitVFL is less vulnerable to attacks than aggVFL.}\nUsing DCS metrics, we directly compare all the aforementioned attacks and defenses under aggVFL and splitVFL settings to understand the impact of changing the model partition strategy on VFL's vulnerability against attacks. We mainly use the DCS gap, defined as $\\text{DCS}^{\\text{splitVFL}}-\\text{DCS}^{\\text{aggVFL}}$ for each attack-defense point. \\cref{fig:mnist_splitVFL_DCS_gap,fig:nuswide_split_DCS_gap} \\begin{wrapfigure}{l}{0.55\\textwidth}\n  \\centering\n  \\includegraphics[width=0.99\\linewidth]{figures/split_agg_compare/splitagg_hist_total.pdf}\n  \\caption{DCS gap Distribution, y-axis represents density [MNIST dataset, splitVFL/aggVFL, FedSGD]}   \n  \\label{fig:mnist_splitVFL_DCS_hist}           \n\\end{wrapfigure}\n \n\\cref{fig:mnist_splitVFL_DCS_hist,fig:nuswide_split_hist} displays the distribution of the DCS gaps depicted in \\cref{fig:mnist_splitVFL_DCS_gap,fig:nuswide_split_DCS_gap} respectively.\nAs all the black square points in \\cref{fig:mnist_splitVFL_DCS_gap} appear above or close to the red horizontal line at a value of $0.0$ (see also the blue histograms that appear mostly at the right of the vertical line at a value of $0.0$ in \\cref{fig:mnist_splitVFL_DCS_hist}), we can conclude that splitVFL is less vulnerable to attacks than aggVFL when no defense is applied. In addition, splitVFL has an overall positive effect on boosting defense performance against attacks as well, as most of the DCS gap is positive in the last subplot of \\cref{fig:mnist_splitVFL_DCS_gap} when no attack is applied. \n\n\\begin{wrapfigure}{l}{0.55\\textwidth}\n      \\centering\n      \\includegraphics[width=0.99\\linewidth]{figures/bcd_compare/BCD_hist_total.pdf}\n    \\caption{DCS gap Distribution, y-axis represents density [MNIST dataset, aggVFL, FedBCD/FedSGD]}\n  \\label{fig:mnist_FedBCD_DCS_hist}           \n\\end{wrapfigure}\n\n\\textbf{FedBCD is less vulnerable to attacks than FedSGD.} In addition, we compare DCS gap under FedSGD setting and FedBCD with $Q=5$ to assess the impact of different communication protocols on model's vulnerability to attacks. DCS gap is defined as $\\text{DCS}^{\\text{FedBCD}}-\\text{DCS}^{\\text{FedSGD}}$ for each attack-defense point. \n\nAs shown in \\cref{fig:mnist_FedBCD_DCS_hist}, the blue histograms generally appear on the right of the vertical line of value $0.0$, indicating that a system with FedBCD protocol is less vulnerable to attacks when no defense method is applied. In addition, a system with FedBCD also has an overall positive effect on boosting defense performance against FR and NTB attacks. This is evidenced by the fact that that the majority of DCS gaps are positive for FR and NTB attacks as shown in \\cref{fig:mnist_FedBCD_DCS_hist}.\nSimilar conclusions can be drawn from \\cref{fig:nuswide_FedBCD_hist,fig:nuswide_FedBCD_DCS_gap}.\n\n\n\\section{Conclusions and Limitations}\n\n\nIn this work, we introduce a light-weight VFL framework \\verb|VFLAIR| that implements basic VFL training and evaluation flow under multiple model partition, model architectures,communication protocols and attacks and defenses algorithms using datasets of different modality. We also introduce unified evaluation metrics and benchmark model utility, communication and computation efficiency, and defense performance under various VFL settings, which sheds lights on choosing partition, communication and defense techniques in practical deployment. \nCurrently, the library has limited implementations on cryptographic techniques. Combination of non-cryptograhic and cryptographic techniques would be an interesting next step and we plan to add more advanced privacy-preserving and communication-efficient methods to our library. %encryption implementation, since is not implemented for NN-based VFL, we will add this part in the near future.}}\n\n\n\n\\section{Reproducibility Statement}\n\n Our code is also available at\n\n\n\n\n", "DeepReviewer": "\n## Summary:\n\nThis paper introduces VFLAIR, a lightweight and extensible framework for Vertical Federated Learning (VFL), designed to facilitate research and practical deployment. The core contribution of this work lies in the creation of a comprehensive benchmark for VFL, encompassing a wide range of models, datasets, communication protocols, and, crucially, various attack and defense strategies. The authors implement 29 local model architectures, 6 global models, and 5 communication protocols, alongside 11 different attack methods and 8 defense mechanisms. The framework supports 13 datasets, including image, text, and tabular data, allowing for a diverse set of experiments. The authors benchmark the performance of these components under different communication and model partition settings, aiming to provide insights and recommendations on the choice of defense strategies for different practical VFL deployment scenarios. The empirical findings include evaluations of model utility, defense performance, and the computational overhead of different configurations. The paper introduces new evaluation metrics, such as Defense Capability Score (DCS), Type-level DCS (T-DCS), and Comprehensive DCS (C-DCS), to assess the effectiveness of defense methods. The authors also explore the trade-offs between model utility and defense effectiveness, providing concrete recommendations for practitioners. The paper's significance lies in its attempt to provide a unified platform for VFL research, addressing the need for a comprehensive benchmark that includes a wide range of methods and scenarios. However, the paper's primary focus is on implementation and empirical evaluation, with limited theoretical contributions. The authors acknowledge the limitations of the current implementation, particularly in terms of cryptographic techniques, and suggest future directions for improvement. Overall, the paper presents a valuable contribution to the VFL community by providing a practical framework and benchmark, but it also highlights areas where further research is needed.\n\n\n## Soundness:\n\n3.0\n\n\n## Presentation:\n\n3.0\n\n\n## Contribution:\n\n3.0\n\n\n## Strengths:\n\nI find several strengths in this paper, primarily centered around its contribution to the practical application and benchmarking of Vertical Federated Learning (VFL). The most significant strength is the development of VFLAIR, a lightweight and extensible VFL framework. This framework, as the authors describe, includes a wide array of VFL components, such as 29 local model architectures, 6 global models, and 5 communication protocols. This comprehensive collection of components, coupled with the support for 13 diverse datasets, makes VFLAIR a valuable resource for researchers and practitioners alike. The inclusion of 11 attack methods and 8 defense mechanisms further enhances the framework's utility as a benchmarking tool. The authors have clearly put considerable effort into implementing state-of-the-art VFL methods, which is a non-trivial undertaking. The empirical evaluation is another strong point. The authors conduct extensive experiments, benchmarking the performance of various VFL settings, including model partitioning, communication protocols, and defense strategies. The introduction of new evaluation metrics, such as DCS, T-DCS, and C-DCS, is a positive step towards a more nuanced understanding of defense effectiveness. These metrics allow for a more comprehensive assessment of the trade-offs between model utility and defense performance. The paper also provides concrete recommendations on the choice of defense strategies for different practical VFL deployment scenarios, which is particularly helpful for practitioners. The authors have clearly identified a need for a comprehensive VFL benchmark and have made a commendable effort to address this gap. The framework's extensibility is also a notable strength, as it allows for future additions and modifications, ensuring its continued relevance to the VFL community. The paper's focus on practical deployment scenarios is also a positive aspect, as it highlights the real-world applicability of the research. The authors have successfully created a valuable tool that can facilitate further research and development in the field of VFL.\n\n\n## Weaknesses:\n\nDespite the strengths of this paper, I have identified several weaknesses that warrant careful consideration. First, the paper's novelty is limited. While the implementation of a comprehensive VFL framework is a valuable contribution, the paper does not introduce any novel defense techniques. As I've verified, the authors explicitly state that they implement and benchmark existing methods, rather than proposing new ones. This focus on implementation and empirical evaluation, while useful, positions the paper more as a system paper than a theoretical contribution. The lack of detailed implementation specifics further reinforces this perception. Second, the paper lacks a significant theoretical contribution. The focus is primarily on empirical evaluation and benchmarking, with no in-depth theoretical analysis or proofs. While the introduction of new evaluation metrics is a contribution, it is still within the realm of empirical analysis. The paper does not delve into the theoretical underpinnings of the observed empirical results, such as the relationship between the properties of the defense mechanisms and their ability to mitigate specific types of attacks. Third, the paper does not provide a detailed analysis of the computational overhead of the proposed defense methods. While the authors include 'Execution Time' as a metric in some experiments, this is primarily for comparing the overall training time with and without encryption for tree-based models. There is no detailed breakdown of the computational overhead introduced by each individual defense method. This is a significant omission, as the computational cost of defense methods is a crucial factor to consider in practical deployments, especially in resource-constrained environments. Fourth, the paper does not explore the potential vulnerabilities of the proposed defense methods to more advanced attacks. The evaluation is limited to a set of 11 specific attacks, which are established attacks but not necessarily adaptive attacks designed to circumvent these specific defenses. The paper does not consider the robustness of these defenses against sophisticated adversaries who might develop new attack strategies to target the weaknesses of the evaluated defenses. Fifth, the evaluation of the framework is limited to a relatively small number of datasets and model architectures. While the paper includes 13 datasets and 29 local model architectures, a wider range of datasets and models would further demonstrate the framework's generalizability. The current selection, while diverse, could be expanded to include more complex data modalities and tasks. Finally, the paper lacks a detailed discussion of the limitations of the proposed framework and potential future research directions. While the 'Conclusions and Limitations' section briefly mentions some limitations, it does not provide a comprehensive analysis of the challenges of deploying VFL in real-world scenarios, such as the need for robust security mechanisms and the difficulty of handling heterogeneous data distributions. The paper also does not thoroughly outline potential future research directions, such as the development of more efficient and robust defense methods or the exploration of new VFL architectures. These limitations, taken together, suggest that while the paper makes a valuable contribution in terms of a practical framework, it falls short in terms of theoretical depth, computational analysis, and exploration of advanced attack scenarios. My confidence in these identified weaknesses is high, as they are directly supported by the paper's content and lack of specific details in the areas mentioned.\n\n\n## Suggestions:\n\nBased on the identified weaknesses, I recommend several concrete improvements to enhance the paper's contribution. First, the authors should conduct a more in-depth analysis of the computational overhead associated with the proposed VFL framework, particularly the defense methods. This analysis should include a breakdown of the time spent on different stages of the training process, such as forward and backward propagation, communication, and encryption/decryption. This analysis should be conducted for various model architectures and datasets to understand how these factors influence the overall training time. Furthermore, the impact of different defense methods on the computational cost should be quantified. For example, the authors could measure the increase in training time and communication costs when using techniques like gradient perturbation or homomorphic encryption. This detailed analysis would allow practitioners to make informed decisions about the trade-offs between security and efficiency when deploying the framework in real-world scenarios. It would also be beneficial to explore optimization techniques to mitigate the computational overhead of the defense methods. Second, the authors should expand the evaluation of the framework to include a wider range of datasets and model architectures. While the current evaluation includes 13 datasets and 29 local model architectures, it would be beneficial to include datasets with varying characteristics, such as different data modalities (e.g., images, text, time series) and different data sizes. This would help to assess the generalizability of the framework and identify potential limitations. Similarly, the evaluation should include a wider range of model architectures, such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers, to demonstrate the framework's versatility. Furthermore, the authors should consider evaluating the framework's performance on more complex tasks, such as multi-label classification or regression problems. This would provide a more comprehensive understanding of the framework's capabilities and limitations. Third, the authors should explore the potential vulnerabilities of the proposed defense methods to more advanced attacks. This could involve considering adaptive attacks that are specifically designed to circumvent the defenses. The authors should also consider exploring the use of adversarial training techniques to improve the robustness of the defenses against such advanced attacks. This could involve training the models on adversarial examples generated by more sophisticated attack methods, or using techniques such as robust optimization to minimize the impact of adversarial perturbations. Furthermore, the authors should investigate the potential for attacks that exploit the communication protocols used in VFL, such as man-in-the-middle attacks or replay attacks. Fourth, the authors should include a more detailed discussion of the limitations of the proposed framework and potential future research directions. The authors should acknowledge the challenges of deploying VFL in real-world scenarios, such as the need for robust security mechanisms and the difficulty of handling heterogeneous data distributions. They should also discuss the limitations of the current evaluation, such as the limited number of datasets and model architectures. Furthermore, the authors should outline potential future research directions, such as the development of more efficient and robust defense methods, the exploration of new VFL architectures, and the application of VFL to new domains. This discussion would provide valuable insights for researchers in the field and help to guide future research efforts. Finally, the authors should consider adding more implementation details to the paper, such as code snippets or architectural diagrams of the framework itself. This would make the paper more useful for researchers who want to use the framework and would also help to clarify the technical contributions of the work. These suggestions, if implemented, would significantly strengthen the paper and make it a more valuable contribution to the VFL community.\n\n\n## Questions:\n\nBased on my analysis, I have several questions that I believe are crucial for a deeper understanding of the paper's contributions and limitations. First, how do the proposed defense methods perform against more advanced attacks that are specifically designed to target these defenses? The current evaluation focuses on a set of existing attacks, but it does not consider the potential for adaptive adversaries to circumvent these defenses. Understanding the robustness of the defenses against such advanced attacks is crucial for assessing their practical utility. Second, what is the computational overhead of the proposed defense methods, and how does it impact the scalability of VFL systems? The paper does not provide a detailed analysis of the computational cost of each defense method, which is a critical factor to consider in practical deployments, especially in resource-constrained environments. A detailed breakdown of the time spent on different stages of the training process, such as forward and backward propagation, communication, and encryption/decryption, would be beneficial. Third, how do the proposed defense methods affect the convergence rate and stability of VFL training? The paper does not explore the impact of the defense methods on the training dynamics, which is an important aspect to consider. Understanding how these methods affect the convergence rate and stability of the training process is crucial for ensuring the reliability of the framework. Fourth, how do the proposed defense methods perform in real-world VFL deployment scenarios, and what are the practical challenges of implementing these defenses in such settings? The paper focuses on benchmarking in a controlled environment, but it does not address the practical challenges of deploying these defenses in real-world scenarios, such as handling heterogeneous data distributions and dealing with communication constraints. Fifth, what are the potential limitations of the proposed framework, and what are the future research directions that the authors envision? The paper briefly mentions some limitations, but a more detailed discussion of the challenges of deploying VFL in real-world scenarios and the potential future research directions would be valuable. Finally, how does the proposed framework compare to other existing VFL frameworks in terms of performance, scalability, and security? A comparative analysis with other existing frameworks would provide a better understanding of the strengths and weaknesses of VFLAIR. These questions, I believe, are essential for a comprehensive understanding of the paper's contributions and limitations and would help to guide future research in this area.\n\n\n## Rating:\n\n6.0\n\n\n## Confidence:\n\n3.5\n\n\n## Decision:\n\nAccept", "other": "\n## Summary:\n\nThis paper introduces VFLAIR, a modular and lightweight framework designed to facilitate research and development in Vertical Federated Learning (VFL). The authors aim to address the gap between industrial-grade VFL solutions and the needs of the research community by providing a comprehensive platform that supports a wide range of VFL algorithms, defense methods, and evaluation metrics. VFLAIR includes standardized pipelines for training and validation, accommodating diverse datasets, local model architectures, global models, model partition settings, communication protocols, and encryption methods. The framework supports both tree-based and neural network models, and it incorporates various communication protocols such as FedSGD, FedBCD, CELU-VFL, Quantize, and Top-k. A key contribution of this work is the introduction of new evaluation metrics, including the Defense Capability Score (DCS), which aims to provide a unified measure for comparing the effectiveness of different defense strategies against various attacks. The authors conducted extensive experiments to benchmark various aspects of VFL, including model performance, communication efficiency, and robustness to attacks. These experiments provide insights into system design choices and aim to guide future development and practical deployment of VFL systems. The paper presents results on several datasets, including MNIST, CIFAR10, and others, demonstrating the performance of different VFL models under various settings. The authors also evaluate the effectiveness of different defense methods against a range of attacks, providing a comprehensive analysis of the trade-offs between model accuracy, communication efficiency, and security. Overall, the paper seeks to provide a valuable resource for the VFL research community by offering a flexible and comprehensive framework for experimentation and benchmarking, along with a new evaluation metric to assess defense capabilities. However, the paper also has some limitations, including a lack of detailed comparisons with existing frameworks, a limited discussion of ethical implications, and a lack of analysis of computational complexity and scalability, which I will discuss in detail in the following sections.\n\n\n## Soundness:\n\n2.75\n\n\n## Presentation:\n\n2.75\n\n\n## Contribution:\n\n2.75\n\n\n## Strengths:\n\nI find several strengths in this paper that contribute to its overall value. First, the development of VFLAIR as a lightweight and modular framework is a significant contribution. The framework's modularity allows for easy integration and experimentation with various VFL algorithms, defense methods, and evaluation metrics. This flexibility is crucial for researchers who need a platform that can adapt to different research questions and experimental setups. The inclusion of standardized pipelines for training and validation further enhances the usability of the framework, making it accessible to a broader audience. Second, the paper's comprehensive approach to benchmarking is commendable. The authors evaluate various aspects of VFL, including model performance, communication efficiency, and robustness to attacks. This holistic approach provides a more complete picture of the strengths and weaknesses of different VFL configurations. The inclusion of diverse datasets, model architectures, and communication protocols makes the benchmark more representative of real-world scenarios. Third, the introduction of the Defense Capability Score (DCS) is a valuable contribution. This metric provides a unified way to compare the effectiveness of different defense strategies against various attacks, which is essential for developing robust VFL systems. The authors' efforts to provide a comprehensive evaluation of defense methods are particularly relevant given the growing concerns about security in federated learning. Finally, the paper is well-written and organized, with clear explanations of the framework, experimental setup, and results. The authors provide sufficient details to understand the methodology and reproduce the experiments, which is crucial for the scientific community. The inclusion of tables and figures to present the results makes the paper more accessible and easier to understand. Overall, the paper's strengths lie in its comprehensive approach to VFL research, its modular framework, and its introduction of a new evaluation metric, all of which contribute to its overall significance.\n\n\n## Weaknesses:\n\nDespite the strengths of this paper, I have identified several weaknesses that warrant careful consideration. First, the paper lacks a detailed comparison of VFLAIR with existing VFL frameworks and libraries. While the authors mention several frameworks such as FATE, Fedlearner, PaddleFL, Pysygt, FedTree, and FedML, they do not provide a thorough analysis of how VFLAIR compares to these existing solutions in terms of features, performance, and ease of use. The paper states that industrial-grade frameworks are not designed for research advances, but it does not provide a feature-by-feature comparison to demonstrate the specific advantages of VFLAIR. This omission makes it difficult for readers to understand the novelty and contributions of the proposed framework. For example, a table comparing the supported model architectures, communication protocols, and defense mechanisms, along with their specific implementations in each framework, would be highly valuable. This lack of comparative analysis is a significant weakness, as it limits the reader's ability to assess the practical advantages of VFLAIR. My confidence in this weakness is high, as the paper explicitly mentions these frameworks but fails to provide a detailed comparison. Second, the paper lacks a thorough discussion of the limitations of the proposed framework and potential future research directions. While the conclusion briefly mentions a limitation regarding cryptographic techniques, a broader discussion of the scenarios where VFLAIR might not be the most suitable solution is missing. The authors do not discuss the potential challenges in extending VFLAIR to support new models, datasets, or defense mechanisms. Furthermore, the paper does not outline specific research directions that could address these limitations. This omission is a weakness because it prevents the reader from fully understanding the scope and applicability of the framework. My confidence in this weakness is high, as the paper lacks a dedicated section on limitations and future work. Third, the paper does not provide a detailed analysis of the computational complexity and scalability of VFLAIR. While the paper provides some empirical data on execution time, it lacks a theoretical analysis of the computational cost of the framework, considering the number of participants, the size of the datasets, and the complexity of the models. The paper also does not provide empirical results demonstrating the scalability of VFLAIR, showing how the training time and communication overhead vary with the number of participants and the size of the datasets. This lack of analysis is a weakness because it limits the reader's ability to assess the practical applicability of VFLAIR in real-world scenarios. My confidence in this weakness is high, as the paper lacks a theoretical analysis of computational complexity and a systematic evaluation of scalability. Fourth, the paper does not discuss the potential ethical implications of VFL and the proposed framework. The paper focuses on the technical aspects of VFL, including attacks and defenses, but it does not address the ethical considerations related to data privacy, security, and fairness in the context of VFL. This omission is a weakness because it fails to acknowledge the potential risks of data leakage, bias in the models, and the potential for misuse of the framework. My confidence in this weakness is high, as the paper lacks any discussion on the ethical implications of VFL. Fifth, the paper only considers 11 attacks and 8 defenses, which may not be representative of all possible attacks and defenses in VFL. The selection criteria for these specific attacks and defenses are not clearly justified, raising concerns about the generalizability of the benchmark. It is unclear if the chosen attacks represent the most potent threats or if the defenses are the most effective against those specific attacks. This lack of justification is a weakness because it limits the reader's ability to assess the robustness of the framework. My confidence in this weakness is high, as the paper lists the attacks and defenses but does not provide a clear justification for their selection. Sixth, the paper does not provide a detailed analysis of the computational cost of the different attacks and defenses. While the paper provides overall execution time for VFL training, it does not provide a breakdown of the computational cost for each individual attack and defense method. This lack of analysis is a weakness because it makes it difficult to assess the practical applicability of each method. My confidence in this weakness is high, as the paper does not provide a detailed analysis of the computational cost of individual attacks and defenses. Seventh, the paper does not consider the impact of different data distributions on the performance of the attacks and defenses. The experiments are conducted on standard datasets without manipulating their distribution. This omission is a weakness because it does not reflect real-world scenarios where data distribution may not be uniform. My confidence in this weakness is high, as the paper does not include experiments or analysis on the impact of different data distributions. Eighth, the paper only provides an overview of the workflow in the method section, lacking a detailed introduction to the specific implementation of various components within the framework. While the authors mention modularity, the paper lacks a clear explanation of how each component is implemented and how they interact within the framework. For instance, the specific data preprocessing steps for each dataset, the exact model architectures used, and the detailed algorithmic steps for each defense method are not elaborated upon. This lack of detail makes it difficult to understand the practical implementation of the framework and to reproduce the results. My confidence in this weakness is high, as the method section provides an overview but lacks detailed implementation specifics for various components. Ninth, the results of the VFL benchmark are presented in the form of bar charts and tables, but the authors do not provide a detailed analysis and discussion of these results. The analysis primarily focuses on comparing the performance of different defense methods based on the proposed DCS metric, but it lacks a deeper investigation into why certain methods perform better than others under specific attack scenarios. For example, the paper does not explore the underlying reasons for the observed trade-offs between main task performance and attack performance for different defense methods. This lack of in-depth analysis is a weakness because it limits the reader's ability to fully understand the results. My confidence in this weakness is high, as the results section focuses on presenting data and high-level analysis based on DCS, lacking deeper mechanistic explanations. Finally, the authors introduce a new evaluation metric, DCS, but do not compare it with other existing evaluation metrics. While the authors argue that there are no directly applicable evaluation metrics for the specific problem of evaluating defense methods against various attacks in VFL, they do not provide a thorough justification for why a comparison with related metrics from broader fields like privacy or security is not relevant. Furthermore, the paper does not include a detailed analysis of the limitations of the DCS metric, such as its sensitivity to the choice of weighting parameters or its potential biases towards certain types of attacks or defenses. This lack of comparison and analysis is a weakness because it limits the reader's ability to assess the validity and usefulness of the DCS metric. My confidence in this weakness is high, as the paper introduces DCS without comparing it to other metrics or analyzing its limitations in detail.\n\n\n## Suggestions:\n\nTo address the identified weaknesses, I recommend several concrete improvements. First, the authors should provide a more detailed comparison of VFLAIR with existing VFL frameworks. This comparison should not only focus on the features supported but also on the implementation details, performance characteristics, and ease of use. A table comparing the supported model architectures, communication protocols, and defense mechanisms, along with their specific implementations in each framework, would be highly valuable. Furthermore, the comparison should include quantitative metrics such as training time, communication overhead, and memory usage for different scenarios. This would allow readers to assess the practical advantages and disadvantages of VFLAIR compared to existing solutions and understand the specific niche it aims to fill. The authors should also discuss the trade-offs made in VFLAIR's design, such as the choice of specific algorithms or the level of abstraction, and how these choices impact its performance and applicability. Second, the authors should include a thorough discussion of the limitations of VFLAIR and potential future research directions. The authors should acknowledge the specific scenarios where VFLAIR might not be the most suitable solution, such as very large-scale federated learning settings or specific types of data distributions. They should also discuss the potential challenges in extending VFLAIR to support new models, datasets, or defense mechanisms. Furthermore, the authors should outline specific research directions that could address these limitations, such as exploring more efficient communication protocols, developing more robust defense mechanisms, or improving the scalability of the framework. This discussion should also include potential ethical implications of VFL and the proposed framework. The authors should address the ethical considerations related to data privacy, security, and fairness in the context of VFL. This discussion should include potential risks of data leakage, bias in the models, and the potential for misuse of the framework. The authors should also discuss how VFLAIR can be used to mitigate these risks and promote responsible use of VFL. Third, the authors should provide a detailed analysis of the computational complexity and scalability of VFLAIR. This analysis should include a theoretical analysis of the computational cost of the framework, considering the number of participants, the size of the datasets, and the complexity of the models. They should also provide empirical results demonstrating the scalability of VFLAIR, showing how the training time and communication overhead vary with the number of participants and the size of the datasets. This analysis should include a discussion of the bottlenecks in the framework and potential strategies for improving its scalability. For example, the authors could explore techniques such as model compression, distributed training, or asynchronous communication to reduce the computational cost and communication overhead. Fourth, the authors should provide a clear rationale for the selection of the 11 attacks and 8 defenses. This should include a discussion of the threat model each attack represents and the specific vulnerabilities each defense aims to mitigate. A more systematic approach, perhaps based on a taxonomy of attacks and defenses, would strengthen the benchmark's validity. For example, the authors could categorize attacks based on their target (e.g., model parameters, data privacy, backdoor injection) and defenses based on their mechanism (e.g., gradient perturbation, differential privacy, adversarial training). This would provide a more structured and justifiable selection process. Furthermore, the authors should consider including more recent and sophisticated attacks and defenses to ensure the benchmark remains relevant and challenging. Fifth, the authors should include a detailed evaluation of the time and resource requirements for each attack and defense method. This should include metrics such as training time, inference time, memory usage, and communication overhead. The analysis should also consider the scalability of each method with respect to the number of participants and the size of the dataset. This information is crucial for practitioners to assess the practical feasibility of each method. For example, the authors could provide a table summarizing the computational cost of each method under different settings. This would allow users to make informed decisions about which methods are suitable for their specific resource constraints. Additionally, the authors should discuss the trade-offs between computational cost and performance for each method. Sixth, the authors should include experiments with different data distributions. This should include scenarios with class imbalance, non-IID data, and varying degrees of data heterogeneity. The authors should analyze how these different data distributions affect the performance of the attacks and defenses. This analysis should include a discussion of the specific challenges posed by each data distribution and how the different methods perform under these challenges. For example, the authors could include experiments with synthetic datasets that simulate different data distributions. This would provide a more comprehensive and realistic evaluation of the performance of the attacks and defenses. The authors should also discuss the limitations of the benchmark in terms of the data distributions considered and suggest future research directions to address these limitations. Seventh, the authors should provide a more detailed explanation of the implementation of various components within the VFL framework. This should include a description of the data preprocessing steps for each dataset, the specific model architectures used (including the number of layers, activation functions, and hyperparameters), and the detailed algorithmic steps for each defense method. For example, when discussing the implementation of a specific defense method, the authors should provide the exact mathematical formulation and the step-by-step procedure for integrating it into the VFL training process. This would greatly enhance the reproducibility of the results and allow other researchers to build upon the proposed framework. Furthermore, the authors should include a detailed discussion of the design choices made during the implementation, such as the rationale behind selecting specific model architectures or defense methods for each dataset. This would provide valuable insights into the practical considerations of applying VFL in different scenarios. Eighth, the authors should conduct a more in-depth analysis of the benchmark results. This should include a detailed discussion of the observed trade-offs between main task performance and attack performance for different defense methods. For example, the authors could investigate why certain defense methods are more effective against specific types of attacks and why some methods lead to a significant drop in main task performance. This analysis should also explore the impact of different hyperparameters on the performance of defense methods and the limitations of the proposed DCS metric. The authors should also consider including a sensitivity analysis of the DCS metric to the choice of weighting parameters and discuss its potential biases towards certain types of attacks or defenses. Finally, the authors should provide a more thorough justification for the introduction of the DCS metric and its comparison with existing evaluation metrics from related fields. While it is understandable that there may not be directly applicable evaluation metrics for the specific problem of evaluating defense methods against various attacks in VFL, the authors should explore the possibility of adapting existing metrics from broader fields like privacy or security. For example, metrics such as membership inference attack success rate or differential privacy guarantees could be considered as baselines for comparison. The authors should also discuss the limitations of the DCS metric in detail, including its sensitivity to the choice of weighting parameters and its potential biases towards certain types of attacks or defenses.\n\n\n## Questions:\n\nBased on my analysis, I have several questions that I believe are crucial for a deeper understanding of the paper. First, how does VFLAIR compare to existing VFL frameworks and libraries in terms of features, performance, and ease of use? A detailed comparison would help to clarify the specific advantages and disadvantages of VFLAIR. Second, what are the limitations of VFLAIR, and what are the potential future research directions? A thorough discussion of the limitations would help to understand the scope and applicability of the framework. Third, what is the computational complexity and scalability of VFLAIR, especially when dealing with large datasets and complex models? A detailed analysis of the computational cost and scalability would help to assess the practical applicability of the framework. Fourth, what are the potential ethical implications of VFL and the proposed framework, and how can they be addressed? A discussion of the ethical considerations would help to promote responsible use of VFL. Fifth, what is the rationale for selecting the specific 11 attacks and 8 defenses included in the benchmark? A clear justification for the selection would help to assess the generalizability of the benchmark. Sixth, what is the computational cost of each individual attack and defense method? A detailed analysis of the computational cost would help to assess the practical feasibility of each method. Seventh, how does the performance of the attacks and defenses vary with different data distributions? An analysis of the impact of data distribution would help to assess the robustness of the framework. Eighth, what are the specific implementation details of the various components within the VFL framework, including data preprocessing steps, model architectures, and defense methods? A detailed explanation of the implementation would help to reproduce the results. Ninth, what are the underlying reasons for the observed trade-offs between main task performance and attack performance for different defense methods? A deeper analysis of the results would help to understand the effectiveness of different defense methods. Finally, how does the DCS metric compare to other existing evaluation metrics from related fields, and what are its limitations? A thorough justification for the introduction of the DCS metric and its comparison with existing metrics would help to assess its validity and usefulness.\n\n\n## Rating:\n\n5.0\n\n\n## Confidence:\n\n4.0\n\n\n## Decision:\n\nReject", "other_type": "DeepReviewer_Standard", "v.s.": "B", "result": "```\n**Technical Accuracy - Evidence Analysis:**\n- From Assistant A:\n  - Claims that the paper lacks detailed comparisons with existing frameworks.\n  - Evidence: \"While the authors mention several frameworks such as FATE, Fedlearner, PaddleFL, Pysygt, FedTree, and FedML, they do not provide a thorough analysis of how VFLAIR compares to these existing solutions... This omission makes it difficult for readers to understand the novelty and contributions of the proposed framework.\"\n  - Analysis: Assistant A accurately points out the paper's limited comparative analysis. The paper mentions related works in the introduction and related work section, listing existing frameworks, but does not include a table or detailed paragraph comparing VFLAIR's features and performance against them.\n\n- From Assistant B:\n  - Claims that the paper's novelty is limited because it doesn't introduce novel defense techniques.\n  - Evidence: \"While the implementation of a comprehensive VFL framework is a valuable contribution, the paper does not introduce any novel defense techniques. As I've verified, the authors explicitly state that they implement and benchmark existing methods, rather than proposing new ones.\"\n  - Analysis: Assistant B correctly identifies that the paper focuses on benchmarking existing methods rather than proposing new defense mechanisms. The paper's abstract and introduction emphasize the framework and benchmark aspects, and section 4 \"Overview of VFLAIR\" describes \"multiple existing efficiency enhancement techniques, data leakage and model utility impairing attacks as well as defending methods that aim to mitigate potential threats.\" This confirms that the paper's contribution is primarily in the framework and benchmarking of existing methods.\n\n- Comparative Assessment:\n  - Both assistants are technically accurate in their claims. Assistant A focuses on the lack of comparative framework analysis, while Assistant B highlights the lack of novel defense methods. Both are valid observations based on the paper's content and focus.\n\n**Technical Accuracy - Judgment:**\n**Evidence-Based Reason:** Both assistants accurately identify limitations of the paper based on its content. Assistant A points to the lack of comparative analysis with existing frameworks, and Assistant B notes the absence of novel defense mechanisms, both of which are verifiable from the paper.\n**Better Assistant:** Tie\n- Both reviews demonstrate strong technical accuracy by correctly interpreting the paper's focus and limitations.\n\n---\n**Constructive Value - Evidence Analysis:**\n- From Assistant A:\n  - Suggests providing a table comparing VFLAIR with existing frameworks.\n  - Evidence: \"For example, a table comparing the supported model architectures, communication protocols, and defense mechanisms, along with their specific implementations in each framework, would be highly valuable.\"\n  - Analysis: This is a highly actionable and specific suggestion. Adding a comparative table would directly address the weakness identified by Assistant A and significantly improve the paper's clarity regarding VFLAIR's novelty and advantages.\n\n  - Suggests discussing limitations and future directions, including ethical implications.\n  - Evidence: \"Second, the paper lacks a thorough discussion of the limitations of the proposed framework and potential future research directions... Furthermore, the paper does not outline specific research directions that could address these limitations. This omission is a weakness... Fourth, the paper does not discuss the potential ethical implications of VFL and the proposed framework.\"\n  - Analysis: These are also constructive suggestions. Discussing limitations and future work is standard practice and adds value. Addressing ethical implications is increasingly important, especially in privacy-related fields like federated learning.\n\n- From Assistant B:\n  - Suggests in-depth analysis of computational overhead.\n  - Evidence: \"First, the authors should conduct a more in-depth analysis of the computational overhead associated with the proposed VFL framework, particularly the defense methods. This analysis should include a breakdown of the time spent on different stages of the training process...\"\n  - Analysis: This is a very actionable and specific suggestion. Analyzing computational overhead is crucial for a framework paper, especially one aiming for practical deployment.\n\n  - Suggests expanding evaluation to more datasets and models.\n  - Evidence: \"Second, the authors should expand the evaluation of the framework to include a wider range of datasets and model architectures... This would help to assess the generalizability of the framework and identify potential limitations.\"\n  - Analysis: This is a standard and valuable suggestion for benchmarking papers. Broader evaluation strengthens the paper's conclusions.\n\n  - Suggests exploring vulnerabilities to advanced attacks.\n  - Evidence: \"Third, the authors should explore the potential vulnerabilities of the proposed defense methods to more advanced attacks. This could involve considering adaptive attacks that are specifically designed to circumvent the defenses.\"\n  - Analysis: This is a forward-looking and constructive suggestion, pushing the research towards more robust security evaluations.\n\n- Comparative Assessment:\n  - Both assistants provide highly constructive and actionable feedback. Assistant A's suggestions are broader, covering comparative analysis, limitations, future directions, and ethical considerations. Assistant B's suggestions are more focused on empirical aspects: computational overhead, expanded evaluation, and advanced attack analysis. Both sets of suggestions are valuable and would significantly improve the paper.\n\n**Constructive Value - Judgment:**\n**Evidence-Based Reason:** Both assistants offer valuable and actionable suggestions. Assistant A's suggestions are broader, covering framework comparison, limitations, ethical aspects, and future work. Assistant B's suggestions are more focused on empirical enhancements, such as computational analysis, expanded benchmarks, and advanced security evaluations. Both sets of suggestions are highly relevant and would improve the paper's impact.\n**Better Assistant:** Tie\n- Both reviews are equally strong in providing constructive and actionable feedback, albeit with slightly different focuses.\n\n---\n**Analytical Depth - Evidence Analysis:**\n- From Assistant A:\n  - Analyzes the lack of comparison with existing frameworks in detail, explaining why it's a weakness.\n  - Evidence: \"While the authors mention several frameworks... they do not provide a thorough analysis of how VFLAIR compares... This omission makes it difficult for readers to understand the novelty and contributions...\"\n  - Analysis: Assistant A delves into the *why* behind the weakness, explaining the impact on understanding novelty and contribution.\n\n  - Analyzes the lack of discussion on limitations, ethical implications, computational complexity, and justification for attack/defense selection, providing reasons for each being a weakness.\n  - Evidence: For each weakness, Assistant A explains *why* it is a weakness, relating it to reader understanding, practical applicability, ethical considerations, and benchmark validity.\n  - Analysis: Assistant A consistently provides reasoning and justification for each identified weakness, demonstrating analytical depth.\n\n- From Assistant B:\n  - Analyzes the limited novelty, lack of theoretical contribution, missing computational overhead analysis, unexplored advanced attacks, limited evaluation scope, and insufficient discussion of limitations.\n  - Evidence: For each weakness, Assistant B explains *why* it is a weakness, relating it to theoretical depth, practical deployment considerations, security robustness, generalizability, and completeness of discussion.\n  - Analysis: Assistant B also provides reasoning for each identified weakness, demonstrating analytical depth, particularly in the context of practical deployment and theoretical grounding.\n\n- Comparative Assessment:\n  - Both assistants demonstrate strong analytical depth by not just listing weaknesses but also explaining *why* they are weaknesses and *what* impact they have on the paper's value and understanding. Both reviews thoroughly examine key aspects of the paper.\n\n**Analytical Depth - Judgment:**\n**Evidence-Based Reason:** Both assistants exhibit strong analytical depth. They go beyond simply listing strengths and weaknesses and delve into the reasons and implications of their assessments. Both reviews thoroughly examine key aspects like novelty, theoretical contribution, practical considerations, and evaluation scope, providing reasoned justifications for their critiques.\n**Better Assistant:** Tie\n- Both reviews are equally strong in their analytical depth, providing well-reasoned and justified critiques.\n\n---\n**Communication Clarity - Evidence Analysis:**\n- From Assistant A:\n  - Uses clear and concise language.\n  - Organizes feedback into strengths, weaknesses, and suggestions with clear headings.\n  - Provides specific examples and quotes from the review itself to illustrate points.\n  - Reasoning is well-explained in each point.\n  - Suggestions are concrete and actionable.\n\n- From Assistant B:\n  - Uses clear and concise language.\n  - Organizes feedback into strengths, weaknesses, suggestions, and questions with clear headings.\n  - Provides specific examples and refers to the paper's content to support claims.\n  - Reasoning is well-explained for each point.\n  - Suggestions are concrete and actionable.\n\n- Comparative Assessment:\n  - Both assistants communicate their feedback with excellent clarity. Both reviews are well-organized, use clear language, provide specific examples, and explain their reasoning effectively.\n\n**Communication Clarity - Judgment:**\n**Evidence-Based Reason:** Both assistants demonstrate excellent communication clarity. Their reviews are well-organized, use clear and concise language, provide specific examples, and effectively explain their reasoning. Both reviews are easy to understand and follow.\n**Better Assistant:** Tie\n- Both reviews are equally strong in terms of communication clarity.\n\n---\n**Comprehensive Analysis:**\nBoth Assistant A and Assistant B provide strong and insightful reviews of the paper \"VFLAIR: A Research Library and Benchmark for Vertical Federated Learning\". They both accurately summarize the paper's contributions and identify key strengths, particularly the development of the VFLAIR framework and the comprehensive benchmarking effort.\n\nIn terms of weaknesses, both assistants identify important areas for improvement. Assistant A focuses on the lack of comparative analysis with existing frameworks, insufficient discussion of limitations and ethical implications, and the need for more detailed implementation specifics and result analysis. Assistant B emphasizes the limited novelty (no new defense methods), lack of theoretical contribution, missing computational overhead analysis, unexplored advanced attacks, and limited evaluation scope.\n\nWhile there is some overlap in their critiques (both mention limitations and scope of evaluation), they also offer distinct perspectives. Assistant A is more concerned with contextualizing VFLAIR within the existing landscape and ensuring ethical considerations are addressed. Assistant B is more focused on the depth of the research, particularly in terms of theoretical grounding, computational analysis, and robustness against advanced threats.\n\nBoth reviews are highly constructive, offering actionable suggestions for improvement. Assistant A's suggestions are broader, encompassing framework comparison, ethical considerations, and deeper result analysis. Assistant B's suggestions are more technically focused, emphasizing computational overhead analysis, expanded evaluation, and exploration of advanced attacks.\n\nIn terms of overall quality, both reviews are excellent. They are technically accurate, highly constructive, analytically deep, and clearly communicated. The slight differences in their focus and emphasis do not make one review definitively better than the other. They complement each other, offering a comprehensive and balanced critique of the paper.\n\n**Overall Judgment:**\n**Evidence-Based Reason:** Both Assistant A and Assistant B provide reviews of comparable high quality. They are both technically accurate, offer valuable and actionable constructive feedback, demonstrate strong analytical depth in their critiques, and communicate their points with excellent clarity. While they emphasize slightly different aspects of the paper's weaknesses and suggestions, both reviews are equally insightful and would be highly beneficial to the authors for improving their paper.\n**Better Assistant:** Tie\n- Both reviews are comparable in overall quality, providing equally valuable and insightful feedback. They offer complementary perspectives that together provide a comprehensive evaluation of the paper.\n```"}
