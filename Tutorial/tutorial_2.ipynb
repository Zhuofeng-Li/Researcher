{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Turorial 2: Understanding CycleReviewer ",
   "id": "ae9cb0ef4f9eaf58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The CycleReviewer (WhizReviewer) is a set of generative large language models that have undergone additional supervised training, with sizes of 8B, 70B, and 123B respectively. All models are pure text language models, with the 8B and 70B derived from the Llama3.1 pre-trained language model, and the 123B from the Mistral-Large-2 model. They all use the Transformer architecture.\n",
    "\n",
    "\n"
   ],
   "id": "b6de9b19931fd2f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_name = \"WestlakeNLP/WhizReviewer-ML-Pro-123B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = LLM(\n",
    "        model=model_name,\n",
    "        tensor_parallel_size=8,\n",
    "        max_model_len=18000,\n",
    "        gpu_memory_utilization=0.95,\n",
    "    )\n",
    "\n",
    "system_prompt = \\\n",
    "\"\"\"You are an expert academic reviewer tasked with providing a thorough and balanced evaluation of research papers. For each paper submitted, conduct a comprehensive review addressing the following aspects:\n",
    "\n",
    "1. Summary: Briefly outline main points and objectives.\n",
    "2. Soundness: Assess methodology and logical consistency.\n",
    "3. Presentation: Evaluate clarity, organization, and visual aids.\n",
    "4. Contribution: Analyze significance and novelty in the field.\n",
    "5. Strengths: Identify the paper's strongest aspects.\n",
    "6. Weaknesses: Point out areas for improvement.\n",
    "7. Questions: Pose questions for the authors.\n",
    "8. Rating: Score 1-10, justify your rating.\n",
    "9. Meta Review: Provide overall assessment and recommendation (Accept/Reject).\n",
    "\n",
    "Maintain objectivity and provide specific examples from the paper to support your evaluation.\n",
    "\n",
    "You need to fill out **4** review opinions.\"\"\"\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.4, top_p=0.95, max_tokens=4096)\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5c5ed5b624f76079"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8f07149c1d2b1cf8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T10:37:54.494441Z",
     "start_time": "2024-10-23T10:37:54.457128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "def get_paper_context(item):\n",
    "    context = \"\"\n",
    "    context += r'\\title{'+item['title']+'}\\n'\n",
    "    context += r'\\begin{abstract}'+'\\n'+item['abstract']+'\\n\\end{abstract}\\n'\n",
    "    \n",
    "    for section in item['sections']:\n",
    "        context += r'\\section{'+section[0]+'}\\n'\n",
    "        context += section[1]\n",
    "    return context\n",
    "\n",
    "\n",
    "with open('./demo_data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(json.dumps(get_paper(data[0])))\n"
   ],
   "id": "1920bcc733283b77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\\\title{CofiPara: A Coarse-to-fine Paradigm for Multimodal Sarcasm Target Identification with Large Multimodal Models}\\n\\\\begin{abstract}\\nSocial media abounds with multimodal sarcasm, and identifying sarcasm targets is particularly challenging due to the implicit incongruity not directly evident in the text and image modalities. Current methods for Multimodal Sarcasm Target Identification (MSTI) predominantly focus on superficial indicators in an end-to-end manner, overlooking the nuanced understanding of multimodal sarcasm conveyed through both the text and image. This paper proposes a versatile MSTI framework with a coarse-to-fine paradigm, by augmenting sarcasm explainability with reasoning and pre-training knowledge. Inspired by the powerful capacity of Large Multimodal Models (LMMs) on multimodal reasoning, we first engage LMMs to generate competing rationales for coarser-grained pre-training of a small language model on multimodal sarcasm detection. We then propose fine-tuning the model for finer-grained sarcasm target identification. Our framework is thus empowered to adeptly unveil the intricate targets within multimodal sarcasm and mitigate the negative impact posed by potential noise inherently in LMMs. Experimental results demonstrate that our model far outperforms state-of-the-art MSTI methods, and markedly exhibits explainability in deciphering sarcasm as well.\\n\\n\\\\end{abstract}\\n\\\\section{Introduction}\\n\\nSarcasm, a prevalent form of figurative language, is often used in daily communication to convey irony, typically implying the opposite of its literal meaning~\\\\citep{joshi2017automatic}. As an important component in deciphering sarcasm, automated Sarcasm Target Identification (STI) is crucial for Natural Language Processing (NLP) in customer service~\\\\citep{davidov2010semi}, opinion mining~\\\\citep{riloff2013sarcasm}, and online harassment detection~\\\\citep{yin2009detection}. Although prior research on STI has primarily centered on textual content~\\\\citep{joshi2018sarcasm, parameswaran2019detecting}, the surge in multimodal user-generated content has propelled the field of multimodal sarcasm target identification to the forefront of research~\\\\citep{wang2022multimodal}, making it a significant area of study in both NLP applications and multimedia computing. \\n\\nThe MSTI task is to extract the entities being ridiculed (i.e., sarcasm targets) from both the text and image in multimodal sarcastic content. Previous work~\\\\citep{devlin2019bert, bochkovskiy2020yolov4} attempted to straightforwardly integrate a BERT-based textual encoder and a CNN-based visual encoder for just modeling the sarcasm text and image, respectively. The state-of-the-art approach~\\\\citep{wang2022multimodal} treats MSTI merely as an end-to-end task, primarily focusing on the superficial signals evident in the surface-level text and image. However, a more thorough investigation and understanding of the underlying meanings are essential, particularly in cases where the correlation between image and text is not immediately apparent in multimodal sarcasm~\\\\citep{tian2023dynamic}.\\n{\\n\\\\begin{figure}[t]\\n\\\\subfigure[]{\\n\\\\begin{minipage}[t]{0.5\\\\linewidth}\\n\\\\centering\\n\\\\scalebox{0.75}{\\\\includegraphics[width=5cm]{intro_1.pdf}}\\n\\\\label{fig:sarcasm_1}\\n\\\\end{minipage}%\\n}%\\n\\\\subfigure[]{\\n\\\\begin{minipage}[t]{0.5\\\\linewidth}\\n\\\\centering\\n\\\\scalebox{0.75}{\\\\includegraphics[width=5cm]{intro_2.pdf}}\\n\\\\label{fig:sarcasm_2}\\n\\\\end{minipage}%\\n}%\\n\\\\centering\\n\\\\caption{Examples of multimodal sarcasm on \\\\textbf{Twitter}: (\\\\textbf{a}) ``\\\\textit{never seen a \\\\#\\\\textcolor{red}{dlr train driver} before. looks like a tough job \\\\#london}''; (\\\\textbf{b}) ``\\\\textit{thank god for no product placement in \\\\#ukraine \\\\#eurovision}''. Boxes in \\\\textcolor{green(ncs)}{green} and words in \\\\textcolor{red}{red} denote the visual and textual targets.}\\n\\\\label{fig:motivation}\\n\\\\end{figure}}\\n\\nComprehending and analyzing multimodal sarcasm poses a considerable challenge, because its implicit meaning demands an in-depth understanding and reasoning of commonsense knowledge. For example, as shown in Figure~\\\\ref{fig:sarcasm_1}, a human checker needs reasonable thoughts between visual and textual sarcasm targets, to understand that the man's leisurely sitting posture in front of the control panel creates a sarcastic contrast between the idea of the train driver's job being difficult and the actual scene. Moreover, as the example shows in Figure~\\\\ref{fig:sarcasm_2}, the sarcasm target sometimes does not appear explicitly in the text, which makes it more challenging for conventional models to cognize that the example implies that the presence of the Pepsi bottle is a form of product placement, which is often seen as a marketing tactic. We contend the challenge lies in delivering rich multimodal knowledge that consistently assists in deciphering the concealed semantics within the multimodal nature of sarcasm.\\n \\nIn this paper, we adhere to the following two key principles in the knowledge-augmented design of our approach: 1) LMM Reasoning: To grasp the implicit meanings intrinsic in sarcasm, we resort to the extensive prior knowledge embedded within Large Multimodal Models (LMMs)~\\\\citep{liu2023visual, bai2023qwen}. This design philosophy enables complex reasoning, thereby enhancing both the MSTI accuracy and explainability; 2) MSD Pre-training: Previous literature~\\\\citep{joshi2018sarcasm} indicates that MSTI inherently appraises the presence of sarcasm targeting each entity within sarcastic content. Similar to Multimodal Sarcasm Detection (MSD)~\\\\citep{qin2023mmsd2}, which involves determining the sarcasm in multi-modalities at a holistic content level, MSTI actually engages in finer-grained sarcasm detection at the localized entity level. Considering such a close correlation between MSD and MSTI, it is assumed that insights from the coarser-grained MSD are instrumental in discerning sarcasm targets in the finer-grained MSTI. Thus we devise a cohesive framework to operate on the coarse-to-fine training paradigm, aimed at pinpointing nuanced visual and textual targets of multimodal sarcasm for MSTI by benefitting from LMM reasoning and MSD pre-training.\\n\\nTo these ends, we propose a novel framework with a \\\\textbf{\\\\underline{Co}}arse-to-\\\\textbf{\\\\underline{fi}}ne \\\\textbf{\\\\underline{Para}}digm, \\\\textbf{CofiPara}, by leveraging the divergent knowledge extracted from LMMs for multimodal sarcasm target identification. Specifically, we integrate text and image modalities within the coarse-to-fine training paradigm, which consists of two phases: 1) Coarser-grained Sarcasm Detection: Initially, we engage LMMs in critical thinking to generate rationales from both sarcastic and non-sarcastic perspectives. Utilizing these generated sarcasm rationales, we pre-train a smaller model to act as a rationale referee to implicitly extract sarcasm-indicative signals in the competing rationales for sarcasm prediction. This process aligns multimodal features between the sarcasm content and its underlying rationales, alleviating the negative impact of inevitable noise from LMMs through competing rationales; 2) Finer-grained Target Identification: Subsequently, we further fine-tune the smaller model pre-trained in the previous stage for multimodal sarcasm target identification. This phase enhances our model with the multimodal reasoning knowledge, acquired in the pre-training stage and the rationale in sarcastic perspective, to reveal the meanings concealed within the comprehensive multimodal information of sarcasm samples. In this manner, our CofiPara framework could be naturally output as the explanatory basis for deciphering multimodal sarcasm. Extensive experiments conducted on two public sarcasm datasets reveal that our approach far outperforms previous state-of-the-art MSTI methods, and achieves competitive results compared with MSD baselines. The experimental analysis further underscores the enhanced ability to provide superior explainability in the realm of multimodal sarcasm. Our contributions are summarized as follows in three folds:\\n\\\\begin{itemize}\\n    \\\\item To the best of our knowledge, we are the first to study multimodal sarcasm from a fresh perspective on explainability in both multimodal targets and natural texts, by exploiting advanced large multimodal models.\\n}\\n    \\\\item We propose a universal MSTI framework with the novel coarse-to-fine paradigm that incorporates the multimodal sarcasm target identification and the textual explanation for deciphering the multimodal sarcasm, which enhances sarcasm explainability in conjunction with effective multimodal sarcasm detection.\\n    \\\\item Extensive experiments confirm that our framework could yield superior performance on multimodal sarcasm target identification, and further provide informative explanations for a better understanding of multimodal sarcasm.\\n\\\\end{itemize}\\n\\n\\n\\\\section{Related Work}\\n\\\\textbf{MSD.}\\nSarcasm detection involves discerning sentiment incongruity within a context, traditionally emphasizing text modality~\\\\citep{xiong2019sarcasm, babanejad2020affective}. Multimodal Sarcasm Detection (MSD), enhanced by image integration, has garnered growing research interest~\\\\citep{schifanella2016detecting}. \\\\citet{cai2019multi} introduced a comprehensive MSD dataset, incorporating text, image, and image attributes, alongside a hierarchical fusion model. Subsequently, a range of studies has utilized attention mechanisms to subtly blend features from different modalities~\\\\citep{xu2020reasoning, pan2020modeling, tian2023dynamic}. Another line of recent advancements has seen the introduction of graph-based methods for sarcasm detection~\\\\citep{liang2021multi, liang2022multi, liu2022towards}, which excel in identifying key indicators across modalities. \\\\citet{qin2023mmsd2} revealed spurious cues in the previous MSD dataset~\\\\citep{cai2019multi} and provided an alternative refined dataset version. Existing solutions, however, only focused on performing multimodal sarcasm classification (i.e., predicting if a sample is sarcastic) with limited explanations for its prediction. In this paper, we delve into the explainability of multimodal sarcasm from both multimodal targets and textual rationales, aiming to decipher multimodal sarcasm using more intuitive forms and assisting users in gaining a better understanding.\\n\\n\\\\textbf{MSTI.}\\nRecent advancements in sarcasm analysis have seen a significant focus on Sarcasm Target Identification (STI), with notable contributions from researchers. STI aims to pinpoint the subject of mockery in sarcastic texts. \\\\citet{joshi2018sarcasm} introduced the concept of STI and discussed its application in the 2019 ALTA shared task~\\\\citep{molla2019overview}, highlighting evaluation metrics like Exact Match accuracy and F1 score. \\\\citet{patro2019deep} later developed a deep learning model enhanced with socio-linguistic features for target identification, while \\\\citet{parameswaran2019detecting} utilized a combination of classifiers, \\nfollowed by a rule-based method for extracting textual sarcasm targets. Moreover, \\\\citet{wang2022multimodal} pioneered STI in multimodal contexts by integrating sequence labeling with object detection in an end-to-end manner, but only capturing the superficial signals of different modalities in sarcasm. In this work, we regard the MSD task as the predecessor pre-training phase of MSTI, to better derive the prior reasoning knowledge absorbed in the coarser-grained auxiliary task MSD to the finer-grained goal task MSTI. \\n\\n\\\\textbf{LLMs and LMMs.}\\nRecently, Large Language Models (LLMs) have demonstrated exceptional versatility across various tasks. Significant advancements by leading tech companies have resulted in highly proficient, though often proprietary, LLMs~\\\\citep{brown2020language, OpenAI2023GPT4TR, chowdhery2022palm, team2023gemini}. Meanwhile, the NLP community has seen the rise of open-source LLMs, with publicly shared model weights~\\\\citep{black2022gpt, zeng2022glm, touvron2023llama, touvron2023llama2, wizardlm, wizardcoder}. More recently, LLMs have also been developed to adapt in processing both textual and visual data, marking a significant advancement. Recent research has focused on constructing versatile multimodal datasets~\\\\citep{yang2023dawn} from platforms like GPT-4 and GPT-4V~\\\\citep{OpenAI2023GPT4TR}, fine-tuning open-source LMMs such as LLaVA~\\\\citep{liu2023visual}, Qwen-VL~\\\\citep{bai2023qwen}, and other innovative projects~\\\\citep{Dai2023InstructBLIPTG, wang2023cogvlm}. These LMMs have shown excellent emergent abilities in multimodal tasks. In this work, we foster divergent thinking in LMMs by employing potential sarcasm labels as prompts, which promotes a coarse-to-fine strategy for fine-tuning smaller Language Models (LMs). Combined with MSTI, this design philosophy enhances the sarcasm understanding within the universal framework, steering it towards greater sarcasm explainability.\\n\\n\\\\section{Our Approach}\\n\\\\begin{figure*}\\n    \\\\centering\\n    \\\\includegraphics[width=\\\\linewidth]{model.pdf}\\n    \\\\caption{An overview of our framework, CofiPara, for multimodal sarcasm target identification.} \\n    \\\\label{fig:model}\\n\\\\end{figure*}\\n\\n\\\\textbf{Problem Statement.}\\nWe define a multimodal sample as $M=\\\\{I, T\\\\}$, which consists of an image $I$ and a text $T$. In the context of the coarser-grained MSD task, the label $y$ of the sample falls into either of the categories: {\\\\texttt{sarcastic}} or {\\\\texttt{non-sarcastic}}.  As for the finer-grained MSTI task, the label $y$ is a tuple consisting of a textual sarcasm target $y_{text}$, and a visual bounding box $y_{img}$, where the model is tasked with pinpointing the sarcasm entity targeted within the provided text and image modalities of the \\\\textit{sarcastic} sample. In this paper, we focus on improving the finer-grained MSTI task by leveraging insights from the coarser-grained MSD task.\\n\\nClosed to the MSD task, which establishes the presence of sarcasm in holistic semantics at the coarser level, the MSTI task inherently detects sarcasm targeting each entity of the multimodal sarcastic content to explicitly identify the specific sarcasm targets at the finer level~\\\\citep{joshi2018sarcasm}. This work is designed mainly to integrate MSD and MSTI into a versatile framework with the coarse-to-fine training paradigm, which utilizes MSD as the predecessor foundational stage to facilitate the subsequent MSTI process, through incorporating rationales generated from LMMs.\\nThe overview of our model is illustrated in Figure \\\\ref{fig:model}, which consists of: 1) Divergent Thinking with LMM ($\\\\S$\\\\ref{LMM}), 2) Coaser-Grained Pre-Training ($\\\\S$\\\\ref{MSD}), and 3) Finer-Grained Fine-Tuning ($\\\\S$\\\\ref{MSTI}).\\n\\\\subsection{Divergent Thinking with LMM}\\\\label{LMM}\\n\\nGenerally, LLMs can generate reasonable thoughts~\\\\citep{wei2022chain} to unveil the underlying meaning of the sarcasm. The rationales from LLMs usually express perspectives grounded with commonsense or related to certain social scenarios~\\\\citep{huang2023chatgpt, lin2024explainable}, which can be used as extensive prior knowledge for smaller downstream models to facilitate decision-making.\\nAlthough LLMs have shown emergent abilities in reasoning and interpreting, they still suffer from preconception bias and may generate uncredible content or even make false assertions~\\\\citep{hu2023bad, lin2023beneath}. Therefore, the downstream decision-maker model would have to be robust enough to alleviate the negative impact imposed by the input noisy LLM-generated rationales. In pursuit of this, we resort to the inspiration of divergent multimodal thinking with vision LLMs, i.e., LMMs, fostering our model to explore a more reliable reasoning pathway from the conflicting and noisy insights provided by LMMs. \\n\\nGiven an input ${M} = \\\\{I,T\\\\}$, we prompt LMMs to generate a pair of competing rationales based on the text $T$, the image $I$, and the potential sarcasm labels ${*} \\\\in \\\\{\\\\texttt{sarcastic}, \\\\texttt{non-sarcastic}\\\\}$, by using a prompt template $p$ we curated in advance. To exploit LMMs' divergent reasoning ability, we construct each sample with different potential sarcasm labels, respectively. Specifically, we design the prompt template $p$ as follows:\\n\\n\\\\textit{``Given a tweet that consists of a text and an image, please give me a rationale of why the tweet is $\\\\{*\\\\}$. \\\\\\\\\\ntweet text: $\\\\{T\\\\}$ \\\\\\\\\\ntweet image: $\\\\{I\\\\}$''\\n}\\n\\nNote that the potential labels $*$ are just used to formalize two opposite standpoints for multimodal reasoning regardless of the ground-truth label. Then we can derive the competing rationales $r_{pos}$ or $r_{neg}$ from LMMs to support the \\\\texttt{sarcastic} or \\\\texttt{non-sarcastic} positions.\\nBy introducing adversarial labels, we encourage LMMs to adopt diverse perspectives, thereby providing a range of background knowledge enriched with deliberate noise. Because the rationale for the false class ideally contains more useless information than the other one for the ground truth. The contextual subtleties of sarcasm that are pivotal to rival candidate sarcasm categories, can be thus more effectively highlighted and contrasted. This allows the rest of the model to achieve the logical reasoning of the true sarcastic intent by considering it from diverse perspectives, while moderating vulnerability to the potential noise in the LMM-generated rationales.\\n\\n\\\\subsection{Coarser-Grained Pre-Training}\\\\label{MSD}\\n\\nGiven the close correlation between the coarser-grained multimodal sarcasm detection with the finer-grained multimodal sarcasm target identification, we advocate for initial pre-training in multimodal sarcasm detection, allowing the model to grasp the essence of sarcasm preliminarily. This foundational understanding could set the stage for a more nuanced and detailed identification of sarcasm targets in subsequent fine-tuning.\\n\\\\noindent\\\\textbf{Encoding and Fusion.} \\nFor an input sample ${M} = \\\\{I,\\\\hat{T}\\\\}$ packed with the generated competing rationales, where $\\\\hat{T} = \\\\{T, r_{pos}, r_{neg}\\\\}$ is the input text, we first extract textual and visual features as:\\n{\\n\\\\setlength{\\\\abovedisplayskip}{0.1cm}\\n\\\\setlength{\\\\belowdisplayskip}{0.1cm}\\n\\\\begin{equation}\\nH_T =\\\\mathsf{E}_T(\\\\hat{T}), \\n~~\\nI_E =\\\\mathsf{E}_I(I),\\n\\\\end{equation}}\\nwhere $H_{T} \\\\in \\\\mathbb{R}^{m \\\\times d}$ is the token embedding output by the text encoder $\\\\mathsf{E}_T(\\\\cdot)$ implemented by Transformer Encoder~\\\\citep{raffel2020exploring}, $m$ is the input token length and $d$ is the dimension of hidden states. $\\\\mathsf{E}_I(\\\\cdot)$ denotes the image encoder based on Vision Transformer~\\\\citep{liu2021swin}, used to fetch the patch-level features of the image with $n$ patches, which are projected into the visual features $I_E \\\\in \\\\mathbb{R}^{n \\\\times d}$. Since both the text and image encoders are designed as Transformer-based, the embeddings shaped by the isomorphic encoding structure can enhance consecutive multimodal fusion during encoding~\\\\citep{liu2023grounding}.\\nThen, to align the semantics in the text and image, we adopt a bi-directional attention module based on the cross-attention mechanism. Taking the text-to-image cross-attention ${CrossAttn}(H_T,I_E)$ as an example, we define the query, key and value as $\\\\{Q_{{T}},  K_{{I}}, V_{{I}}\\\\} = \\\\{H_T W_Q, I_E W_K, I_E W_V\\\\}$, where $\\\\{W_{Q}, W_{K}, W_{V}\\\\} \\\\in \\\\mathbb{R}^{d \\\\times d_k}$ are trainable weights. Then the calculation is as follows:\\n{\\n\\\\setlength{\\\\abovedisplayskip}{0.1cm}\\n\\\\setlength{\\\\belowdisplayskip}{0.1cm}\\n\\\\begin{equation}\\n    \\\\begin{aligned}\\nH_{T}^0  & =\\\\operatorname{softmax}\\\\left(\\\\frac{Q_T K_I^{\\\\top}}{\\\\sqrt{d_k}}\\\\right) V_I,\\n\\\\end{aligned}\\n\\\\label{c_attn}\\n\\\\end{equation}}\\nWith Equation~\\\\ref{c_attn}, similarly, we can calculate the image-to-text cross-attention. Combing the two cross-attention modules together, we fuse the multimodal features during encoding as follows:\\n{\\n\\\\setlength{\\\\abovedisplayskip}{0.1cm}\\n\\\\setlength{\\\\belowdisplayskip}{0.1cm}\\n\\\\begin{equation}\\n    \\\\begin{aligned}\\nH_{T}^0 &= {CrossAttn}(H_T, I_E), \\\\\\\\\\nI_B &= {CrossAttn}(I_E,H_T),\\n\\\\end{aligned}\\n\\\\end{equation}} where $H_{T}^0, I_B$ are the attended textual and visual features, respectively. \\nTo optimize the information integration of multimodal sarcasm with competing rationales, we further develop a query selection mechanism to prioritize image region features that exhibit higher correlations with the input text. This yields:\\n{\\n\\\\setlength{\\\\abovedisplayskip}{0.1cm}\\n\\\\setlength{\\\\belowdisplayskip}{0.1cm}\\n\\\\begin{equation}\\n\\\\arg \\\\max_n \\\\left(\\\\max_m \\\\left(I_B {H_{T}^0}^{\\\\top} \\\\right)\\\\right),\\n\\\\end{equation}}\\nwith which we obtain the index of the topmost relevant local visual features, queried by the textual features ${H_{T}^0}$ from the global visual features $I_B$. We name the selected local visual features as $I_Q$.\\n\\n\\\\noindent\\\\textbf{Cross-Modality Text Decoding.}  Based on the attended textual features $H_{T}^0$ and query-selected local visual features $I_Q$, we then devise a multimodal decoding strategy with textual outputs to infer sarcasm for MSD. Specifically, during decoding, we only exploit a text-to-image cross-attention module to attain the textual features attended with the visual ones:\\n{\\n\\\\setlength{\\\\abovedisplayskip}{0.1cm}\\n\\\\setlength{\\\\belowdisplayskip}{0.1cm}\\n\\\\begin{equation}\\n    \\\\begin{aligned}\\n{H_T^i}_{attn} &= {CrossAttn}(H_{T}^i, I_Q),\\n\\\\end{aligned}\\n\\\\end{equation}}\\nwhere $H_T^i$ is the textual feature input of the $i^{th}$ LM decoder layer, and ${H_T^i}_{attn}$ is the attended textual feature output of the cross-attention ${CrossAttn}$. Then by adding the attended features ${H_T^i}_{attn}$ to the output of the $i^{th}$ LM decoder $LM_{dec}^i$, the fused intermediate features $H_T^{i+1}$ fed into the next LM decoder layer are:\\n{\\n\\\\setlength{\\\\abovedisplayskip}{0.1cm}\\n\\\\setlength{\\\\belowdisplayskip}{0.1cm}\\n\\\\begin{equation}\\n\\\\begin{aligned}\\nH_T^{i+1} & = LM_{dec}^i(H_T^i) + {H_T^i}_{attn}.\\n\\\\end{aligned}\\n\\\\end{equation}}\\nAfter $L$ layers of cross-modality LM decoder, we have the final textual representations $H_T^L$, further decoded as the text output to clearly express whether the sample is sarcastic. Finally, we train the model $f$ by minimizing the following loss:\\n{\\n\\\\setlength{\\\\abovedisplayskip}{0.1cm}\\n\\\\setlength{\\\\belowdisplayskip}{0.1cm}\\n\\\\begin{equation}\\n\\\\mathcal{L}_{text} = CE(f(I,\\\\hat{T}),y),\\n\\\\label{eq6}\\n\\\\end{equation}}\\nwhere $CE(\\\\cdot)$ denotes the cross-entropy loss between the generated label token and ground truth label $y$ for MSD. During the coarser-grained pre-training, the model is trained to distill the essence and discard irrelevant elements from the divergent thinking of LMMs about sarcasm. Such a process could fortify our model's resilience in the subsequent fine-tuning stage for MSTI, ensuring robustness against the potential inaccuracies stemming from LMMs, leading to a more independent and refined thought of the LMM-generated rationales.\\n\\\\subsection{Finer-Grained Fine-Tuning} \\n\\\\label{MSTI}\\nAfter the coarser-grained pre-training stage, our model could be resilient against the potential variation and bias in LMMs through the competing rationales, to first comprehend what constitutes sarcasm. As the goal of our approach is to identify both the textual and visual sarcasm targets for further deciphering sarcasm, we conduct the finer-grained fine-tuning stage for MSTI, which shares the same model architecture, parameters of the multimodal encoding and text decoding procedures as $\\\\S$\\\\ref{MSD} but differs in the text decoding output and an additional image decoding procedure.\\n\\nDifferent from the pre-training stage in $\\\\S$\\\\ref{MSD}, the sample $M$ in MSTI is set as \\\\textit{sarcastic} prior due to the nature of this specific task~\\\\citep{wang2022multimodal}.\\nThus the input text for a given \\\\textit{sarcastic} sample $M$ is formed as $\\\\hat{T} = \\\\{T, r_{pos}\\\\}$ in this stage, where we only provide the text $T$ and the LMM-generated rationale $r_{pos}$ that explains why $M$ is sarcastic. For the cross-modality text decoding, we generate the predicted textual sarcasm targets that are entities in the text $T$. Then the textual target loss $\\\\hat{\\\\mathcal{L}}_{text}$ can be computed akin to that outlined in Equation~\\\\ref{eq6}.   \\n\\n\\\\noindent\\\\textbf{Cross-Modality Image Decoding.} \\nFor visual object detection, we use a cross-modality image decoder to discern the visual sarcasm target, where the textual features $H_T^0$ and the global visual features $I_B$ are used to attend to the local visual features $I_Q$ in each Transformer decoder layer:\\n{\\n\\\\setlength{\\\\abovedisplayskip}{0.1cm}\\n\\\\setlength{\\\\belowdisplayskip}{0.1cm}\\n\\\\begin{equation}\\n\\\\begin{aligned}\\n& I_Q^{j^{\\\\prime}}={SelfAttn}\\\\left(I_Q^j\\\\right), \\\\\\\\\\n& I_Q^{j^{\\\\prime \\\\prime}}={CrossAttn}\\\\left(I_Q^{j^{\\\\prime}}, I_B\\\\right), \\\\\\\\\\n& I_Q^{j+1}={CrossAttn}\\\\left(I_Q^{j^{\\\\prime \\\\prime}}, H_T^0\\\\right),\\n\\\\end{aligned}\\n\\\\end{equation}}\\nwhere ${SelfAttn}(\\\\cdot)$ denotes self-attention, and $I_Q^j$ is the input of the $j^{th}$ Transformer decoder layer. After $K$ layers of the image decoder, we have the final visual features $I_Q^K$. Afterwards, we decode $I_Q^K$ as the image output consisting of a bounding box output and its confidence score. Following previous object detection work~\\\\citep{zhang2022dino}, we use the L1 loss $\\\\mathcal{L}_{l1}$ and the GIOU~\\\\citep{rezatofighi2019generalized} loss $\\\\mathcal{L}_{giou}$ for bounding box regressions, and the cross-entropy classification loss $\\\\mathcal{L}_{cls}$ for confidence scores as the joint optimization objective:\\n{\\n\\\\setlength{\\\\abovedisplayskip}{0.1cm}\\n\\\\setlength{\\\\belowdisplayskip}{0.1cm}\\n\\\\begin{equation}\\n\\\\mathcal{L}_{img} = \\\\alpha  \\\\mathcal{L}_{l1} + \\\\beta  \\\\mathcal{L}_{giou} +\\\\gamma  \\\\mathcal{L}_{cls},\\n\\\\label{eq_imgloss}\\n\\\\end{equation}}\\nwhere $\\\\alpha, \\\\beta$ and $\\\\gamma$ are the hyper-parameters to scale the losses, $\\\\mathcal{L}_{img}$ is the visual target loss. Finally, the overall training loss $\\\\mathcal{L}$ for this stage is:\\n{\\n\\\\setlength{\\\\abovedisplayskip}{0.1cm}\\n\\\\setlength{\\\\belowdisplayskip}{0.1cm}\\n\\\\begin{equation}\\\\mathcal{L} = \\\\mathcal{L}_{img} + \\\\hat{\\\\mathcal{L}}_{text}.\\n\\\\end{equation}}\\n\\n\\\\paragraph{Model Training.}\\nWe implement model training following a coarse-to-fine paradigm: 1) Pre-training on the coarser-grained MSD task by minimizing $\\\\mathcal{L}_{text}$, and 2) Fine-tuning on the finer-grained MSTI task by minimizing $\\\\mathcal{L}$, where the auxiliary task MSD is the predecessor training phase of the goal task MSTI. To this end, we unify the classification task for MSD and the sequence tagging task for textual target identification in MSTI into a text generation task. Note that for model testing on MSD, we use the model parameters obtained after the coarser-grained pre-training; in terms of the goal task MSTI, we directly input the test sarcastic sample into our finer-grained fine-tuned model to identify multimodal sarcasm targets. \\n\\n\\\\section{Experiments}\\n\\\\subsection{Experimental Setup}\\n\\\\textbf{Datasets.} Our experiments are conducted based on two publicly available multimodal sarcasm datasets for evaluation: MMSD2.0~\\\\citep{qin2023mmsd2} and MSTI~\\\\citep{wang2022multimodal}. Specifically, MMSD2.0 is a correction version of the raw MMSD dataset~\\\\citep{cai2019multi}, by removing the spurious cues and fixing unreasonable annotation.\\nIn the coarser-grained pre-training stage, we utilized the large-scale MMSD2.0 dataset to pre-train our model for multimodal sarcasm detection. Thus we introduce a refined version, i.e., MSTI2.0, to address the low-quality issue of the raw MSTI data by removing the visual target labels in images of only characters and converting them into the textual sarcasm target labels. Afterwards, MSTI2.0 is employed to fine-tune and evaluate the model in the finer-grained fine-tuning stage of our framework. \\n\\\\begin{table}[] \\\\small\\n\\\\centering\\n\\\\resizebox{0.95\\\\linewidth}{!}{\\\\begin{tabular}{lcccc}\\n\\\\toprule\\n\\\\multicolumn{1}{c}{}       & \\\\multicolumn{1}{l}{Acc.} & P     & R     & F1    \\\\\\\\ \\\\hline\\nAtt-BERT                   & 80.03                    & 76.28 & 77.82 & 77.04 \\\\\\\\\\nCMGCN                      & 79.83                    & 75.82 & 78.01 & 76.90 \\\\\\\\\\nHKE                      & 76.50                    & 73.48 & 71.07 & 72.25 \\\\\\\\\\nDynRT                      & 72.06                    & 71.79 & 72.18 & 71.98 \\\\\\\\\\nMulti-view CLIP           & 84.31                    & 79.66 & 85.34 & 82.40 \\\\\\\\ \\\\hline\\nCofiPara-\\\\textsc{Msd}               & \\\\textbf{85.70}                    & \\\\textbf{85.96} & \\\\textbf{85.55} & \\\\textbf{85.89} \\\\\\\\\\n\\\\toprule %\\\\hline\\n\\\\end{tabular}}\\n\\\\caption{Multimodal sarcasm detection results.}\\n\\\\label{msd_res}\\n\\\\end{table}\\n\\n\\\\noindent\\\\textbf{Baselines.} We compare our model with the following multimodal baselines for multimodal sarcasm detection, which is the auxiliary task: \\n1) \\\\textsf{Att-BERT}~\\\\citep{pan2020modeling}; 2) \\\\textsf{CMGCN}~\\\\citep{liang2022multi}; 3) \\\\textsf{HKE}~\\\\citep{liu2022towards}; 4) \\\\textsf{DynRT-Net}~\\\\citep{tian2023dynamic}; 5) \\\\textsf{Multi-view CLIP}~\\\\citep{qin2023mmsd2}. We adopt Accuracy, F1 score, Precision, and Recall to evaluate the MSD performance.\\n\\nTo evaluate our model in multimodal sarcasm target identification that is our goal task, we compare the following state-of-the-art MSTI systems: 1) \\\\textsf{BERT-Base}~\\\\citep{devlin2019bert}; 2) \\\\textsf{BERT-Large}; 3) \\\\textsf{Mask R-CNN}~\\\\citep{he2017mask}; 4) \\\\textsf{YOLOv8}~\\\\citep{terven2023comprehensive}; 5) \\\\textsf{OWL-ViT}~\\\\citep{minderer2022simple}; 6) \\\\textsf{Grounding DINO}~\\\\citep{liu2023grounding}; 7) \\\\textsf{MSTI-RB}~\\\\citep{wang2022multimodal}; 8) \\\\textsf{MSTI-VB}; 9) \\\\textsf{MSTI-CB}; 10) \\\\textsf{MSTI-CL}. We use Exact Match (EM)~\\\\citep{joshi2018sarcasm} and F1 score~\\\\citep{molla2019overview} as evaluation metrics of textual sarcasm target identification; and Average Precision (AP)~\\\\citep{lin2014microsoft}, i.e., the COCO-style AP, AP50, and AP75, as the metrics for visual sarcasm target identification. \\n\\nThe data statistics, construction details of MSTI2.\\n\\n\\\\begin{table*}[]\\n\\\\resizebox{\\\\textwidth}{!}{\\n\\\\begin{tabular}{lcccccccccc}\\n\\\\toprule\\n\\\\multicolumn{1}{c}{}              & \\\\multicolumn{5}{c}{Dev}               & \\\\multicolumn{5}{c}{Test}              \\\\\\\\ \\\\cmidrule(lr){2-6} \\\\cmidrule(l){7-11}\\n                                  & EM    & F1    & AP    & AP50  & AP75  & EM    & F1    & AP    & AP50  & AP75  \\\\\\\\ \\\\hline\\nBERT-Base                         & 26.82 & 45.23 & /     & /     & /     & 26.01 & 46.64 & /     & /     & /     \\\\\\\\\\nBERT-Large                        & 29.29 & 46.42 & /     & /     & /     & 27.89 & 46.93 & /     & /     & /     \\\\\\\\ \\\\hline\\nMask R-CNN                        & /     & /     & 06.90 & 13.30 & 05.70 & /     & /     & 07.60 & 14.30 & 07.30 \\\\\\\\\\nYOLOv8                            & /     & /     & 06.58 & 12.81 & 06.13 & /     & /     & 10.49 & 17.57 & 11.18 \\\\\\\\ \\\\hline\\nOWL-ViT                           & 14.80 & 01.20 & 03.36 & 13.75 & 00.17 & 18.40 & 01.64 & 03.32 & 14.47 & 00.91 \\\\\\\\\\nGrounding DINO                    & 18.29 & 01.60 & 11.15 & 19.77 & 10.37 & 15.22 & 00.59 & 10.92 & 17.26 & 11.30  \\\\\\\\\\nMSTI-RB (ResNet+BERT-Base)        & 27.09 & 47.28 & 01.82 & 06.71 & 00.14 & 28.84 & 47.05 & 02.11 & 07.80 & 00.30 \\\\\\\\\\nMSTI-VB (VGG19+BERT-Base)         & 28.19 & 45.74 & 02.03 & 07.43 & 00.27 & 29.51 & 49.02 & 02.57 & 08.92 & 00.24 \\\\\\\\\\nMSTI-CB (CSPDarkNet53+BERT-Base)  & 27.62 & 48.00 & 03.78 & 13.68 & 00.40 & 27.89 & 48.39 & 03.80 & 13.06 & 01.03 \\\\\\\\\\nMSTI-CL (CSPDarkNet53+BERT-Large) & 28.18 & 48.32 & 02.64 & 09.56 & 00.86 & 28.70 & 49.78 & 02.80 & 11.02 & 00.91 \\\\\\\\ \\\\hline\\nCofiPara-\\\\textsc{Msti}                    & \\\\textbf{31.96} & \\\\textbf{49.53} & \\\\textbf{15.38} & \\\\textbf{34.29} & \\\\textbf{15.57} & \\\\textbf{32.26} & \\\\textbf{50.27} & \\\\textbf{13.79} & \\\\textbf{32.49} & \\\\textbf{12.01} \\\\\\\\ \\\\toprule%\\\\hline\\n\\\\end{tabular}}\\n\\\\caption{Multimodal sarcasm target identification results.}\\n\\\\label{msti_res}\\n\\\\end{table*}\\n\\n\\\\subsection{Main Results}\\n\\n\\\\textbf{Sarcasm Detection Performance.} Table \\\\ref{msd_res} illustrates the performance (\\\\%) of our proposed method versus all the compared representative multimodal baselines on the auxiliary task MSD. From these results, we have the following observations: 1) Compared to graph-based methods such as CMGCN and HKE and routing-based DynRT, Att-BERT that relies on semantic understanding has better performance, indicating that this task requires models to capture deep semantic information rather than superficial attributes. 2) Multi-view CLIP shows an overall advantage in its ability to align textual and visual features, and the isomorphic structures of text and image encoder also contribute to its superiority. 3) Our proposed CofiPara-\\\\textsc{Msd} surpasses the leading baseline by 1.39\\\\% and 3.49\\\\% in accuracy and F1 score, additionally demonstrating a more balanced performance in terms of recall and precision, despite not primarily targeting the MSD task. The distinctive advantage of our model lies in the fact that while all the baselines solely focus on recognition, our model is equipped with rationales from divergent thinking with LMMs, which empowers our model to effectively uncover sarcastic content by adeptly leveraging the interplay between seemingly unrelated textual and visual elements within sarcasm.\\n\\\\noindent\\\\textbf{Target Identification Performance.}\\nTable \\\\ref{msti_res} shows the performance (\\\\%) of our method versus unimodal and multimodal baselines on the goal task MSTI. It can be observed that: 1) The unimodal methods, like text-modality models in the first group and image-modality models in the second group, fall short in simultaneously identifying both visual and textual sarcasm targets compared to the multimodal methods in the third group. 2) The textual target identification performance of visual grounding models (i.e., OWL-ViT and Grounding DINO), is hindered by the discrepancy between the MSTI task and their original pre-training objectives. Additionally, the lack of a consistent one-to-one correspondence between textual and visual targets in MSTI samples further contributes to their suboptimal performance. 3) Our method drastically excels in EM and AP50 compared to baselines, especially in visual target identification. We observe that CofiPara-\\\\textsc{Msti} improves textual target identification performance by 3.26\\\\% on average EM score compared to MSTI-VB, suggesting that our model is more precise in discerning sarcasm targets in the text modality of multimodal contents. On the other hand, our model exhibits a substantial superiority in visual target identification performance, especially on the AP50 metric, for an average improvement of 14.88\\\\% over the best visual performed baseline, indicating that our model can capture the correct visual targets within the image modality that contain sarcastic meanings, while baseline models perform poorly by simply identifying object rather than sarcasm targets, which further implies that our model displays a better understanding of multimodal sarcasm.\\n\\n\\n\\\\subsection{Ablation Study of Target Identification}\\n\\n\\\\begin{table}[]\\n\\\\resizebox{\\\\linewidth}{!}{\\\\begin{tabular}{lccccc}\\n\\\\toprule\\n                               & EM    & F1    & AP    & AP50  & AP75  \\\\\\\\ \\\\hline\\nCofiPara-\\\\textsc{Msti}                  & 32.26 & 50.27 & 13.79 & 32.49 & 12.01 \\\\\\\\ \\\\hline\\nw/o MSD               & 30.24 & 49.61 & 13.72 & 30.39 & 12.15 \\\\\\\\\\nw/o LMM                 & 30.91 & 48.32 & 07.50 & 19.36 & 04.22 \\\\\\\\\\nw/o MSD\\\\&LMM & 30.10 & 50.72 & 06.34 & 17.21 & 04.61 \\\\\\\\ \\\\toprule\\n\\\\end{tabular}}\\n\\\\caption{Ablation results on MSTI2.0 test set.}\\n\\\\label{abla_msti_test}\\n\\\\end{table}\\n\\nAs MSTI is our goal task, we conduct ablative studies on MSTI2.0 test data with the following variants: 1) \\\\textit{w/o MSD}: Simply train our model on the MSTI task without knowledge from pre-training on MSD.\\n2) \\\\textit{w/o LMM}: Use model parameters initialized by pre-training on the MSD task, and fine-tune directly on the MSTI task without knowledge from LMMs. 3) \\\\textit{w/o MSD\\\\&LMM}: Train our model directly on the MSTI task without any knowledge of LMM reasoning and MSD pre-training.\\n\\nAs demonstrated in Table \\\\ref{abla_msti_test}, our model shows different degrees of performance degradation when MSD pre-training or LMM reasoning knowledge is ablated, indicating the effectiveness of our proposed method. Specifically, visual target identification performances show significant degradations by 2.10\\\\% and 13.13\\\\% on AP50 for \\\\textit{w/o MSD} and \\\\textit{w/o LMM} settings, respectively. This indicates that both LMM reasoning and MSD pre-training are helpful in identifying sarcasm targets, and that external LMM knowledge has a relatively larger impact on visual performance. We also notice that, the \\\\textit{w/o LMM} setting has relatively mild improvement over \\\\textit{w/o MSD\\\\&LMM}. This can be attributed to the fact that although the MSD pre-training itself may not necessarily significantly enhance model performance on the MSTI task with a large margin, it could help our model learn to implicitly ignore useless expressions and extract informative signals in the rationales from LMMs, highlighting its synergistic complementary with the LMM knowledge.\\n\\n\\\\subsection{Case Study of Explainability}\\n\\\\label{case_study_sec}\\n\\nTo better understand the mechanism of how LMM-generated rationales facilitate sarcasm target identification, we conduct a case study on the correctly predicted samples for better sarcasm explainability, as shown in Figure \\\\ref{case_study}, where visual sarcasm targets are annotated by green rectangles and textual sarcasm targets are highlighted in red italics.\\n\\nIn these examples, we observe that: 1) rationales generated by LMM help promote the connections between two modalities. As shown in Figure \\\\ref{case_study}(a), we notice that in the generated rationale, the image is depicted as a photo of Narendra Modi, which is then linked to the man who makes a refusing gesture in the picture. By introducing the connection between the word ``narendramodi'' and the man in the image, the target can be more easily recognized by our model; 2) on the other hand, rationales can complement background messages that are not given in the original texts and images, including both common sense and political knowledge. For example, in Figure \\\\ref{case_study}(a), the rationale first recognizes the man as Prime Minister of India, and then offers a correction ``for the first time ever'' to the non-standard abbreviation of ``for d 1st time ever'', which is further explained as an expression of sarcastic tone towards Modi. Similarly, in Figure \\\\ref{case_study}(b), LMM interprets ``mlk'' as Martin Luther King Jr. Day, the day to memorize dissenters who fought for civil rights, while the fact that police are arresting the dissenter in the image is in conflict with the context that expresses thanks to police, as well as the hashtag \\\\textit{\\\\#thinblueline}. The sarcasm target in the image is explained as the unjust political situation for people who fight for human rights, which is depicted in the image but outside the text. In this way, the rich but implicit correlations between the sarcasm text and image could be explained in visualized targets and readable snippets, which are also potentially valuable for aiding human checkers in verifying the sarcasm.\\n\\n\\\\begin{figure}\\n    \\\\centering\\n    \\\\includegraphics[width=\\\\linewidth, scale=1.00]{casestudy.pdf}\\n    \\\\caption{Examples of correctly identified samples.} \\n    \\\\label{case_study}\\n\\\\end{figure}\\n\\n\\\\section{Conclusion and Future Work}\\nIn this paper, we proposed a novel coarse-to-fine paradigm to decode the implicit meanings hidden beneath the surface of texts and images in multimodal sarcasm for MSTI, by leveraging rich prior knowledge from LMM reasoning and MSD pre-training. We first inspired divergent thinking with LMMs to derive competing rationales for coarser-grained pre-training of a small language model on MSD. Then we conducted finer-grained fine-tuning of the model on MSTI. Comprehensive experiments and analyses confirm the advantages of our framework. Future efforts aim to enhance our research by focusing on explicitly extracting useful information from generated rationales, to further relieve the inherent bias and variation in LMMs.\\n\\n\"\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for item in data:\n",
    "    context = get_paper_context(item)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": context},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(messages+[{'role':'assistant','content':'\\n\\n## Reviewer\\n'}], tokenize=False,add_generation_prompt=True)[:-4]\n",
    "    outputs = llm.generate([input_ids], sampling_params)\n",
    "    output = outputs[0]\n",
    "    generated_text = output.outputs[0].text\n",
    "    review_context = get_review_context(generated_text)"
   ],
   "id": "9815b89dae54916a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
