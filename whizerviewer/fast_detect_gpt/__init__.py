import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from fast_detect_gpt import get_sampling_discrepancy_analytic
import json
import argparse


class WhizReviewerDetector:
    def __init__(self, model_name="meta-llama/Meta-Llama-3.1-8B", device="cuda", cache_dir="../cache",
                 ref_path="local_infer_ref.json"):
        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
        self.model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir).to(device)
        self.model.eval()
        self.prob_estimator = self.ProbEstimator(ref_path)

    class ProbEstimator:
        def __init__(self, ref_path):
            self.real_crits = []
            self.fake_crits = []
            with open(ref_path, 'r') as fin:
                res = json.load(fin)
                self.real_crits.extend(res['predictions']['real'])
                self.fake_crits.extend(res['predictions']['samples'])
            print(f'ProbEstimator: total {len(self.real_crits) * 2} samples.')

        def crit_to_prob(self, crit):
            offset = np.sort(np.abs(np.array(self.real_crits + self.fake_crits) - crit))[100]
            cnt_real = np.sum((np.array(self.real_crits) > crit - offset) & (np.array(self.real_crits) < crit + offset))
            cnt_fake = np.sum((np.array(self.fake_crits) > crit - offset) & (np.array(self.fake_crits) < crit + offset))
            return cnt_fake / (cnt_real + cnt_fake)

    def detect(self, text):
        tokenized = self.tokenizer(text, return_tensors="pt", padding=True, return_token_type_ids=False).to(self.device)
        labels = tokenized.input_ids[:, 1:]
        with torch.no_grad():
            logits = self.model(**tokenized).logits[:, :-1]
            crit = get_sampling_discrepancy_analytic(logits, logits, labels)

        prob = self.prob_estimator.crit_to_prob(crit)
        is_whizreviewer = prob > 0.5
        confidence = max(prob, 1 - prob)

        return is_whizreviewer, confidence


def main():
    parser = argparse.ArgumentParser(description="Detect if a given text is generated by WhizReviewer")
    parser.add_argument("--sentence", type=str, required=True, help="The text to analyze")
    parser.add_argument("--model_name", type=str, default="meta-llama/Meta-Llama-3.1-8B",
                        help="Name of the model to use")
    parser.add_argument("--device", type=str, default="cuda", help="Device to run the model on (cuda or cpu)")
    parser.add_argument("--cache_dir", type=str, default="../cache", help="Directory to cache the model")
    parser.add_argument("--ref_path", type=str, default="local_infer_ref.json", help="Path to the reference file")

    args = parser.parse_args()

    detector = WhizReviewerDetector(
        model_name=args.model_name,
        device=args.device,
        cache_dir=args.cache_dir,
        ref_path=args.ref_path
    )

    is_whizreviewer, confidence = detector.detect(args.sentence)
    print(f"Is WhizReviewer generated: {is_whizreviewer}")
    print(f"Confidence: {confidence:.2f}")


if __name__ == "__main__":
    main()